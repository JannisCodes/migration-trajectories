---
output: latex_fragment
bibliography: ../referencesZotero.bib
csl: ../apa.csl
---

```{r}
#| label: section setup
#| include: false

# Global Chunk Options
knitr::opts_chunk$set(
  fig.width = 12,
  fig.height = 8,
  fig.path = "../figures/",
  include = FALSE,
  warning = FALSE,
  message = FALSE
)
```


```{r}
#| label: cluster perfomance indices

library(fpc)

kmeansPerf <- list()
for(i in 2:10){
  kmeansPerf[[i-1]] <- fpc::cluster.stats(d = dist(scaled_pc), clustering = kmeansRes[[i-1]]$cluster, G2 = TRUE, G3 = TRUE, aggregateonly = TRUE) %>% unlist
}
kmeansPerfComp <- data.frame(Reduce(rbind, kmeansPerf))

kmeansStatNames <- c(
  "n" = "number of cases", 
  "cluster.number" = "number of points", 
  "min.cluster.size" = "size of smallest cluster", 
  "noisen" = "number of noise points", # Not relevant for kmeans 
  "average.between" = "average distance between clusters", 
  "average.within" = "average distance within clusters", # (reweighted so that every observation, rather than every distance, has the same weight)
  "max.diameter" = "maximum cluster diameter",
  "min.separation" = "minimum cluster separation", 
  "ave.within.cluster.ss" = "within clusters sum of squares", # generalisation
  "avg.silwidth" = "average silhouette width", 
  "g2" = "Goodman Kruskal's Gamma coefficient", # See Milligan and Cooper (1985), Gordon (1999, p. 62). 
  "g3" = "G3 coefficient", # See Gordon (1999, p. 62)
  "pearsongamma" = "Normalized gamma", # correlation between distances and a 0-1-vector where 0 means same cluster, 1 means different clusters. see Halkidi et al. (2001)
  "dunn" = "Dunn index", # minimum separation / maximum diameter, Dunn index, see Halkidi et al. (2002).
  "dunn2" = "Dunn index 2", # minimum average dissimilarity between two cluster / maximum average within cluster dissimilarity, another version of the family of Dunn indexes
  "entropy" = "entropy of distribution of cluster memberships", # see Meila(2007)
  "wb.ratio" = "average within / average between", 
  "ch" = "Calinski and Harabasz index", # (Calinski and Harabasz 1974, optimal in Milligan and Cooper 1985; generalised for dissimilarites in Hennig and Liao 2013)
  "widestgap" = "widest within-cluster gap", 
  "sindex" = "separation index"
)

kmeansPerfComp %>%
  kbl(.,
      escape = FALSE,
      booktabs = T,
      align = "c",
      digits = 2,
      row.names = FALSE,
      col.names = kmeansStatNames,
      caption = "Variables by Interaction Types and Study Availability") %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )

kmeansOut <- kmeans(scaled_pc, centers = 2, nstart = 100)
```

```{r}
#| label: Plot Performance

# Elbow method
fviz_nbclust(scaled_pc, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = 2)+
  labs(subtitle = "Elbow method") +
  theme_Publication()
# Silhouette method
fviz_nbclust(scaled_pc, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method") +
  theme_Publication()

fviz_cluster(kmeansOut, geom = "point", data = scaled_pc, ggtheme = theme_Publication())
fviz_cluster(kmeansOut, geom = "point", data = z_data, ggtheme = theme_Publication())
```


In our own illustration example, we used the \texttt{cluster.stats()} function from the \texttt{fpc} \textsf{R} package, which calculates a wide variety of internal cluster validity statistics for each of the extracted clustering solutions. With real-world data no single evaluation measure is likely perfect, and different measures may produce different results depending on the characteristics of the data and the research question being addressed \citep{kittler1998}. It is therefore important to consider a variety of evaluation measures and to carefully interpret the results in the context of the specific analysis \citep{vinh2009}. We found that across most indices, the analysis with $k=2$ clusters performed the best. Three commonly reported indices we would like to highlight are the comparison of within clusters sum of squares, the average silhouette score, and the Calinski-Harabasz index. The first statistic we looked at was the total within-cluster sum of square $WCSS$ (see also \equatref{eq:kWCSS}). While the within-cluster variation will naturally decrease with (more) smaller clusters, we observed that the decrease in $WCSS$ was largest until $k=2$ after which the decrease was much smaller. This method is also sometimes referred to as the 'elbow method' \citep{syakur2018}. We then looked at a second, commonly used measure, the average silhouette score. This statistic measures the degree to which each time feature data point is similar to other points within the same cluster, compared to points in other clusters \citep{rousseeuw1987}. In our case, the $k=2$ solution maximized the silhouette coefficient ($s_2=$ `r format(round(kmeansPerfComp$avg.silwidth[kmeansPerfComp$cluster.number==2], 2), nsmall = 2)`). Finally, the Calinski-Harabasz index assesses the compactness and separation of the clusters by assessing the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters --- thus, the higher the score the better the performances \citep{calinski1974}. In our case, the $k=2$ solution also showed the highest Calinski-Harabasz index ($CH_2=$ `r format(round(kmeansPerfComp$ch[kmeansPerfComp$cluster.number==2], 2), nsmall = 2)`; a full table of all extracted validity statistics is available in Supplemental Material A)\footnote{It is important to note that another commonly assessed aspect of the evaluation is determining the stability and robustness of the clusters \citep{berkhin2006}. This can be assessed by evaluating the sensitivity of the clusters to different feature sets or clustering algorithms, or by using techniques such as bootstrapping to assess the uncertainty of the clusters \citep{vinh2009}. Especially when comparing different clustering algorithms one common index is the Bayesian information criterion (BIC), where a lower BIC indicates that a model is more representative of the data \citep{vandeschoot2017}.}. In the final $k=2$ solution the k-means analysis also assigned a relatively even number of participants to cluster 1 ($n_{C_1}=$ `r table(kmeansOut$cluster)[1]`) and cluster 2 ($n_{C_1}=$ `r table(kmeansOut$cluster)[2]`).
