---
output: latex_fragment
bibliography: ../referencesZotero.bib
csl: ../apa.csl
---

```{r}
#| label: section setup
#| include: false

# Global Chunk Options
knitr::opts_chunk$set(
  fig.width = 12,
  fig.height = 8,
  fig.path = "../figures/",
  include = FALSE,
  warning = FALSE,
  message = FALSE
)
```

```{r}
#| label: featureClustering

# load libraries
library(factoextra)
library(NbClust)

# check variance differences
max(pca.out %>% summarize_all(var))/min(pca.out %>% summarize_all(var))
pca.out %>%
  summarize_all(sd) %>%
  pivot_longer(everything(), names_to = "PC", values_to = "sd") %>%
  ggplot(., aes(x=reorder(PC, sd), y=sd, label=sd)) +
  geom_bar(stat='identity', width=.5) +
  coord_flip() +
  theme_Publication()

# prep data
scaled_pc <- pca.out #%>% scale
```

```{r}
#| label: k-means modeling based on selected k

kmeansRes <- list()
for (i in 2:10) {
  kmeansRes[[i-1]] <- kmeans(scaled_pc, centers = i, nstart = 100)
}
```

During the k-means clustering itself, the analysis seeks to minimize the total within-cluster variation. The analysis is designed to optimize the clustering of the feature data into $k$ groups, where $k$ is a pre-defined number of clusters. We used the Hartigan and Wong algorithm, which is a widely used algorithm in k-means clustering \citep{hartigan1979}. The algorithm starts by randomly separating the data points into k clusters and then iteratively updates the assignment of each point to the nearest cluster center until convergence. To do so, the Hartigan and Wong algorithm specifically calculates the within-cluster variation ($W$) of cluster $C_i$ as the summed squared Euclidean distances of the feature $x$ to the closest cluster centroid $\mu_i$:

```{=tex}
\begin{equation} \label{eq:kWCi}
  W(C_i) = \sum_{x \in C_i}(x-\mu_i)^2
\end{equation}
```

By summing the within-cluster sum of squares from all $k$ clusters, we can then derive the total within-cluster sum of square $WCSS$:

```{=tex}
\begin{equation} \label{eq:kWCSS}
  WCSS = \sum_{i=1}^k W(C_i) = \sum_{i=1}^k \sum_{x \in C_i} (x - \mu_i)^2
\end{equation}
```

It is this $WCSS$ that becomes the objective function to be minimized, by iteratively moving features from one cluster to another \citep{hartigan1979}. In particular, the algorithm (1) calculates the cluster centroids of the initial partitioning, (2) checks whether any feature has a centroid that is closer than that of the currently assigned cluster (3) updates the centroids based on any reassigned features, and then iterates between steps two and three until $WCSS$ is minimized (i.e., locally optimal convergence) or a maximum number of iterations is reached \citep{jain2010}. Given the iterative nature of the algorithm, the initial partitioning is often important because the algorithm might arrive at a suboptimal clustering where the $WCSS$ cannot be further reduced by moving any feature to another cluster, despite a better solution existing \citep[i.e., a local minimum;][]{timmerman2013}. It is, therefore often recommended to run the k-means clustering with several different starting positions.

In our case we entered the participants' PC-scores from the feature reduction step into the k-means algorithm. Because we did know the underlying number of clusters within our sample, we calculated the cluster solutions for $k=\{2, \dots , 10\}$ . To avoid local minima we used 100 random initial centroid positions for each run. Each of the 9 cluster solutions converged within the iteration limit. In the next step we will then evaluate which of the extracted cluster solutions offers the best fit with the data.

