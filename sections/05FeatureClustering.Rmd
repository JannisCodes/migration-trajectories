---
output: latex_fragment
bibliography: ../referencesZotero.bib
csl: ../apa.csl
---

```{r}
#| label: section setup
#| include: false

# Global Chunk Options
knitr::opts_chunk$set(
  fig.width = 12,
  fig.height = 8,
  fig.path = "../figures/",
  include = FALSE,
  warning = FALSE,
  message = FALSE
)
```

```{r}
#| label: featureClustering

# load libraries
library(factoextra)
library(NbClust)

# check variance differences
max(pca.out %>% summarize_all(var))/min(pca.out %>% summarize_all(var))
pca.out %>%
  summarize_all(sd) %>%
  pivot_longer(everything(), names_to = "PC", values_to = "sd") %>%
  ggplot(., aes(x=reorder(PC, sd), y=sd, label=sd)) +
  geom_bar(stat='identity', width=.5) +
  coord_flip() +
  theme_Publication()

# prep data
scaled_pc <- pca.out #%>% scale

# Elbow method
fviz_nbclust(scaled_pc, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = 2)+
  labs(subtitle = "Elbow method") +
  theme_Publication()
# Silhouette method
fviz_nbclust(scaled_pc, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method") +
  theme_Publication()
```

```{r}
#| label: k-means modeling based on selected k

kmeansOut <- kmeans(scaled_pc, centers = 2, nstart = 100)
```

```{r}
#| label: Plot Convex Hulls

fviz_cluster(kmeansOut, geom = "point", data = scaled_pc, ggtheme = theme_Publication())
fviz_cluster(kmeansOut, geom = "point", data = z_data, ggtheme = theme_Publication())
```

<!-- \Question{\textcolor{cyan}{It is good practice to standardize the input of the k-means algorithm. However, I think that does not make sense for PCs because the PCA is based on the idea of decomposing variances (and capturing the most variance in PC1, ...). If I actually standardize the PC-score, we get really weird k-means results. The only discussion of this I could find was a tutorial that DID NOT standardize the PCs: \url{https://medium.com/more-python-less-problems/principal-component-analysis-and-k-means-clustering-to-visualize-a-high-dimensional-dataset-577b2a7a5fe2}  -->

<!-- I had originally written:  -->
<!-- "To ensure that the clustering is weighing the principal components equally (as opposed to their variance), we standardize the particpants' PC-scores."  -->
<!-- but I went back to the 'raw' PC-scores now. But we should discuss this.}} -->

During the k-means clustering itself, the analysis seeks to minimize the total within-cluster variation. The analysis is designed to optimize the clustering of the feature data into $k$ groups, where $k$ is a pre-defined number of clusters. We used the Hartigan and Wong algorithm, which is a widely used algorithm in k-means clustering \citep{hartigan1979}. The algorithm starts by randomly separating the data points into k clusters and then iteratively updates the assignment of each point to the nearest cluster center until convergence. To do so, the Hartigan and Wong algorithm specifically calculates the within-cluster variation ($W$) of cluster $C_i$ as the summed squared Euclidean distances of the feature $x$ to the closest cluster centroid $\mu_i$:

```{=tex}
\begin{equation} \label{eq:kWCi}
  W(C_i) = \sum_{x \in C_i}(x-\mu_i)^2
\end{equation}
```

By summing the within-cluster sum of squares from all $k$ clusters, we can then derive the total within-cluster sum of square $WCSS$:

```{=tex}
\begin{equation} \label{eq:kWCSS}
  WCSS = \sum_{i=1}^k W(C_i) = \sum_{i=1}^k \sum_{x \in C_i} (x - \mu_i)^2
\end{equation}
```

It is this $WCSS$ that becomes the objective function to be minimized, by iteratively moving features from one cluster to another \citep{hartigan1979}. In particular, the algorithm (1) calculates the cluster centroids of the initial partitioning, (2) checks whether any feature has a centroid that is closer than that of the currently assigned cluster (3) updates the centroids based on any reassigned features, and then iterates between steps two and three until $WCSS$ is minimized (i.e., locally optimal convergence) or a maximum number of iterations is reached. Given the iterative nature of the algorithm, the initial partitioning is often important because the algorithm might arrive at a suboptimal clustering where the $WCSS$ cannot be further reduced by moving any feature to another cluster, despite a better solution existing (i.e., a local minimum). It is, therefore often recommended to run the k-means clustering with several different starting positions.

In our case we used the Elbow and the Silhouette method to determine the recommended number of $k$ clusters, which for both methods was two clusters (see Supplemental Material A). We then entered the participants' PC-scores from the feature reduction step into the k-means algorithm. To avoid local minima we used 100 random initial centroid positions. In the final solution the k-means analysis assigned `r table(kmeansOut$cluster)[1]` participants to cluster 1 and `r table(kmeansOut$cluster)[2]` participants to cluster 2.

