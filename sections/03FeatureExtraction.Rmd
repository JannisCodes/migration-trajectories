---
output: latex_fragment
bibliography: ../referencesZotero.bib
csl: ../apa.csl
---

```{r}
#| label: section setup
#| include: false

# Global Chunk Options
knitr::opts_chunk$set(
  fig.width = 12,
  fig.height = 8,
  fig.path = "../figures/",
  include = FALSE,
  warning = FALSE,
  message = FALSE
)
```

```{r}
#| label: featMiss

featureExclusion <- c(
  "mean",
  "sd",
  "rmssd",
  "ar02",
  "edf_ar",
  "_n"
)

pMissFull <- pMissFeat(
  all = featFull$features %>% select(-ends_with(featureExclusion)),
  contact = featFullContact$features,
  nocontact = featFullNoContact$features,
  title = "Feature-wise Missingess Across all Studies"
)
```

\paragraph{Central tendency.}
The central tendency refers to the statistical measures that represent the "typical" or "average" of a set of data. The most common measures of central tendency are the mean, median, and mode \citep{weisberg1992}. As a familiar statistic from probability theory, the central tendency sits at the heart of many fundamental questions about psychological time series. Researchers might, for example, be interested in whether "Over a one-month period, are some people happier than others?"

For the central tendency feature of our illustration we chose the robust \textit{median}, which can avoid potential issues with non-normally distributed time series responses or outliers \citep{weisberg1992}. To calculate the \textit{median} ($M$), we let $X_{ij}$ be the ordered list of values from the time series of variable $j$ for participant $i$. The calculation depends on whether the number of measurements in a time series $n$ is odd or even.

```{=tex}
\begin{equation} \label{eq:median}
  M(X_{ij}) = 
    \begin{cases}
      X \left[ \frac{n+1}{2} \right] & \text{if $n$ is odd} \\
      \frac{X \left[ \frac{n}{2} \right] + X \left[ \frac{n}{2} +1 \right]}{2} & \text{if $n$ is even}
    \end{cases}
\end{equation}
```


\paragraph{Variability.}
Variability captures the degree to which a set of data differs from the central tendency and is sometimes also referred to as the dispersion or spread of the data \citep{weisberg1992}. In time series analyses, variability is conceptually important because information about the distribution and diversity of the data has been found to be indicative of worse psychological states \citep{myin-germeys2018, helmich2021}. Person level differences of ESM measurements have, for example, been associated with higher levels of psycho-pathological recurrences among depression patients \citep{timm2017}. As such, psychological researchers and practitioners are often empirically interested in person-level differences in variability. Researchers on polarization and radicalization might for example ask: "Are people settled in their attitudes towards migrants or do they vary across the measurement period?''

For our illustration data, we chose to capture the time series variability with the \textit{Median Absolute Deviation} ($MAD$), where we calculate the \textit{median} ($M$; calculated as in \equatref{eq:median}) for the absolute deviations of measurement $x$ at time point $t$ for participant $i$ and variable $j$ from the median of that time series $X$. We again chose the robust statistic because the Median-based measure is less affected by non-normal distributions and extreme values or outliers compared to other measures of variability like the standard deviation \citep{weisberg1992}

```{=tex}
\begin{equation} \label{eq:mad}
  MAD(X_{ij}) = M(\left| x_{ijt} - M(X_{ij}) \right|)
\end{equation}
```


\paragraph{Instability.}
Instability captures the average change between two consecutive measurements \citep{ebner-priemer2009}. While instability is conceptually related to the variability feature, variability does not take into account temporal dependency, whereas instability looks at the 'jumpy-ness' of the data over time. In other words, variability reflects the range or diversity of values in a time series data, while instability reflects the fluctuation or inconsistency in a time series data over time  \citep{trull2008}. For example, if a person has rapid and extreme changes in mood their mood is highly unstable, while if a person's mood responses span a wide range over the entire study period, their mood is highly variable \citep{jahng2008}. Within psychological time series, instability measurements have especially been important in the research of borderline personality disorder \citep{trull2008} and suicidality \citep{kivela2022}, but also in understanding early warning signals more generally \citep{wichers2019}. Conceptually, the instability feature, thus, relates to a broad range of research questions, including: "What is the nature of daily mood swings in individuals with bipolar disorder?" or "How does the pattern of day-to-day (i.e., measurement-to-measurement) fluctuations in self-esteem during adolescence reflect the process of identity formation and self-concept development?"

For our data we chose the \textit{mean absolute change} \citep[$MAC$; e.g.,][]{ebner-priemer2009, barandas2020}, which looks at the average absolute difference of two consecutive measurements $x$ at time points $t$ and $t-1$, for each time series $X$ of participant $i$ and variable $j$.

```{=tex}
\begin{equation} \label{eq:mac}
  MAC(X_{ij}) = \frac{1}{n-1} \sum_{t=1, \ldots, t-1}\left|x_{t}-x_{t-1}\right|
\end{equation}
```

Another common measurement of instability is the \textit{Mean of the Squared Successive Differences} ($MSSD$), which is often preferred where differences in magnitude are more important than the frequency of those changes, for example, when big shifts in time series are considered more impactful or when outliers are meaningful and need to be taken into account.


\paragraph{Self-similarity.}
Self-similarity in time series data refers to the property of a time series to exhibit similar patterns of behavior over different time scales \citep{dmello2021}. That is, self-similarity describes how much a measurement carries over to future measurements. One important self-similarity in psychological time series is \texit{inertia} --- how much a measurement carries over to its next measurement \citep{kuppens2010, suls1998}. If inertia is high a development tends to stay in a certain state. Because high inertia is resistant to change, in emotion dynamics high inertia of negative affect has been found to be indicative of under-reactive systems and to be characteristic of psychological maladjustment \citep{kuppens2010}. In a similar vein, high inertia in negative affect at baseline was even predictive of the initial onset of depression \citep{kuppens2012}. Conceptually, inertia is more broadly connected to research questions such as: ``Do patients stay in a negative mood for several measurements?'' 

For our illustration case, we chose the commonly used autocorrelation or autoregression with a lag-1 to capture the inertia. High autocorrelation values can indicate high levels of inertia, while low autocorrelation values may indicate a more unpredictable or volatile time series \citep{dejonckheere2019}. The lag--1 autocorrelation $r_{ij,1}$ looks at the average correlation between a measurement $x$ and the proceeding measurement $x_{t-1}$ for the time series $X$ of participant $i$ and variable $j$. 

```{=tex}
\begin{equation} \label{eq:ar}
  r_{ij,1} = \frac{\sum_{t=1}^{n-l}(x_{ijt}-\overline{x}_{ij})(x_{ij,t-1}-\overline{x}_{ij})}{\sum_{t=1}^{n}(x_{ijt}-\overline{x}_{ij})^2}
\end{equation}
```

\paragraph{Linear trend.}
In non-stationary time series, a linear trend can be observed when there is a consistent increase or decrease in the data over time \citep{nyblom1986}. For psychological time series, researchers have, for example, pointed out the importance of linear trends in interpersonal communications \citep{vasileiadou2014}, and emotion dynamics \citep{oravecz2016}. Theoretically, linear trends are often considered the simplest way of assessing whether a psychological theory of change is appropriate \citep{gottman1969}. In empirical practice, linear trends are, thus, commonly exemplified by research questions such as "Do patient symptoms improve consistently?" or "Does worker productivity decline continuously?"

For the variables in our illustration data set, we chose an overall linear regression slope to capture the linear trend. The regression slope $b_{ij}$ provides the average change from one time point $t$ to the next across all measurements $x$ of a time series $X$ of participant $i$ and variable $j$. The specific form of the OLS slope formula we provide below calculates $b_{ij}$ as the sum across all time points of the product of the deviation of time $t$ from its mean $\overline{t}$ and the deviation of $x_{ij}$ from its mean $\overline{x}_{ij}$ at each time point, divided by the sum across all time points of the square of the deviation of time from its mean ($\sum(t-\overline{t})^2$). Intuitively, the formula captures the rate of change of variable $x_{ij}$ with respect to time. This slope will indicate how the variable $x_{ij}$ changes over time, controlling for its mean value and the mean of time. If the slope is positive, $x_{ij}$ increases over time; if it's negative, $x_{ij}$ decreases over time.

```{=tex}
\begin{equation} \label{eq:lin}
  b_{ij} = \frac{\sum(t-\overline{t})(x_{ijt}-\overline{x}_{ij})}{\sum(t-\overline{t})^2}
\end{equation}
```


\paragraph{Nonlinearity.}
Changes in psychology are not always linear, instead, nonlinearity is a common feature of psychological time series \citep{hayes2007}. As an example, episodic disorders, such as depression, are most likely best described as non-linear systems \citep{hosenfeld2015}. Similarly, patients in recovery from depression showed sudden changes in the improvement of depression \citep{helmich2020a}. But also substance abuse \citep{boker1998} or attitude changes rarely develop linearly \citep{vandermaas2003}. Conceptually, researchers might have research questions about the type of the development: "Is the development of anxiety a nonlinear process?" as well as the shape and structure of the development: "How many spikes in anxiety did a patient experience?" 
We summarized the nonlinear trend with the \textit{estimated degrees of freedom} of an empty GAM spline model. The $edf$ summarizes the \textit{wiggliness} of a spline trend line by estimating the number of parametric and non-parametric (or smooth) terms $p$ in the model. An edf of 1 would be equivalent to a linear relationship (i.e., one linear slope parameter), whereas a higher edf (particularly an edf > 2) is indicative of a non-linear trend.

```{=tex}
\begin{equation} \label{eq:edf}
  edf \approx \sum p
\end{equation}
```


Beyond our main features of interest, we also extracted the participant's number of completed ESM measurements to ensure that the clusters are comparable in that regard (i.e., to exclude spurious explanations for the cluster assignments). After the feature extraction, we found that about `r format(round(pMissFull$pmiss %>% filter(set == "All") %>% select(pMissTotal) %>% pull, 2), nsmall=2)`% of the extracted features are missing across the `r ncol(featFull$features %>% select(-ends_with(featureExclusion)))-1` features per participant. This might, for example, happen if participants do not have two subsequent measurements with outgroup interactions, so that an autocorrelation with lag-1 cannot be calculated for the contact-specific variables. The small number of missing values indicates that the feature-based approach indeed largely avoids the structural missingness issue. However, even the few missing values can be an issue for some feature reduction or feature clustering algorithms. We, thus, impute the missing feature values with a single predictive mean matching imputation using the MICE library \citep[][]{buuren2011}. Note again that with this procedure we only need to impute an extremely small number of missing values as most feature calculations can use the available data instead.
