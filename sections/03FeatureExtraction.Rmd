---
output: latex_fragment
bibliography: ../referencesZotero.bib
csl: ../apa.csl
---

```{r}
#| label: section setup
#| include: false

# Global Chunk Options
knitr::opts_chunk$set(
  fig.width = 12,
  fig.height = 8,
  fig.path = "../figures/",
  include = FALSE,
  warning = FALSE,
  message = FALSE
)
```

```{r}
#| label: featMiss

featureExclusion <- c(
  "mean",
  "sd",
  "rmssd",
  "ar02",
  "edf_ar",
  "_n"
)

pMissFull <- pMissFeat(
  all = featFull$features %>% select(-ends_with(featureExclusion)),
  contact = featFullContact$features,
  nocontact = featFullNoContact$features,
  title = "Feature-wise Missingess Across all Studies"
)
```

\paragraph{Central tendency.}
The central tendency refers to the statistical measures that represent the "typical" or "average" of a set of data. The most common measures of central tendency are the mean, median, and mode \citep{weisberg1992}. As a familiar statistic from probability theory, the central tendency sits at the heart of many fundamental questions about psychological time series. Researchers might, for example, be interested in whether "Over a one month period, are some people happier than others?".
For our central tendency feature we chose the robust \textit{median}, which can avoid potential issues with non-normally distributed time series responses or outliers \citep{weisberg1992}. To calculate the \textit{median} ($M$), we let $X_{ij}$ be the ordered list of values from the time series of variable $j$ for participant $i$. The calculation depends on whether the number of measurements in a time series $n$ is odd or even.

```{=tex}
\begin{equation} \label{eq:median}
  M(X_{ij}) = 
    \begin{cases}
      X \left[ \frac{n+1}{2} \right] & \text{if $n$ is odd} \\
      \frac{X \left[ \frac{n}{2} \right] + X \left[ \frac{n}{2} +1 \right]}{2} & \text{if $n$ is even}
    \end{cases}
\end{equation}
```


\paragraph{Variability.}
Variability captures the degree to which a set of data differs from the central tendency and is sometimes also referred to as the dispersion or spread of the data \citep{weisberg1992}. In time series analyses, variability is conceptually important because information about the distribution and diversity of the data has been found to be indicative of worse psychological states \citep{myin-germeys2018}. Person level differences of ESM measurements have, for example, been associated with higher levels of psycho-pathological recurrences among depression patients \citep{timm2017}.
For our illustration data, we chose to capture the time series variability with the \textit{Median Absolute Deviation} ($MAD$), where we calculate the \textit{median} ($M$; calculated as in \equatref{eq:median}) for the absolute deviations of measurement $x$ at time point $t$ for participant $i$ and variable $j$ from the median of that time series $X$.

```{=tex}
\begin{equation} \label{eq:mad}
  MAD(X_{ij}) = M(\left| x_{ijt} - M(X_{ij}) \right|)
\end{equation}
```


\paragraph{Instability.}
Instability captures the average change between two consecutive measurements \citep{ebner-priemer2009}. While instability is conceptually related to the variability feature, variability does not take into account temporal dependency, whereas instability looks at the 'jumpy-ness' of the data over time. In other words, variability reflects the range or diversity of values in a time series data, while instability reflects the fluctuation or inconsistency in a time series data over time  \citep{trull2008}. For example, if a person has rapid and extreme changes in mood their mood is highly unstable, while if a person's mood responses span a wide range over the entire study period, their mood is highly variable \citep{jahng2008}. Within psychological time series, instability measurements have especially been important in the research of borderline personality disorder \citep{trull2008} and suicidality \citep{kivela2022}, but also in understanding early warning signals more generally \citep{wichers2019}. Conceptually, the instability feature, thus, relates to a broad range of research questions, including: "Do changes tend to be slow and gradual or fast and abrupt?" 
For our data we chose the \textit{mean absolute change} \citep[$MAC$; e.g.,][]{ebner-priemer2009, barandas2020}, which looks at the average absolute difference of measurements $x$ at time points $t$ and $t-1$, for each time series $X$ of participant $i$ and variable $j$.

```{=tex}
\begin{equation} \label{eq:mac}
  MAC(X_{ij}) = \frac{1}{n-1} \sum_{t=1, \ldots, t-1}\left|x_{t}-x_{t-1}\right|
\end{equation}
```


\paragraph{Self-similarity.}
Self-similarity in time series data refers to the property of a time series to exhibit similar patterns of behavior over different time scales \citep{dmello2021}. That is, self-similarity describes how much a measurement carries over to future measurements. One important self-similarity in psychological time series is \texit{inertia} --- how much a measurement carries over to its next measurement \citep{kuppens2010}. If inertia is high a development tends to stay in a certain state or continues to move towards a certain direction. Because high inertia is resistant to change, in emotion dynamics high inertia has been found to be indicative of underreactive systems and to be characteristic of psychological maladjustment \citep{kuppens2010}. In a similar vein, high inertia at baseline was even predictive of the initial onset of depression \citep{kuppens2012}. Conceptually, inertia is more broadly connected to research questions such as: ``Do patients stay in a depressed mood for several measurements?'' 
For our illustration case, we chose the commonly used autocorrelation or autoregression with a lag-1 to capture the inertia. High autocorrelation values can indicate high levels of inertia, while low autocorrelation values may indicate a more unpredictable or volatile time series \citep{dejonckheere2019}. The lag--1 autocorrelation $r_{ij,1}$ looks at the average correlation between a measurement $x$ and the proceeding measurement $x_{t-1}$ for the time series $X$ of participant $i$ and variable $j$. 

```{=tex}
\begin{equation} \label{eq:ar}
  r_{ij,1} = \frac{\sum_{t=1}^{n-l}(x_{ijt}-\overline{x}_{ij})(x_{ij,t-1}-\overline{x}_{ij})}{\sum_{t=1}^{n}(x_{ijt}-\overline{x}_{ij})^2}
\end{equation}
```


\paragraph{Linear trend.}
In non-stationary time series a linear trend can be observed when there is a consistent increase or decrease in the data over time \citep{nyblom1986}. For psychological time series, researchers have, for example, pointed out the importance of linear trends in interpersonal communications \citep{vasileiadou2014}, and emotion dynamics \citep{oravecz2016}. Theoretically, linear trends are often considered the simplest way of assessing whether a psychological theory of change is appropriate \citep{gottman1969}. In empirical practice, linear trends are, thus, commonly exemplified by research questions such as "Do patient symptoms improve consistently?" or "Does worker productivity decline continuously?"
For the variables in our illustration data set we chose an overall linear regression slope to capture the linear trend. 

```{=tex}
\begin{equation} \label{eq:lin}
  b_{ij} = \frac{\sum(t-\overline{t})(x_{ijt}-\overline{x}_{ij})}{\sum(t-\overline{t})^2}
\end{equation}
```


\paragraph{Nonlinearity.}
Changes in psychology are not always linear, instead nonlinearity is a common feature of psychological time series \citep{hayes2007}. As an example, episodic disorders, such as depression, are most likely best described as non-linear systems \citep{hosenfeld2015}. Similarly, patients in recovery from depression showed sudden changes in the improvement of depression \citep{helmich2020a}. But also substance abuse \citep{boker1998} or attitude changes rarely develop linearly \citep{vandermaas2003}. Conceptually, researchers might have research question about the type of the development: "Is the development of anxiety a nonlinear process?" as well as the shape and structure of the development: "Does anxiety change as a sudden or smooth transition?" 
We summarized the nonlinear trend with the \textit{estimated degrees of freedom} of an empty GAM spline model. The $edf$ summarizes the \textit{wiggliness} of a spline trend line by estimating the number of parametric and non-parametric (or smooth) terms $p$ in the model. An edf of 1 would be equivalent to a linear relationship (i.e., one linear slope parameter), whereas a higher edf (particularly an edf > 2) is indicative of a non-linear trend.

```{=tex}
\begin{equation} \label{eq:edf}
  edf \approx \sum p
\end{equation}
```


Beyond our main features of interest, we also extracted the participant's number of completed ESM measurements to ensure that the clusters are comparable in that regard. After the feature extraction, we found that about `r format(round(pMissFull$pmiss %>% filter(set == "All") %>% select(pMissTotal) %>% pull, 2), nsmall=2)`% of the extracted features are missing across the `r ncol(featFull$features %>% select(-ends_with(featureExclusion)))-1` features per participant. This might, for example, happen if participants do not have two subsequent measurements with outgroup interactions, so that an autocorrelation with lag-1 cannot be calculated for the contact-specific variables. The small number of missing values indicates that the feature-based approach indeed largely avoids the structural missingness issue. The few missing values can, however, be an issue for some feature reduction or feature clustering algorithms. We, thus, impute the missing feature values with a single predictive mean matching imputation using the MICE library.
