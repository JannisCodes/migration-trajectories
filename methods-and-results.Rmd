---
output: latex_fragment
editor_options:
  chunk_output_type: console
bibliography: referencesZotero.bib
csl: apa.csl
---

```{r setup, include=FALSE}
# R Studio Clean-Up
cat("\014") # clear console
# rm(list=ls()) # clear workspace - use restart R instead [cmd/alt + shift + F10]
gc() # garbage collector

# Install and Load Packages
# !IMPORTANT!
# BEFORE FIRST RENDER:
# To install all relevant packages please run "renv::restore()" (or renv::init() and then initiate from lockfile) in the console before the first use to ensure that all packages are using the correct version.
# to store the packages in a contained library within the project folder: renv::settings$use.cache(FALSE) and add 'RENV_CONFIG_SANDBOX_ENABLED = FALSE' to an '.Renviron' file
lib <- c(
  "rmarkdown",
  "knitr",
  "remedy",
  "bookdown",
  #"tidyverse",
  "MASS",
  "brms",
  "psych",
  "ggplot2",
  "ggthemes",
  "haven",
  "RColorBrewer",
  "plotly",
  "grid",
  "gridExtra",
  "ggpattern",
  "lme4",
  "nlme",
  "jtools",
  "gtsummary",
  "sessioninfo",
  "tibble",
  "pander",
  "devtools",
  "mada",
  "data.table",
  "plyr",
  "dplyr",
  "tidyr",
  "Hmisc",
  "kableExtra",
  "papaja",
  "stringr",
  "stringi",
  "reshape2",
  "anytime",
  "lubridate",
  "purrr",
  "metafor",
  "dygraphs",
  "readxl", 
  "reshape",
  "factoextra",
  "Amelia", 
  "ThreeWay",
  "DescTools",
  # GAM
  "gratia",
  #"MASS",
  "mgcv",
  # For moving window models
  "Kendall",
  # For the Hamed & Rao correction of tau
  "modifiedmk", 
  "mgcViz",
  "shiny"
)
invisible(lapply(lib, library, character.only = TRUE))
rm(lib)

anytime::addFormats("%d-%m-%Y %H:%M:%S")  ## add format to anytime package (not default)
anytime::addFormats("%Y-%m-%d %H:%M:%S")
anytime::addFormats("%d-%m-%Y")  
#options(rgl.useNULL = TRUE)

# Load Custom Packages
source("./scripts/functions/fun.panel.R")
source("./scripts/functions/themes.R")
source("./scripts/functions/binaryCor.R")
source("./scripts/functions/MlCorMat.R")
source("./scripts/functions/MlTbl.R")
source("./scripts/functions/metaLmer.R")
source("./scripts/functions/meanViz.R")
source("./scripts/functions/gam.R")
source("./scripts/functions/tsFeatureExtractor.R")

# Markdown Options
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # set working directory
knitr::opts_knit$get("root.dir") # check working directory
options(
  scipen = 999,
  digits = 4,
  width = 400
) # removes scientific quotation
# knitr::opts_chunk$set(echo = TRUE, cache = F, cache.path = rprojroot::find_rstudio_root_file('cache/')) # cache settings

# Global Chunk Options
knitr::opts_chunk$set(
  fig.width = 12,
  fig.height = 8,
  fig.path = "Figures/",
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

```{r import data, include=FALSE}
# Load variable name lookup table
varNames <- readxl::read_excel("preregistration/varNames.xlsx")

# import full datasets
load("data/s123Full.RData")

# import reduced datasets
load("data/s123Red.RData")

# import pre-calculated features for faster renders
load("data/features/featOut.RData")
#save(featData, featFullImp, file = "data/features/featOutFullImp.RData") # speed up attempt 
#load("data/features/featOutFullImp.RData")
```


```{r Preparation of Variable Names, include=FALSE}
varNamIndicesS1 <- c(
  "autonomy",
  "competence",
  "relatedness",
  "education_level.pre",
  "associationMerged.pre",
  "assimilation.pre",
  "separation.pre",
  "integration.pre",
  "marginalization.pre",
  "VIA_heritage.pre",
  "VIA_Dutch.pre",
  "SSAS_surrounding.pre",
  "SSAS_privat.pre",
  "SSAS_public.pre",
  "assimilation.post",
  "separation.post",
  "integration.post",
  "marginalization.post",
  "VIA_heritage.post",
  "VIA_Dutch.post",
  "rosenberg.post",
  "social_support.post",
  "stress.post",
  "discrimination.post",
  "discrimination_month.post",
  "NLE_1month.post",
  "NLE_6month.post",
  "NLE_12month.post"
  )

varNamS123Aux <- varNames %>%
  filter(
    aux != 0,
    studyS1 == "S1",
    studyS2 == "S2",
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull
varNamS1Aux <- varNames %>%
  filter(
    aux != 0,
    studyS1 == "S1"
  ) %>%
  select(varNam) %>%
  pull
varNamS2Aux <- varNames %>%
  filter(
    aux != 0,
    studyS2 == "S2"
  ) %>%
  select(varNam) %>%
  pull
varNamS3Aux <- varNames %>%
  filter(
    aux != 0,
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull

varNamS123PCA <- varNames %>%
  filter(
    pca != 0,
    studyS1 == "S1",
    studyS2 == "S2",
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull
varNamS1PCA <- varNames %>%
  filter(
    pca != 0,
    studyS1 == "S1"
  ) %>%
  select(varNam) %>%
  pull
varNamS2PCA <- varNames %>%
  filter(
    pca != 0,
    studyS2 == "S2"
  ) %>%
  select(varNam) %>% 
  pull
varNamS3PCA <- varNames %>%
  filter(
    pca != 0,
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull

varNamOut <- c(
  "ResponseId",
  "relatednessNoInteraction",
  "relatednessSelf",
  "relatednessOther",
  "autonomy_Int",
  "autonomy_NoInt",
  "competence_Int",
  "competence_NoInt",
  varNamS1PCA[grepl('^MDMQ', varNamS1PCA)],
  varNamS2PCA[grepl('^ProSo|^AntiSo', varNamS2PCA)],
  varNamS3PCA[grepl(
    '^ProSo|^AntiSo|^agency|^autoFrust|^autoSat|^relatFrust|^relatSat|^compFrust|^compSat|^lonely[0-9]|^emotRegPos|^emotRegNeg',
    varNamS3PCA
  )]
)
varNamCore <- c(
  "PID",
  "TID",
  "TIDnum"
)
varNamNewAll <- c(
  "relatedness",
  "autonomy",
  "competence"
)
varNamNewS1 <- c(
  "relatedness",
  "autonomy",
  "competence",
  "alertness", 
  "calmness", 
  "valence"
)
varNamNewS2 <- c(
  "relatedness",
  "autonomy",
  "competence",
  "ProSo", 
  "AntiSo"
)
varNamNewS3 <- c(
  "relatedness",
  "autonomy",
  "competence",
  "ProSo", 
  "AntiSo",
  "agency",
  "autoFrust",
  "autoSat",
  "relatFrust",
  "relatSat",
  "compFrust",
  "compSat",
  "lonely",
  "emotRegPos",
  "emotRegNeg"
)

varNamNewCat <- c(
  "closeness_Calc",
  "gender_Calc",
  "ethnicity_Calc",
  "relationship_Calc"
)

varNamIntDep <- c(
  varNamS123PCA[grepl('^InteractionContext|AttitudesPartner|KeyNeedDueToPartner|^quality', varNamS123PCA)]
)

varNamIndiceItemsS1 <- varNames %>%
  filter(
    aux == -1,
    studyS1 == "S1"
  ) %>%
  select(varNam) %>%
  pull %>%
  append(., gsub(".pre|.post|_calc", "", varNamIndicesS1)) %>%
  unique

varNamS123MI <- c(varNamCore, varNamNewAll, varNamS123Aux)
varNamS1MI <- c(varNamCore, varNamIndicesS1, varNamS1Aux[!varNamS1Aux %in% varNamIndiceItemsS1]) #
varNamS2MI <- c(varNamCore, varNamNewS2, varNamS2Aux)
varNamS3MI <- c(varNamCore, varNamNewS3, varNamS3Aux)

varNamS123PCA <- c(varNamCore, varNamS123PCA[!varNamS123PCA %in% varNamOut])
varNamS1PCA <- c(varNamCore, varNamNewS1, varNamS1PCA[!varNamS1PCA %in% varNamOut])
varNamS2PCA <- c(varNamCore, varNamNewS2, varNamS2PCA[!varNamS2PCA %in% varNamOut])
varNamS3PCA <- c(varNamCore, varNamNewS3, varNamS3PCA[!varNamS3PCA %in% varNamOut])

varNamS123MIPsbl <- c(
  paste(varNamS123MI, rep(".pre", length(varNamS123MI)), sep = ""),
  varNamS123MI,
  paste(varNamS123MI, rep(".post", length(varNamS123MI)), sep = "")
)

varNamS123MiRed <- Reduce(
  intersect,
  list(
    dtS1Red %>% select(PID, TID, TIDnum, any_of(varNamS123MIPsbl)) %>% names,
    dtS2Red %>% select(PID, TID, TIDnum, any_of(varNamS123MIPsbl)) %>% names,
    dtS3Red %>% select(PID, TID, TIDnum, any_of(varNamS123MIPsbl)) %>% names
  )
)

idVars <- c("ID", "PID", "TID", "TIDnum", "date", "week", "study")
```

```{r variable names for all analyses, include=FALSE}
# ID Variables (just to re-iterate)
idVars <- idVars

# Common variables across studies 
varNamS123 <- varNamS123PCA[!varNamS123PCA %in% idVars]

# Non-Interaction Specific Variables
varNamS123NoInt <- varNames %>%
  filter(
    varNam %in% varNamS123PCA,
    contactSpecific == 0
  ) %>%
  select(varNam) %>%
  pull

# Interaction Specific Variables
varNamS123Int <- varNames %>%
  filter(
    varNam %in% varNamS123PCA,
    contactSpecific == 1
  ) %>%
  select(varNam) %>%
  pull

# S1 variables
varNamS1Clust <- varNamS1PCA[!varNamS1PCA %in% idVars]

# S1 Non-Interaction Specific Variables
varNamS1NoInt <- varNames %>%
  filter(
    varNam %in% varNamS1Clust,
    contactSpecific == 0
  ) %>%
  select(varNam) %>%
  pull

# S1 Interaction Specific Variables
varNamS1Int <- varNames %>%
  filter(
    varNam %in% varNamS1Clust,
    contactSpecific == 1
  ) %>%
  select(varNam) %>%
  pull


# S2 variables
varNamS2Clust <- varNamS2PCA[!varNamS2PCA %in% idVars]

# S2 Non-Interaction Specific Variables
varNamS2NoInt <- varNames %>%
  filter(
    varNam %in% varNamS2Clust,
    contactSpecific == 0
  ) %>%
  select(varNam) %>%
  pull

# S2 Interaction Specific Variables
varNamS2Int <- varNames %>%
  filter(
    varNam %in% varNamS2Clust,
    contactSpecific == 1
  ) %>%
  select(varNam) %>%
  pull

# S3 variables
varNamS3Clust <- varNamS3PCA[!varNamS3PCA %in% idVars]

# S3 Non-Interaction Specific Variables
varNamS3NoInt <- varNames %>%
  filter(
    varNam %in% varNamS3Clust,
    contactSpecific == 0
  ) %>%
  select(varNam) %>%
  pull

# S3 Interaction Specific Variables
varNamS3Int <- varNames %>%
  filter(
    varNam %in% varNamS3Clust,
    contactSpecific == 1
  ) %>%
  select(varNam) %>%
  pull
```

```{r variable names by response type and study, include=FALSE}
varNames %>%
  filter(
    pca != 0,
    varNam %in% c(varNamS123PCA, varNamS1PCA, varNamS2PCA, varNamS3PCA)
  ) %>%
  mutate(
    contactSpecific = recode(
      .$contactSpecific,
      `-1` = "No Interaction Only",
      `0` = "unspecific",
      `1` = "Interaction Only"
    ),
    study1 = ifelse(!is.na(studyS1), 1, 0),
    study2 = ifelse(!is.na(studyS2), 1, 0),
    study3 = ifelse(!is.na(studyS3), 1, 0)
  ) %>% 
  select(varNam, varGroup, contactSpecific, study1, study2, study3) %>%
  group_by(varGroup, varNam, contactSpecific, study1, study2, study3) %>%
  summarise(n = n()) %>%
  pivot_wider(names_from = c("varGroup", "varNam", "study1", "study2", "study3"), values_from = n, names_sep=" - ") %>%
  ungroup %>%
  t %>%
  as.data.frame %>%
  janitor::row_to_names(1) %>%
  replace(is.na(.), 0) %>%
  tibble::rownames_to_column(., var = "variable") %>%
  separate(variable, c("concept", "variable", "study1", "study2", "study3"), " - ") %>%
  mutate(across(c(study1, study2, study3), as.numeric)) %>%
  arrange(concept, unspecific, `Interaction Only`) %>% 
  mutate_all(~recode(.x, `0`="", ` 1`="✓")) %>%
  relocate(starts_with("study"), .after = last_col()) %>%
  rename_with(., ~ gsub("study", "Study ", .x, fixed = TRUE)) %>%
  kbl(.,
      escape = FALSE,
      booktabs = T,
      align = c("l", "l", rep("c", ncol(.)-2)),
      digits=2,
      caption = "Variables by Interaction Types and Study Availability") %>%
  add_header_above(c(" " = 2, "Contact Type" = 2, "Study Availability" = 3)) %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

```{r variables included in main analysis by response type, include=FALSE}
varNames %>%
  filter(
    pca != 0,
    varNam %in% c(varNamS123PCA, varNamS1PCA, varNamS2PCA, varNamS3PCA)
  ) %>%
  mutate(
    contactSpecific = recode(
      .$contactSpecific,
      `-1` = "No Interaction Only",
      `0` = "unspecific",
      `1` = "Interaction Only"
    ),
    study1 = ifelse(!is.na(studyS1), 1, 0),
    study2 = ifelse(!is.na(studyS2), 1, 0),
    study3 = ifelse(!is.na(studyS3), 1, 0)
  ) %>% 
  select(varNam, varGroup, contactSpecific, study1, study2, study3) %>%
  filter(study1 == 1 & study2 == 1 & study3 == 1) %>%
  group_by(varGroup, varNam, contactSpecific, study1, study2, study3) %>%
  summarise(n = n()) %>%
  pivot_wider(names_from = c("varGroup", "varNam", "study1", "study2", "study3"), values_from = n, names_sep=" - ") %>%
  ungroup %>%
  t %>%
  as.data.frame %>%
  janitor::row_to_names(1) %>%
  replace(is.na(.), 0) %>%
  tibble::rownames_to_column(., var = "variable") %>%
  separate(variable, c("concept", "variable", "study1", "study2", "study3"), " - ") %>%
  mutate(across(c(study1, study2, study3), as.numeric)) %>%
  #select(-study1, -study2, -study3) %>%
  arrange(concept, unspecific, `Interaction Only`) %>% 
  mutate_all(~recode(.x, `0`="", ` 1`="✓")) %>%
  relocate(starts_with("study"), .after = last_col()) %>%
  rename_with(., ~ gsub("study", "Study ", .x, fixed = TRUE)) %>%
  kbl(.,
      escape = FALSE,
      booktabs = T,
      align = c("l", "l", rep("c", ncol(.)-2)),
      digits=2,
      caption = "Variables by Interaction Types and Study Availability") %>%
  add_header_above(c(" " = 2, "Contact Type" = 2, "Study Availability" = 3)) %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

```{r missingness filtering, include=FALSE}
missInfo <- function(full = NA, reduced = NA){
  missTab <- data.frame(
    nFull = nrow(full),
    nRed = nrow(reduced),
    pptFull = length(unique(full$PID)),
    pptRed = length(unique(reduced$PID)),
    timeFull = length(unique(full$TID)),
    timeRed = length(unique(reduced$TID))
  ) %>%
    mutate(
      nDif = nFull-nRed,
      nDifPerc = nDif/nFull*100,
      pptDif = pptFull-pptRed,
      pptDifPerc = pptDif/pptFull*100,
      timeDif = timeFull-timeRed,
      timeDifPerc = timeDif/timeFull*100
    ) %>%
    select(
      nFull, nRed, nDif, nDifPerc,
      pptFull, pptRed, pptDif, pptDifPerc,
      timeFull, timeRed, timeDif, timeDifPerc
    )
    missTab
}

missS1 <- missInfo(full = dtS1$full, reduced = dtS1Red)
missS2 <- missInfo(full = dtS2$full, reduced = dtS2Red)
missS3 <- missInfo(full = dtS3$full, reduced = dtS3Red)

missS123 <- rbind(
  missS1,
  missS2,
  missS3
) %>%
  mutate(study = c(1, 2, 3)) %>%
  select(study, everything())

missS123 %>%
  kbl(.,
      escape = FALSE,
      booktabs = TRUE,
      align = "c", #c("l", rep("c", ncol(.)-1)),
      col.names = c("Study", rep(c("Full","Reduced","$\\Delta$", "%"), 3)),
      digits=2,
      caption = "Missingness Info by Study") %>%
  add_header_above(c(" " = 1, "Measurements" = 4, "Participants" = 4, "Timepoints" = 4)) %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

\subsection{Data}
We used a data set following migration experiences collected by \citet[][]{Kreienkamp2022b}. The data set consists of three studies that followed migrants who had recently arrived in the Netherlands in their daily interactions with the Dutch majority group members. After a general migration-focused pre-questionnaire, participants were invited twice per day to report on their (potential) interactions with majority group members for at least 30 days. The short ESM surveys were sent out at around lunch (12pm) and dinner time (7pm). After the 30 day study period participants filled in a post-questionnaire that mirrored the pre-questionnaire. Participants received either monetary compensation or partial course credits based on the number of surveys they completed. For our empirical example we focus on the variables that were collected during the ESM surveys and were available in all three studies. Full methodological details are available in the empirical article by \citet[][]{Kreienkamp2022b}.

\subsubsection{Sample}
The original studies included `r sum(missS123$pptFull)` participants ($N_{S1}=$ `r missS123$pptFull[missS123$study==1]`, $N_{S2}=$ `r missS123$pptFull[missS123$study==2]`, $N_{S3}=$ `r missS123$pptFull[missS123$study==3]`) with a total of `r format(sum(missS123$nFull), big.mark=",")` measurements. The studies differed substantially in the maximum length of participation ($\text{max}(t_{S1})=$ `r missS123$timeFull[missS123$study==1]`, $\text{max}(t_{S2})=$ `r missS123$timeFull[missS123$study==2]`, $\text{max}(t_{S3})=$ `r missS123$timeFull[missS123$study==3]`). This was likely due to the option to continue participation without compensation in the later two studies. To make the three studies comparable in participation and time frames, we iteratively removed all measurement occasions and participants that had more than 45\% missingness \citep[which was in line with the general rcecommendation for data that might still need to rely on imputations for later model testing][]{Madley-Dowd2019}. This procedure let to a final sample of `r sum(missS123$pptRed)` participants, who jointly produced `r format(sum(missS123$nRed), big.mark=",")` measurements. Importantly, both the participant repsonse-patterns and the time frame were now a lot more comparable ($\text{max}(t_{S1})=$ `r missS123$timeRed[missS123$study==1]`, $\text{max}(t_{S2})=$ `r missS123$timeRed[missS123$study==2]`, $\text{max}(t_{S3})=$ `r missS123$timeRed[missS123$study==3]`). For a full overview of the data reduction procedure and study-specific exclusions see Online Supplemental Material A.

\subsubsection{Variables}
We included 12 variables that were available across all three studies and captured information about the participant's interactions, as well as cognitive-, emotional-, and motivational self in relationship with the majority group. We chose these two aspects in particular because (1) the interaction-specific information exemplified the structural missingness issue of modern ESM data and (2) the motivational, emotional, and cognitive experience offered a diverse conceptualization of migration experience (beyond behavioral measurements) that is becoming more common in the literature \citep[][]{Kreienkamp2022d}. Full methodological details are available in Online Supplemental Material A, but basic item information, descriptives, and correlations are available in \tblref{tab:descrLong}.

```{r descrWide, include=FALSE}
variableLab <- c(
  "AttitudesDutch" = "Outgroup Attitude",
  "AttitudesPartner" = "Int: Partner Attitude",
  "DaytimeNeedFulfillment" = "Need Fulfillment",
  "exWB" = "Well-Being",
  "InteractionContextAccidental" = "Int: Accidental",
  "InteractionContextCooperative" = "Int: Cooperative",
  "InteractionContextRepresentativeNL" = "Int: Representative",
  "InteractionContextvoluntary" = "Int: Voluntary",
  "KeyNeedDueToPartner" = "Int: Need Fulfillment Partner",
  "KeyNeedFulfillment" = "Int: Need Fulfillment",
  "qualityMeaning" = "Int: Meaningful",
  "qualityOverall" = "Int: Quality"
) %>% enframe(., name = "variable", value = "label")

dtAll <- rbind(
  dtS1Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S1") %>%
    mutate(across(!TID & !study, as.numeric)),
  dtS2Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S2"),
  dtS3Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S3")
) %>%
  group_by(study, PID) %>%
  mutate(ID = cur_group_id()) %>%
  ungroup %>%
  mutate(date = as.Date(gsub(" .*", "", TID)),
         week = strftime(date, format = "%Y-W%V")) %>%
  select(ID,
         PID,
         study,
         date,
         week,
         TIDnum,
         any_of(varNamS123PCA)) %>%
  arrange(ID, TIDnum) %>%
  select_if(~ sum(!is.na(.)) > 1) %>% # only include variables that have any data (i.e., not all NA)
  as.data.frame

allMlCor <-
  MlCorMat(
    data = dtAll,
    id = "ID",
    selection = c(
      "InteractionContextAccidental", 
      "InteractionContextvoluntary", 
      "InteractionContextCooperative",
      "InteractionContextRepresentativeNL",
      "qualityMeaning",
      "qualityOverall", 
      "KeyNeedFulfillment", 
      "KeyNeedDueToPartner", 
      "AttitudesPartner",
      "DaytimeNeedFulfillment",
      "AttitudesDutch",
      "exWB"
    ),
    labels = c(
      "Int: Accidental", 
      "Int: Voluntary", 
      "Int: Cooperative",
      "Int: Representative",
      "Int: Meaningful",
      "Int: Quality", 
      "Core Need", 
      "Core Need Due to Partner", 
      "Attitude Partner",
      "Daytime Core Need",
      "Outgroup Attitude",
      "Well-being"
    )
  )

# Sample Descriptives Table --- wider version
allMlCor %>%
  t %>%
  as.data.frame %>%
  kbl(.,
      format = "latex",
      caption = "Correlation Table and Descriptive Statistics",
      booktabs = TRUE,
      align = c("l", rep("c", ncol(.) - 1))) %>%
  kable_classic() %>%
  add_header_above(
    .,
    c(
      "",
      "Correlations" = ncol(allMlCor),
      "Descriptives" = nrow(allMlCor) - ncol(allMlCor)
    )
  ) %>%
  footnote(
    general = c(
      "Upper triangle: Between-person correlations;",
      "Lower triangle: Within-person correlations;",
      "*** p < .001, ** p < .01,  * p < .05"
    )
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  gsub("\\begin{table}", "\\begin{sidewaystable}\n\\centering", ., fixed = TRUE) %>%
  gsub("\\end{table}", "\\end{sidewaystable}", ., fixed = TRUE) %>%
  save_kable("Tables/descrWide.tex")
```

```{r descrLong, include=FALSE}
# Sample Descriptives Table --- longer version
allMlCor %>%
  as.data.frame %>%
  kbl(.,
      format = "latex",
      caption = "Correlation Table and Descriptive Statistics",
      booktabs = TRUE,
      align = "c", #c("l", rep("c", ncol(.) - 1)),
      escape = FALSE,
      col.names = c(
        #"",
        "\\makecell{Int: \\\\ Accidental}",
        "\\makecell{Int: \\\\ Voluntary}", 
        "\\makecell{Int: \\\\ Cooperative}",
        "\\makecell{Int: \\\\ Representative}",
        "\\makecell{Int: \\\\ Meaningful}",
        "\\makecell{Int: \\\\ Quality}", 
        "Core Need", 
        "\\makecell{Core Need \\\\ Due to Partner}", 
        "\\makecell{Attitude \\\\ Partner}",
        "\\makecell{Daytime \\\\ Core Need}",
        "\\makecell{Outgroup \\\\ Attitude}",
        "Well-being"
      )) %>%
  kable_classic() %>%
  pack_rows("Correlations", 1, ncol(allMlCor)) %>%
  pack_rows("Descriptives", ncol(allMlCor)+1, nrow(allMlCor)) %>%
  footnote(
    general = c(
      "Upper triangle: Between-person correlations;",
      "Lower triangle: Within-person correlations;",
      "*** p < .001, ** p < .01,  * p < .05"
    )
  ) %>%
  kable_styling(latex_options = "scale_down") %>%
  gsub("\\begin{table}", "\\begin{sidewaystable}\n\\centering", ., fixed = TRUE) %>%
  gsub("\\end{table}", "\\end{sidewaystable}", ., fixed = TRUE) %>%
  save_kable("Tables/descrLong.tex")
```


\subsection{Analysis and Results}
To perform the main analysis on our selected variables, we showcase the extract--reduce--cluster steps in sequential order. For each of the steps fully reproducible and annotated code is available in Online Supplemental A as well as in the accompanying OSF- and GitHub repositories (REFERENCES). 

\subsubsection{Feature extraction}
```{r featMiss, include=FALSE}
featureExclusion <- c(
  "mean",
  "sd",
  "rmssd",
  "ar02",
  "edf_ar",
  "_n"
)

pMissFull <- pMissFeat(
  all = featFull$features %>% select(-ends_with(featureExclusion)),
  contact = featFullContact$features,
  nocontact = featFullNoContact$features,
  title = "Feature-wise Missingess Across all Studies"
)
```

For the features, we selected and extracted one of each of the features proposed in \tblref{tab:esmFeatures}. We particularly chose the more robust median and median absolute deviation (MAD) for central tendency and variability. Within the variability structure, we chose the slightly more intuitive mean absolute change (MAC) for stability and stayed with the common lag--1 autocorrelation for inertia. For the trend summaries, we chose the overall linear regression slope to describe the linear trend and we summarized the nonlinear trend with the estimated degrees of freedom of an empty GAM spline model (edf) --- the edf summarizes the \textit{wiggliness} of the spline trend line. We chose these features in particular because we consider them to be the most broadly applicable features. We also extracted the participant's number of completed ESM measurements to ensure that the clusters are comparable in that regard. We provide an R-function that automatically extracts and prepares a large selection of the time series feature operationalizations presented in \tblref{tab:esmFeatures} in our GitHub repository (see the \texttt{featureExtractor} function).

After the feature extraction, we found that about `r format(round(pMissFull$pmiss %>% filter(set == "All") %>% select(pMissTotal) %>% pull, 2), nsmall=2)`\% of the extracted features are missing across the `r ncol(featFull$features %>% select(-ends_with(featureExclusion)))-1` features per participant. This might, for example, happen if participants do not have two subsequent measurements with outgroup interactions, so that an autocorrelation with lag-1 cannot be calculated for the contact-specific variables. The small number of missing values indicates that the feature-based approach indeed largely avoids the structural missingness issue. The few missing values can, however, be an issue for some feature reduction or feature clustering algorithms. We, thus, impute the missing feature values with a single predictive mean matching imputation using the MICE library. 

\subsubsection{Feature reduction}
```{r pca, include=FALSE} 
# prepare data
raw_data <- featData
z_data <- featFullImp$featuresImpZ %>% select(-ends_with(featureExclusion), ends_with("_n"))
scaled_data <- featFullImp$featuresImpZMat %>% select(-ends_with(featureExclusion)) 

res.pca <- prcomp(scaled_data, scale. = FALSE)
summary(res.pca)

res.varExplained <- data.frame(
  Explained_Variance = cumsum(summary(res.pca)[["importance"]]['Proportion of Variance',]),
  Components = seq(1, length(cumsum(summary(res.pca)[["importance"]]['Proportion of Variance',])), 1)
)
fviz_eig(res.pca, addlabels = TRUE)
ggplot(data = res.varExplained, aes(x = Components, y = Explained_Variance)) +
  geom_point() +
  geom_line() +
  theme_Publication()

pca_cutoff <- min(which(cumsum(summary(res.pca)[["importance"]]['Proportion of Variance',]) >= 0.8))

plot_ly(
  res.pca$rotation %>% as.data.frame,
  x = ~ PC1,
  y = ~ PC2,
  z = ~ PC3,
  size = 2,
  type = 'scatter3d',
  mode = 'markers',
  text = ~ paste('Variable:', rownames(res.pca$rotation))
) %>%
  layout(title = 'Variables in reduced space')
plot_ly(
  res.pca$x %>% as.data.frame,
  x = ~ PC1,
  y = ~ PC2,
  z = ~ PC3,
  size = 2,
  type = 'scatter3d',
  mode = 'markers',
  text = ~ paste('Participant:', rownames(res.pca$x))
) %>%
  layout(title = 'Participants in reduced space')

# extract coordinates for the individuals (PC-scores)
pca.out <- as.data.frame(res.pca$x[,1:pca_cutoff])
```

For the feature reduction, we chose the common \textit{principal component analysis} (PCA). Some of the more tailor-made feature selection algorithms can be more accurate in reducing the feature dimensionality and might retain feature importance information more directly, depending on the specific data structure. However, PCAs have the distinct benefit that they are well-established within the psychometric literature and can broadly be applied to a wide variety of studies in an automatize manner. As our aim is to present a general illustration that can also be adopted for more general data descriptive uses, we present the workflow using a PCA here but we encourage users to consider more specialized methods as well.

To use the principal component analysis with our extracted time series features, we first standardize all features across participants to ensure that all features are weighted equally. We then enter all `r ncol(scaled_data)` features into the analysis. The PCA will use linear transformations in such a way that the first component captures the most possible variance of the original data (e.g., by finding a vector that maximizes the sum of squared distances). The following components will then use the same method to iteratively explain the most of the remaining variance while also ensuring that the components are linearly uncorrelated. In practice, this meant that the PCA decomposed the `r ncol(scaled_data)` features into `r ncol(scaled_data)` principal components but now (because of the uncorrelated linear transformations) the first few principal components will capture a majority of the variance. We can then decide how much information (i.e., variance) we are willing to sacrifice for a reduced dimensionality. A common rule of thumb is to use the principal components that jointly explain 70--90\% of the original variance (i.e., cumulative percentage explained variance). For our illustration we select the first `r pca_cutoff` principal components that explain 80\% of the variance in the original `r ncol(scaled_data)` features (reducing the dimensionality by `r format(round((1-pca_cutoff/ncol(scaled_data))*100, 2), nsmall=2)`\%). For the extracted principal components we save the `r pca_cutoff` PC-scores for each participant (i.e., the participants' coordinates in the reduced dimensional space).

We would like to comment on two practical matters when using principal components --- the amount of dimensionality reduction and the interpretation of the principal components. As for the expected dimensionality reduction, given its methodology, PCAs tend to 'work better' at reducing dimensions with (highly) correlated variables. Thus, with a set of very homogeneous variables and features users will need less principal components to explain a large amount of variance, while a more diverse set of variables and features will tend to require more principal components to capture the same amount of variance. Our `r pca_cutoff` principal components are still a relatively high number of variables but this is not surprising as we chose a diverse conceptualization and a diverse set of time series features. As for the interpretability, PCA allows users to extract information on the meaning of the principal components. In particular, because the principal components are linear combination of original features, users can extract the relative importance of each feature for the extracted principal components (i.e., the eigenvectors). While this can be useful in understanding the variance in the original data or help with manual feature selection, we use the PCA purely to reduce the dimensionality for the clustering step. Instead of relying on the principal components, we use the original features of interest to interpret the later extracted clusters. We particularly advocate for such an approach if the original features were chosen for their psychological meaning in understanding the time series and broader phenomenon of interest.

\subsubsection{Feature clustering}
```{r featureClustering, include=FALSE}
# load libraries
library(factoextra)
library(NbClust)

# check variance differences
max(pca.out %>% summarize_all(var))/min(pca.out %>% summarize_all(var))
pca.out %>%
  summarize_all(sd) %>%
  pivot_longer(everything(), names_to = "PC", values_to = "sd") %>%
  ggplot(., aes(x=reorder(PC, sd), y=sd, label=sd)) +
  geom_bar(stat='identity', width=.5) +
  coord_flip() +
  theme_Publication()

# prep data
scaled_pc <- pca.out #%>% scale

# Elbow method
fviz_nbclust(scaled_pc, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = 2)+
  labs(subtitle = "Elbow method") +
  theme_Publication()
# Silhouette method
fviz_nbclust(scaled_pc, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method") +
  theme_Publication()
```

```{r k-means modeling based on selected k, include=FALSE}
kmeansOut <- kmeans(scaled_pc, centers = 2, nstart = 100)
```

```{r Plot Convex Hulls, include=FALSE}
fviz_cluster(kmeansOut, geom = "point", data = scaled_pc, ggtheme = theme_Publication())
fviz_cluster(kmeansOut, geom = "point", data = z_data, ggtheme = theme_Publication())
```

\Question{\textcolor{cyan}{It is good practice to standardize the input of the k-means algorithm. However, I think that does not make sense for PCs because the PCA is based on the idea of decomposing variances (and capturing the most variance in PC1, ...). If I actually standardize the PC-score, we get really weird k-means results. The only discussion of this I could find was a tutorial that DID NOT standardize the PCs: \url{https://medium.com/more-python-less-problems/principal-component-analysis-and-k-means-clustering-to-visualize-a-high-dimensional-dataset-577b2a7a5fe2} 
I had originally written: 
"To ensure that the clustering is weighing the principal components equally (as opposed to their variance), we standardize the particpants' PC-scores." 
but I went back to the 'raw' PC-scores now. But we should discuss this.}}

For our illustration of the feature clustering procedure, we use the generalizable and efficient centroid-based k-means clustering. The procedure needs the user to make a selection on the optimal number of clusters in order to run the main algorithm. We use the Elbow and the Silhouette method, which both suggest two clusters to be the optimal solution (see Supplemental Material A). We then entered the participants' PC-scores into the k-means algorithm. We used the recommended Hartigan and Wong algorithm \citep{hartigan1979} with 100 random initial centroid positions to avoid convergence to a sub-optimal solution (i.e., local minima). The k-means analysis assigned `r table(kmeansOut$cluster)[1]` participants to cluster 1 and `r table(kmeansOut$cluster)[2]` participants to cluster 2. 

In order to interpret the two clusters, we compare the two clusters in their features as well as in their raw time series. We first inspect the clusters with a focus on the included variables (see \fgrref[A]{fig:clusterFeatVar}). We (1) see that for some variables the features are generally stronger in separating the clusters (e.g., 'how cooperative an interaction was' compared to 'attitudes towards the Dutch'). Additionally, we see that (2) within the variables some features are better at distinguishing clusters (e.g., median of well-being vs. MAC of well-being).We then inspect the clusters with a focus on the features (see \fgrref[B]{fig:clusterFeatVar}). While this is the same data as for the variable focus, we can see more clearly that some features are better at distinguishing the clusters across variables (e.g., mean and median compared to auto correlations). This offers some information on which features were are most important in understanding the two extracted groups. 

\begin{figure}[!ht] %hbtp
  \caption{Cluster Group Comparisons based on Features and Variables}
  \label{fig:clusterFeatVar}
  \centering\includegraphics[width=\textwidth]{figures/clusterFeatVarComb.pdf}
  \caption*{Note: \\
  Within the "(B) Feature Focus" subplot, the 'n' and 'Discrimination' comparison variables were not part of the original time series clustering.}
\end{figure}

We can then combine the two focus approaches to assess the developments between the two groups more holistically (see \fgrref{fig:clusterTs}). Immediately striking are the mean differences, where participants in the second cluster had more meaningful and fulfilling outgroup interactions also consistently reported more voluntary and cooperative interactions but less accidental and involuntary interactions. The same cluster also reported an increase in need fulfilling interaction over the 30 day period and an increase in interactions that were representative of the outgroup. Whereas the other cluster showed a decrease in voluntary, cooperative, and positive interactions over the 30 days. This 'deterioration' cluster also saw a decrease in general need fulfillment and experienced well-being over the 30 days (see \fgrref[B]{fig:clusterTs}). We also see that while interaction representativeness, outgroup attitudes, well-being are relatively stable for both clusters, the deteriorating cluster also showed substantially higher variablity and instability over time (also see \fgrref[A]{fig:clusterTs}). 

\Question{\textcolor{cyan}{Talk about early warning signals here? E.g., divergences in means over time could be invesitigated with targeted studies.}}

We can also assess the clusters across any other person-level variable. This out-of-feature comparison allows us to check for data artifacts, as well as check whether the developmental clusters are associated with important social markers and individual differences. To illustrate artifact checks, we added the number of measurements into the comparison and find that the participants in the detereoration cluster on average completed more ESM surveys and reported on more intergroup interactions than the cluster with the more positive interactions (see \fgrref[B]{fig:clusterTs}). While this difference could indicate that the clusters might not entirely be comparable in the response patterns, we can find some relief in our data exclusion procedures during which we ensured that the general time frame and completion rates were not too dissimilar. To illustrate the utility of individual differences, we compare the two samples in terms of the participants' self-reported discrimination experiences in the Netherlands (measured during the post-measurement). \fgrref[B]{fig:clusterTs} illustrates that participants in the deteriorating cluster reported substantially higher levels of everyday discrimination. Thus, both intensive longitudinal and cross-sectional variables that were not included in the original clustering step can be used to explore and understand the cluster differences in more detail.

\begin{figure}[!ht] %hbtp
  \caption{Cluster Group Comparisons over time}
  \label{fig:clusterTs}
  \centering\includegraphics[width=\textwidth]{figures/clusterTsComb.pdf}
  \caption*{Note: \\
  Subplot (A) displays the variable cluster means at every measurement occasion. Subplot (B) shows the GAM spline for each cluster across the measurement occasions.}
\end{figure}

```{r average feature by cluster, include=FALSE}
# CLUSTER VARIABLE LABELS
variableLab <- c(
  "AttitudesDutch" = "Outgroup Attitude",
  "AttitudesPartner" = "Int: Partner Attitude",
  "DaytimeNeedFulfillment" = "Need Fulfillment",
  "exWB" = "Well-Being",
  "InteractionContextAccidental" = "Int: Accidental",
  "InteractionContextCooperative" = "Int: Cooperative",
  "InteractionContextRepresentativeNL" = "Int: Representative",
  "InteractionContextvoluntary" = "Int: Voluntary",
  "KeyNeedDueToPartner" = "Int: Need Fulfillment Partner",
  "KeyNeedFulfillment" = "Int: Need Fulfillment",
  "qualityMeaning" = "Int: Meaningful",
  "qualityOverall" = "Int: Quality"
) %>% enframe(., name = "variable", value = "label")

# ADD VALIDATION VARS:
validationNam <- c(
  "gender.pre",
  "IntGrAnx01.pre",
  "IntGrAnx02.pre",
  "IntGrAnx03.pre",
  "IntGrAnx04.pre",
  "IntGrAnx05.pre",
  "IntGrAnx06.pre",
  "IntGrAnx07R.pre",
  "IntGrAnx08R.pre",
  "IntGrAnx09R.pre",
  "IntGrAnx10R.pre",
  "IntGrAnx11R.pre",
  "IntGrAnx12R.pre",
  "SWL01.pre",
  "SWL02.pre",
  "SWL03.pre",
  "SWL04.pre",
  "SWL05.pre",
  "EvDayDiscr01.post",
  "EvDayDiscr02.post",
  "EvDayDiscr03.post",
  "EvDayDiscr04.post",
  "EvDayDiscr05.post",
  "EvDayDiscr06.post",
  "EvDayDiscr07.post",
  "EvDayDiscr08.post",
  "EvDayDiscr09.post"
)

# EXTRACT OUT-OF-CLUSTER VALIDATION VARIABLES
validation <- rbind(
  dtS1Red %>% select(any_of(idVars), any_of(validationNam)) %>% mutate(study = "S1", gender.pre = (gender.pre-2)*-1),
  dtS2Red %>% select(any_of(idVars), any_of(validationNam)) %>% mutate(study = "S2"),
  dtS3Red %>% select(any_of(idVars), any_of(validationNam)) %>% mutate(study = "S3")
) %>% 
  group_by(study, PID) %>%
  mutate(ID = cur_group_id()) %>%
  ungroup %>%
  group_by(study, ID) %>%
  summarise(
    gender = gender.pre,
    IntGrAnx.pre = rowMeans(across(starts_with("IntGrAnx")), na.rm = TRUE),
    SWL.pre = rowMeans(across(starts_with("SWL")), na.rm = TRUE),
    EvDayDiscr.post = rowMeans(across(starts_with("EvDayDiscr")), na.rm = TRUE)
  ) %>%
  ungroup %>%
  distinct %>% 
  mutate(
    gender.z = scale(gender, scale = TRUE),
    IntGrAnx.pre.z = scale(IntGrAnx.pre, scale = TRUE),
    SWL.pre.z = scale(SWL.pre, scale = TRUE),
    EvDayDiscr.post.z = scale(EvDayDiscr.post, scale = TRUE)
  ) %>%
  select(
    ID,
    #gender.z,
    #IntGrAnx.pre.z,
    #SWL.pre.z,
    EvDayDiscr.post.z
  )
val <- validation %>% select(-ID) %>% names
validationFeature <- validation
for(feature in variableLab$variable) {
  validationFeature <- validationFeature %>% bind_cols(., select(., setNames(val, paste(feature, val, sep = "_"))))
}
validationFeature <- validationFeature %>% select(-any_of(val))


# CLUSTER VARIABLE LABELS
variableLab <- c(
  "AttitudesDutch" = "Outgroup Attitude",
  "AttitudesPartner" = "Int: Partner Attitude",
  "DaytimeNeedFulfillment" = "Need Fulfillment",
  "exWB" = "Well-Being",
  "InteractionContextAccidental" = "Int: Accidental",
  "InteractionContextCooperative" = "Int: Cooperative",
  "InteractionContextRepresentativeNL" = "Int: Representative",
  "InteractionContextvoluntary" = "Int: Voluntary",
  "KeyNeedDueToPartner" = "Int: Need Fulfillment Partner",
  "KeyNeedFulfillment" = "Int: Need Fulfillment",
  "qualityMeaning" = "Int: Meaningful",
  "qualityOverall" = "Int: Quality"
) %>% enframe(., name = "variable", value = "label")
validationLab <- c(
  "gender.z" = "Val: Gender",
  "SWL.pre.z" = "Val: Sat. with life",
  "IntGrAnx.pre.z" = "Val: Intergroup Anxiety",
  "EvDayDiscr.post.z" = " Discrimination"
) %>% enframe(., name = "variable", value = "label")

featCluster <- data.frame(ID = as.numeric(names(kmeansOut$cluster)), cluster = kmeansOut$cluster) %>%
  merge(., z_data) %>%
  group_by(cluster) %>%
  summarise(across(everything(), list(mean = mean, se = se), na.rm=TRUE)) %>%
  ungroup %>% 
  select(-starts_with("ID")) %>%
  pivot_longer(
    cols = -c(cluster),
    names_to = c("variable", ".value"),
    names_pattern = "(.*)_(.*)"
  ) %>% 
  mutate(
    nam = variable,
    variable = gsub("\\_.*", "", nam),
    feature = gsub("^.*?\\_", "", nam)
  ) %>% 
  merge(., variableLab) %>%
  select(
    cluster,
    nam,
    variable,
    label,
    feature,
    everything()
  )

featClusterVal <- data.frame(ID = as.numeric(names(kmeansOut$cluster)), cluster = kmeansOut$cluster) %>%
  merge(., z_data) %>%
  merge(., validationFeature) %>%
  group_by(cluster) %>%
  summarise(across(everything(), list(mean = mean, se = se), na.rm=TRUE)) %>%
  ungroup %>% 
  select(-starts_with("ID")) %>%
  pivot_longer(
    cols = -c(cluster),
    names_to = c("variable", ".value"),
    names_pattern = "(.*)_(.*)"
  ) %>% 
  mutate(
    nam = variable,
    variable = gsub("\\_.*", "", nam),
    feature = gsub("^.*?\\_", "", nam)
  ) %>% 
  merge(., variableLab) %>% 
  mutate(feature = stri_replace_all_regex(
    feature,
    pattern=validationLab$variable,
    replacement=validationLab$label,
    vectorize=FALSE
  )) %>% 
  select(
    cluster,
    nam,
    variable,
    label,
    feature,
    everything()
  )

rawCluster <- data.frame(ID = as.numeric(names(kmeansOut$cluster)), cluster = kmeansOut$cluster) %>%
  merge(., raw_data) %>%
  select(
    ID,
    TIDnum,
    cluster,
    any_of(varNamS123)
  ) %>%
  reshape2::melt(id.vars = c("cluster", "ID","TIDnum")) %>% 
  merge(., variableLab)
```

```{r clusterVariableGrid, include=FALSE}
clusterVariableGrid <- featCluster %>% 
  ggplot(., aes(x = feature, y = mean, group = cluster, color = as.factor(cluster))) +
  geom_point() +
  geom_line() +
  #geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width=.1, position=position_dodge(0.1), size=0.5, alpha=0.4) +
  coord_flip() +
  labs(
    x = "Features",
    y = "Standardized Mean",
    color = "Cluster"
  ) +
  facet_wrap(~ label) +
  theme_Publication()
clusterVariableGrid
```

```{r clusterFeatureGrid, include=FALSE}
clusterFeatureGrid <- featClusterVal %>% 
  ggplot(., aes(x = label, y = mean, group = cluster, color = as.factor(cluster))) +
  geom_point() +
  geom_line() +
  #geom_ribbon(aes(ymin = mean-se, ymax = mean+se), alpha = 0.2) +
  #geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width=.1, position=position_dodge(0.1), size=0.5, alpha=0.4) +
  coord_flip() +
  labs(
    x = "Features",
    y = "Standardized Mean",
    color = "Cluster"
  ) +
  facet_wrap(~ factor(feature, levels = unique(featClusterVal$feature)), ncol=4) +
  theme_Publication()
clusterFeatureGrid
```

```{r clusterFeatVarComb, include=FALSE}
clusterFeatVarComb <- ggpubr::ggarrange(
  clusterVariableGrid + theme(axis.title.x = element_blank()),
  clusterFeatureGrid,
  ncol = 1,
  labels = c("(A) Variable Focus", "(B) Feature Focus"),
  common.legend = TRUE,
  legend = "bottom",
  align = "v"
) 
ggsave("Figures/clusterFeatVarComb.pdf", clusterFeatVarComb, width = 12, height = 12)
```


```{r clusterTS, include=FALSE}
clusterTS <- rawCluster %>% 
  ggplot(., aes(x=TIDnum, y=value, group=cluster, color=as.factor(cluster))) +
  geom_line(aes(x=TIDnum, y=value, group=ID, color=as.factor(cluster)), alpha=0.05) +
  stat_summary(fun=mean, geom="line") +
  facet_wrap(~ label, scales = "free_y") +
  labs(
    x = "Time ID",
    y = "Response (Mean Cluster Response Bold)",
    color = "Cluster"
  ) +
  theme_Publication() 
clusterTS
```

```{r clusterTSTrend, include=FALSE}
clusterTSTrend <- rawCluster %>% 
  ggplot(., aes(x=TIDnum, y=value, group=cluster, color=as.factor(cluster))) +
  geom_line(aes(x=TIDnum, y=value, group=ID, color=as.factor(cluster)), alpha=0.05) +
  geom_smooth(aes(group=cluster)) +
  #stat_summary(fun=mean, geom="line") +
  facet_wrap(~ label, scales = "free_y") +
  labs(
    x = "Time ID",
    y = "Response (Mean Cluster Response Bold)",
    color = "Cluster"
  ) +
  theme_Publication()
clusterTSTrend
```

```{r clusterTsComb, include=FALSE}
clusterTsComb <- ggpubr::ggarrange(
  clusterTS + theme(axis.title.x = element_blank()),
  clusterTSTrend,
  ncol = 1,
  labels = c("(A) Timepoint Means", "(B) Average Spline Trends"),
  common.legend = TRUE,
  legend = "bottom"
)
ggsave("Figures/clusterTsComb.pdf", clusterTsComb, width = 12, height = 12)
```
