% define document type (i.e., template. Here: A4 APA manuscript with 12pt font)
\documentclass[man, 12pt, a4paper, mask, floatsintext]{apa7}

% change margins (e.g., for margin comments):
%\usepackage{geometry}
% \geometry{
% a4paper,
% marginparwidth=30mm,
% right=50mm,
%}

% add packages
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage[style=apa, sortcites=true, sorting=nyt, backend=biber, natbib=true, uniquename=false, uniquelist=false, useprefix=true]{biblatex}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{setspace,caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{fourier}
\usepackage{stackengine}
\usepackage{scalerel}
\usepackage{fontawesome5}
\usepackage[normalem]{ulem}
% \usepackage{longtable}
\usepackage{amsmath, nccmath}
\usepackage{mdframed}
\usepackage{ntheorem}
\usepackage{afterpage}
\usepackage{float}
\usepackage{array}
\usepackage{censor}
\usepackage{pdflscape}
\usepackage{lscape}
\usepackage{pdfpages}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{tabu}


% make warning with red triangle
\newcommand\Warning[1][2ex]{%
  \renewcommand\stacktype{L}%
  \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries !}}{#1}}%

% make question with red triangle
\newcommand\Question[1][2ex]{%
  \renewcommand\stacktype{L}%
  \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries ?}}{#1}}%
  
% add definition sections
\theoremstyle{break}
\newtheorem{definition}{Definition}

% add hypothesis sections
\theoremstyle{plain}
\theoremseparator{:}
\newtheorem{hyp}{Hypothesis}

\newtheorem{subhyp}{Hypothesis}
   \renewcommand\thesubhyp{\thehyp\alph{subhyp}}

% add quote section
\usepackage{csquotes}

% framed box section
\usepackage{framed}
\emergencystretch=1em

% formatting links in the PDF file
\hypersetup{
pdfpagemode={UseOutlines},
bookmarksopen=true,
bookmarksopenlevel=0,
hypertexnames=false,
colorlinks   = true, %Colours links instead of ugly boxes
urlcolor     = blue, %blue,Colour for external hyperlinks
linkcolor    = black, %blue, Colour of internal links
citecolor   = black, % cyan, Colour of citations
pdfstartview={FitV},
unicode,
breaklinks=true,
}

% ref labels
\newcommand{\fgrref}[2][]{\hyperref[#2]{Figure \ref*{#2}#1}}
\newcommand{\tblref}[2][]{\hyperref[#2]{Table \ref*{#2}#1}}
\newcommand{\appref}[2][]{\hyperref[#2]{Appendix \ref*{#2}#1}}

% custom open science badge height
\newlength{\badgeheight}
\setlength{\badgeheight}{1em}

% prevent multipage footnotes
\interfootnotelinepenalty=10000

% language settings
\DeclareLanguageMapping{american}{american-apa}

% add reference library file
\addbibresource{referencesZotero.bib}

% Title and header
\title{Describe and Explore: Using feature-based time series clustering to identify meaningful structures in intensive longitudinal data}
\shorttitle{feature-based time series clustering}

% Authors
\author[*,1,3]{Jannis Kreienkamp}
\author[1,3]{Laura F. Bringmann}
\author[1,3]{Kai Epstude}
\author[1,3]{Maximilian Agostini}
\author[1,3]{Peter de Jonge}
\author[2,3]{Rei Monden}
\affiliation{\hfill}

\affil[1]{University of Groningen, Department of Psychology}
\affil[2]{Hiroshima University, Graduate School of Advanced Science and Engineering}
\affil[3]{Author order TBD}

\authornote{   
   \addORCIDlink{* Jannis Kreienkamp}{0000-0002-1831-5604}\\
   \addORCIDlink{Laura F. Bringmann}{0000-0002-8091-9935}\\
   \addORCIDlink{Kai Epstude}{0000-0001-9817-3847}\\
   \addORCIDlink{Maximilian Agostini}{0000-0001-6435-7621}\\
   \addORCIDlink{Peter de Jonge}{0000-0002-0866-6929}\\
   \addORCIDlink{Rei Monden}{0000-0003-1744-5447}

We have no known conflict of interest to declare. The authors received no specific funding for this work. Materials and  software is available at \url{https://janniscodes.github.io/migration-trajectories/}  \citep{KreienkampTBD}. Protocols, materials, data, and code are available at \url{https://osf.io/TBA} \citep{KreienkampTBD}. The preregistration of our analysis can be accessed as part of our Open Science Framework repository \citep{KreienkampTBD}.

Correspondence concerning this article should be addressed to Jannis Kreienkamp, Department of Psychology, University of Groningen, Grote Kruisstraat 2/1, 9712 TS Groningen (The Netherlands).  E-mail: \href{mailto:j.kreienkamp@rug.nl}{j.kreienkamp@rug.nl}
}

\leftheader{Kreienkamp}

% Abstract
\abstract{
Psychological time series data has not only become more common but also more diverse and complex. Researchers and practitioners are increasingly interested in non-stationary developmental patterns of multivariate concepts, across different time scales, while contextualized measurements bring about issues of non-imputable missingness. At the same time, clustering analyses offer the potential of reducing these complexities to important and meaningful patterns. Such procedures aid researchers and practitioners in describing and exploring patterns across participants and can promote the creation of more embedded theories and interventions. In this manuscript, we propose feature-based time series clustering as a flexible, transparent, and well-grounded clustering approach that addresses the growing challenges of dimensionality, missingness, and time scales. We introduce the individual feature extraction, -reduction, and -clustering steps and illustrate their utility with an empirical example. We show that time series features such as means, autocorrelations, and linear trends are not only familiar to psychological researchers but are also conceptually meaningful in interpreting time series clusters. Beyond the conceptual and methodological introduction, we also provide practical algorithm overviews and readily available code for data preparation, analysis, and interpretation.

}

\keywords{
    time series analysis, feature-based clustering, intensive longitudinal data, ESM\\
    \vspace{1em}
    \textit{Open Science Practices:}
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/material-color.png}} Open Materials, 
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/data-color.png}} Open Data,
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/code-color.png}} Open Code, \break
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/supplements-color.png}} Open Supplements
}


% set indentation size
\setlength\parindent{1.27cm}

% Start of the main document:
\begin{document}

% add title information (incl. title page and abstract)
\maketitle

% **CHEAT SHEET / LEGEND**
%
% Comments:
% '%' starts a comment in LaTeX (not printed)
% '\todo[inline]{} makes orange boxes in PDF
% '\marginpar{}' notes in margins
% '\footnote{}' footnote
% '\Warning' important note indicator in PDF (triangle with exclamation mark)
% '\Question' question note indicator in PDF (triangle with question mark)
%
% Citation (with Natbib citation style):
% '\citep[e.g.][p. 15]{CitationKey}' citation in parentheses "(e.g., Berry, 2003, p. 15)"
% '\citet{CitationKey}' citation in text "Berry (2003)"
% '\citealt' and '\citealp' alternate citation without parentheses
% '\citeauthor' and '\citeyear' only year or author
% 
% Headings:
% '\part{}' and '\chapter{}' only relevant for multi-part or multi-chapter documents
% '\section{}' heading level 1
% '\subsection{}' heading level 2
% '\subsubsection{}' heading level 3
% '\paragraph{}' heading level 4
% '\subparagraph{}' heading level 5
%
% formatting:
% '\textbf{}' text bold font
% '\textit{}' text italic font
% '\underline{}' text underline
% '\sout{}' text strike out
% '\textsc{}' text small caps
% '\vspace{1em}' add vertical space
% '\hspace{1em}' add horizontal space
% '\\' new line (i.e., line break)
% '\pagebreak' start new page (i.e., page break)
% '\noindent' do not indent current line (e.g., current paragraph)
% 'begin{center}...end{center}' center text or object
%
% Math mode:
% '$\alpha = .8$' mathematical equation inline
% '$$\hat{y} = b_0 + b_1x$$' mathematical equation in its own line
% '\begin{equation}...\end{equation}' multi-line equation
% '\approx' approximate symbol
% '\neq' not equal
% '\bar' mean bar over letter
% '\pm' plus minus sign 
% '^{}' superscript
% '_{}' subscript
% '\fraq{numerator}{denominator}' fraction
% '\sqrt[n]{}' square root
% '\sum_{k=1}^n' sum for 1 through n
%
% Insert things from elsewhere:
% '\input{filename}' inputs the raw (tex) file as a command (e.g., tables and R-Markdown imports)
% '\include{filename}' includes section on new page (incl. possible auxiliary info)
% '\includegraphics[settings]{filename}' add a figure or graph
% '\caption{}' adds a caption to a table or figure
% '\label{}' labels sections, tables, figures, etc. so that they can be referred to.
% '\ref{}' refer to a labelled sections, tables, figures, etc.
% '\begin{enumerate}...\end{enumerate}' numbered list
% '\begin{itemize}...\end{itemize}' bullet-ed list
% '\item' item in list section 
%
% Symbols:
% '\&' and sign
% '\%' percent sign
% '\_' three dotes
% '\#' hash symbol
% ------------------------------------------------------------------

\newlength{\mdfmar}
\setlength{\mdfmar}{1.5em}
\mdfdefinestyle{mdfbox}{
    innerleftmargin = +\mdfmar, %
    innerrightmargin = +2\mdfmar, 
    innertopmargin = +0\mdfmar, 
    innerbottommargin = +1.5\mdfmar, 
    skipabove = -12pt
}

\begin{mdframed}[style=mdfbox]
\noindent\center\textit{Structure}:
\begin{itemize}[nosep]
    \item introduction
    \begin{itemize}[nosep]
        \item increase in number and variety of ESM data + ts clustering to understand complex developmental differences.
        \item why cluster time series? 
        \begin{enumerate}[nosep]
            \item differences between clusters
            \item patterns within clusters
            \item relation to out-of cluster variables
            \item robust theories and interventions
        \end{enumerate}
        \item can't cluster all the raw data (b/c too much data and not directly comparable). Instead, summarize the data and cluster based on the summaries.
        \item ideally these summaries are:
        \begin{enumerate}[nosep]
            \item good at captuing the data
            \item meaningful = help us understand how the clustering arrived at the output (i.e., explainable) + helps us make sense of the final cluster output (i.e., interpretable)
        \end{enumerate}
        \item currently model parameters as summaries.
        \item limitations of parameter summaries: assumptions not always good at capturing data and model parameters not always interpretable
        \item current solution: more complicated models (e.g., address violated assumptions or add different types of parameters)
        \item proposed solution: features as flexible and meaningful approach of summarizing the time series
    \end{itemize}
    \item Feature-based Time Series Clustering
    \begin{itemize}[nosep]
        \item why features: flexible similarity (b/c more variety), less assumptions, easy to calculate/extract, theory-based selection (as per RQ), psychological interpretability, ...
        \item Features in Psychological Time Series
        \begin{itemize}[nosep]
            \item central tendency
            \item variability
            \item (in)stability
            \item self-similarity
            \item linear trend
            \item nonlinearity
        \end{itemize}
        \item Analysis Steps
        \begin{itemize}[nosep]
            \item input variables
            \item feature extraction
            \item (feature reduction)
            \item feature clustering
            \item cluster evaluation
        \end{itemize}
    \end{itemize}
    \item Empirical Illustration
    \item Discussion (summary, limitation, implications, conclusion, etc.)
\end{itemize}
\vspace{1em}
\end{mdframed}
\newpage

Recent years have seen a striking increase in the number and variety of research studies that follow participants' everyday experiences and collect real-world psychological time series \citep[e.g.,][; also see \fgrref{fig:ScopusEsm}]{hamaker2017}\footnote{In psychology, extensive longitudinal data collection methods are often referred to as experience sampling method (ESM), ecological momentary assessment (EMA), or ambulatory assessment (AA) studies. While the terms come for different conceptual backgrounds, they share a focus on collecting data over an extended period of time to capture people's behaviors and experiences as they vary over time and in response to different situations and events. In this article, we will use the experience sampling (ESM) term as it has the strongest footing within the clustering literature.}. These extensive longitudinal data sets come with new forms of heterogeneity, where researchers have to consider differences across large numbers of participants, time points, and variables \citep[e.g.,][]{cattell1966, Monden2015, wardenaar2013}. Oftentimes, however, researchers are interested in precisely this complexity and wish to understand how people differ in their temporal developments across several variables \citep[e.g.,][]{ernst2021}. One relatively recent addition to the toolbox of ESM researchers has been time series clustering --- the idea of inductively grouping participants based on the similarity of their time series \citep{ariens2020}. 

\begin{figure}[!hbtp] %hbtp
  \caption{Scopus ESM Development}
  \label{fig:ScopusEsm}
  \centering\includegraphics[width=\textwidth]{figures/Scopus-ESM-Development.png}
  \caption*{Note: \\
  Search Terms: `( "ecological momentary assessment"  OR  "experience sampling"  OR  "intensive longitudinal")' in Title, Abstract, or Keyword.}
\end{figure}

Time series clustering has a number of conceptual use cases with psychological data. Prime among them is the ability to reduce the time, variable, and person complexity by extracting and organizing participant-level structures. These reduction and structuring qualities can be essential in detecting phenomena and extracting more abstract functional principles \citep[][]{eronen2021a}. These phenomena and principles can be meaningful differences that distinguish participants in different clusters, as well important patterns, trends, and relationships that participants share within a cluster \citep[e.g.,][]{schrodt2000}. Once distinct groups and patterns have been identified, researchers can examine the extent to which these within-group and between-group structures are associated with other variables of interest, such as personality traits, demographic characteristics, or other psychological constructs \citep[e.g.,][]{monden2022}. By detecting meaningful and robust structures and patterns, time series clustering can, thus, be used to inform the development of robust theories as well as targeted interventions and therapies for individuals, for example, with mood disorders and other psychological conditions \citep[e.g.,][]{borsboom2021, eronen2020}.


%Time-series clustering has been extremely common in other empirical fields, including analyses of astronomical, meteorological, and aviation pathways, biological and medical developments, as well as energy and finance patterns \citep{Aghabozorgi2015}. But also for psychological data, time-series clustering has recently become more common \citep[e.g.,][]{ernst2021}.

%The main aim of time series clustering is to inductively group participants in such a way that the dynamics of some variable(s) have maximum similarity for individuals in the same group, and minimum similarity for individuals in different groups \citep[e.g.,][]{Aghabozorgi2015}.

However, while clustering can be incredibly useful, arriving at these clusters critically depends on two core challenges. First, time series need to be made comparable in order to identify key (dis)similarities and second, comparable (dis)similarities need to be accurately distinguishing into different groups \citep[e.g.,][]{Aghabozorgi2015}. In practice, most psychological time series cannot be compared based on the raw data itself. This is the case because in most cases the raw time series include too many data points --- sometimes referred to as the dimensionality curse \citep[e.g.,][]{altman2018} --- and, more importantly, individual time points are oftentimes not directly comparable between participants in psychological data and would lead to misspecifications \citep[e.g., ][]{faloutsos1994}. While such issues can be avoided with transformations for highly regular, controlled, and comparable time series such as EEG data \citep[e.g.,][]{huang1985}, most ESM researchers are usually not interested in directly comparing individual timepoints between participants but are interested in developmental patterns and relationships. 

\begin{figure}[!hbtp] %hbtp
  \caption{Time Series Clustering Taxonomy}
  \label{fig:tsClustTax}
  \centering\includegraphics[width=\textwidth]{figures/TS Cluster Flow/tsClustTax.pdf}
  \caption*{Note: \\
  The taxonomy only exemplifies some of the basic differences between a number of common time series clustering approaches. As such, the taxonomy and the notes are neither exhaustive nor complete in distinguishing different approaches. Additionally, terms and labels are used inconsistently across different types of literature and are chosen to avoid overlapping labels.}
\end{figure}

As a result, most psychological time series are summarized via a numerical representation and these numerical summaries are then comparable and used to cluster participants (e.g., \citealp[]{timmerman2013}; see \fgrref{fig:tsClustTax}). Ideally, the representations that summarize the original time series data should (1) capture the original data accurately without loosing too much information, and (2) should be conceptually meaningful \citep[e.g.,][]{vandermaaten2009}. Extracting accurate and meaningful representations of the time series can be essential for understanding what goes into the clustering algorithm (i.e., assists with explainability) and can be crucial in making sense of the final cluster output \citep[i.e., assists with interpretability; e.g.,][]{Kennedy2021}. 

Thus far, one of the most common representation approaches for clustering of ESM data has been to extract person-specific model parameters --- notably intercepts and slopes from vector autoregression models \citep[VAR; e.g.,][]{ariens2020, bulteel2016, stefanovic2022}. While model parameters are familiar to researchers in the field, the model-based approach has a number of important shortcomings that relate to accuracy and interpretability. In terms of accuracy, the validity of model parameters is restricted by the assumptions of the statistical model. To take the common VAR model as an example, the model does not accurately capture data with unequal time intervals between measurements (i.e., nonequidistant measurements), it does not allow for missing values, and it does not allow for means or variances to change over time \citep[i.e., stationarity assumption;][]{lutkepohl2005}. As for the interpretability of the model parameters, purely data-driven model parameters, such as frequencies from a fourier analysis, are often difficult to interpret conceptually \citep[e.g.,][]{mayor2022}. But even more `meaningful' parameters from models used in theory-testing practices, such as VAR parameters, are commonly restricted to lagged partial correlations \citep[e.g.,][]{bringmann2018c}. Model parameters might, thus, not always accurately capture the original time series and are often restricted in their conceptual interpretation.

%Computationally, the number of parameters compounds with increases in variables and lag orders, which tends to result in efficiency and accuracy reduction for most clustering algorithms and is sometimes referred to as the dimensionality curse \citep{altman2018}\footnote{As an illustration of this scalability issue, consider a VAR model of 8 variables that includes the first three lags (i.e., VAR(3)). For each of the 8 variables, we would have 25 parameters --- one intercept and 24 slopes (8 variables * 3 lags each). That compounds to a total of 200 parameters per person (8 variables * 25 parameters).}.

These shortcomings, however, stand in sharp contrast with the types of data researchers commonly collect and do not align common research interests in the field. Psychological researchers might, for example, collect data based on the occurrence of natural events, which tends to result in non-equidistant measurements \citep[e.g.,][]{myin-germeys2018, hamaker2017} and context-specific missingness that cannot be imputed, for instance, interaction quality perceptions \citep[e.g.,][]{kivela2022, lavori2008}. At the same time, researchers are often specifically interested in non-stationary developments, looking, for example, at how symptom levels and -variations change over time --- often abruptly and non-linearly \citep[e.g.,][; see \appref{app:ChallengesAppendix} for a more extensive discussion of the current challenges that clustering approaches should address]{bringmann2018b, helmich2020a}. There is, thus, a resolute need for more flexible and meaningful time series summaries that can capture diverse measurement and missingness patterns and can capture non-stationary data, including non-linear trends, also across different time scales.

% In short, the time series clustering repertoire should be extended to approaches that can more flexibly allow for diverse measurement and missingness patterns and can capture non-stationary data, including non-linear trends, also across different time scales.

Recent efforts to address the shortcomings of model parameters for time series clustering, have primarily proposed more complicated models aimed at addressing either accuracy or interpretatbility concerns. Accuracy concerns have particularly been addressed by building more advanced models that relax specific assumptions \citep[e.g.,][]{denteuling2021} or by switching to different types of models with less assumptions, including spline models \citep{axen2011}. To expand interpretatbility, researchers have proposed to model additional types of parameters, including variability or granularity parameters \citep[e.g., see][]{krone2018}. In this manuscript we instead, propose to look at the broader machine learning literature and to consider a wider range of time series features to describe psychological time series. In particular, we propose to use an expanded feature-based clustering approach, which allows users to summarize time series based on a research-specific combination of parameters and descriptive statistics. We argue that feature-based clustering offers a flexible variety, less strict assumptions, easy and intuitive extraction, theory-based selection, and meaningful psychological interpretability.

\section{Feature-based Time Series Clustering}

The main aim of feature extraction is to describe the most important and meaningful aspects of a time series. In its most general approach, this can include any statistical summary of the time series \citep[i.e., feature; e.g.,][]{maharaj2019}. The extracted features can then be used as inputs to most clustering algorithms, including k-means or hierarchical clustering to group the participants into clusters \citep{wang2006}. The clustering of model parameters is, thus, technically a restrictive sub-form of feature-based clustering --- where lagged regression coefficients, for example, can be considered to be one type of feature capturing a time series. However, beyond model parameters, time series might also be described with any other statistical feature \citep{tiano2021}. Feature-based time series clustering has been a valued tool in many empirical fields for its flexibility and interpretability. As a result, features have been extracted for a variety of data, including analyses of astronomical, meteorological, and aviation pathways, biological and medical developments, as well as energy and finance patterns \citep{Aghabozorgi2015}. 

With its rich history, a staggering variety of time series features have been proposed, not all of which are meaningful for psychological time series. As an example, \citet{wang2006} propose a set of nine statistical features for describing a trajectory: (1) trend, (2) seasonality, (3) periodicity, (4) skewness, (5) kurtosis, (6) serial correlation, (7) non-linearity, (8) self-similarity, and (9) chaos \citep[also see][]{fulcher2013}. \citet{adya2001} proposed a total of 28 features relevant for forecasting, which largely fit within the selection proposed by \citet{wang2006}. A final illustration comes from a commonly used software package for feature extraction `tsfresh', which allows users to extract a total of 794 features to describe all kinds of aspects of a time series \citep[][]{christ2018}. 

Given that there are hundreds of possible features to describe psychological time series, one approach to choosing relevant features would be to extract a large number of features and then assess which features are most effective at capturing differences in the time series \citep[e.g.,][]{christ2018}. However, such an approach is not always advisable for psychological time series. For one, features should reduce the data dimensionality --- it would thus not necessarily be advisable to describe 60 ESM measurements with 794 time series features. More importantly, however, careful feature extraction can be crucial in the interpretability and explainability of the results. This is particularly the case when features have a conceptual or psychological meaning. Taking the concept of well-being as an example, psychologists might be interested in whether certain participants tend to have higher well-being in general (i.e., mean) or whether some participants fluctuate between extremely high and low well-being (i.e., variance). But psychologists might not necessarily be interested in the exact time point after which 50\% of the summed well-being values lie (i.e., relative mass quantile index) or how much different sine wave patterns within the well-being data correlate with one another (i.e., cross power spectral density). We will, thus, propose and describe a set of time series features that have a strong backing within the ESM literature and as such offer a reasonable starting point for most concepts within the field.

\subsection{Features of Psychological Time Series}

Fortunately, past conceptual and empirical efforts offer valuable discussions of common time series features in psychological research. While the final selection of features should always be driven by the research questions and field-specific conventions, we can build a practical toolbox of meaningful time series features for psychological data. In particular, we propose to focus on six features based on common psychological research questions and recent works on affect dynamics \citep[e.g.,][]{dejonckheere2019, kuppens2017, adya2001}. An overview of the proposed features, their substantive interpretations, and mathematical operationalizations is available in \tblref{tab:esmFeatures}.

To understand emotion dynamics, \citet{kuppens2017} originally proposed four features: (1) within-person variability, (2) co-variance or ICC, (3) inertia or autocorrelation, and (4) cross-lagged correlations. These features were then extended by \citet{krone2018}, adding (5) innovation variance, and (6) mean intensity. \citet{krone2018} even built a parametric model to tentatively cluster study participants. From a slightly different perspective \citet{dejonckheere2019} later added three additional features for psychological time series: (7) instability (8) interdependence (i.e., network density), and (9) diversity \citep[i.e., Gini coefficient; also see][]{wendt2020}\footnote{It should be noted that also within the psychological literature alternative summaries have been proposed that for example include measurement distribution, nonlinear developments, or categorical states. As an example, \citet{kiwuwa-muyingo2011} proposed to extract clinically meaningful states for medical adherence data and suggests these states as meaningful time series features.}. 






This selection of the proposed six time-series features is in no way exhaustive or imperative. Both using a purely data-driven approach or selecting other aspects to summarize the time series are legitimate options. The list seeks to offer a practical toolbox of features that are common and meaningful to psychological research questions and -practice (see \tblref{tab:esmFeatures} for a more complete overview).

\paragraph{Central Tendency} Central tendency refers to the statistical measures that represent the ``middle" or ``average" of a set of data. The most common measures of central tendency are the mean, median, and mode (Hopkins, 2020). As a familiar statistic from probability theory, the central tendency sits at the heart of many fundamental questions about psychological time series. Researchers might, for example, be interested in whether ``Over a one month period, are some people happier than others?''. A recent review paper pointed out that the relatively simple mean statistic, is often overlooked in psychological time series analyses but is usually essential for understanding psychological differences \citep{bringmann2018c}. As such, several time series modeling approaches now advocate for conscious consideration of the mean \citep[e.g.,][]{bringmann2017a}. As with cross-sectional data, for time series data the appropriate measure of central tendency depends on the specific characteristics of the data being analyzed. For instance, if the data are normally distributed, the mean is often an appropriate measure. If the data are skewed or have outliers, the median may be a better choice \citep{weisberg1992}.

\paragraph{Variability} Variability captures the degree to which a set of data differs from the central tendency and is sometimes also referred to as the dispersion or spread of the data \citep{weisberg1992}. In time series analyses, variability is conceptually important because it provides information about the distribution and diversity of the data. Especially within-person changes in variability have recently received increasing attention because they can be a potent early warning signal in psychological time series \citep{helmich2020a, vandeleemput2014}. A marked within-person increase in variance has, for example, been found to precede crucial transitions --- often considered to be part of a phenonenon know as critical slowing down \citep[i.e., anomalous variances; e.g.,][]{scheffer2009, wichers2019}. 

But also more global person-level differences in time series variability have been found to be crucial and relate to important psychological questions. Person-specific differences in variability have, for example, been found to be indicative of worse psychological states \citep{myin-germeys2018}. And, similarly, person-level differences have also been associated with higher levels of psycho-pathological recurrences among depression patients \citep{timm2017}. But also more broadly speaking, psychological researchers and practitioners are theoretical-empirically often interested in person-level differences in variability. Researchers on polarization and radicalization might for example ask: "Are people settled in their attitudes towards migrants or vary across the measurement period?''

Methodologically, there are several measures of variability, including the range, standard deviation, and variance. The appropriate measure to use depends on the specific characteristics of the data being analyzed. For normally distributed data variances and standard deviation are often preferred, whereas data with outliers or nonparametric distributions are often better described with robust variability measures such as the median absolute deviation \citep{weisberg1992}.

\paragraph{(In)stability} Instability captures the average change between two consecutive measurements \citep{ebner-priemer2009}. While instability is conceptually related to the variability feature, variability does not take into account temporal dependency, whereas instability looks at the `jumpy-ness' of the data over time. In other words, instability reflects the fluctuation or inconsistency in a time series data over time, while variability reflects the range or diversity of values in a time series data \citep{trull2008}. For example, if a person has rapid and extreme changes in mood their mood is highly unstable, while if a person's mood responses span a wide range over the entire study period, their mood is highly variable \citep{jahng2008}. Within psychological time series, instability measurements have especially been important in the research of borderline personality disorder \citep{trull2008} and suicidality \citep{kivela2022}, but also in understanding early warning signals more generally \citep{wichers2019}. Conceptually, the instability feature, thus, relates to a broad range of research questions, including: ``Do changes tend to be slow and gradual or fast and abrupt?" Methodologically, (in)stability measurements often look at the average change between two consecutive measurements, such as the mean absolute change \citep[e.g.,][]{ebner-priemer2009,barandas2020}. 

\paragraph{Self-similarity} Self-similarity in time series data refers to the property of a time series to exhibit similar patterns of behavior over different time scales \citep{dmello2021}. That is, self-similarity describes how much a measurement carries over to future measurements. There are two main ways in which experiences tend to be connected to future experiences in psychological time series --- resistance to change and seasonality. Both forms of self-similarity relate to different types of research questions. 

Resistance to change is the self-similarity of a measurement with its next measurement and is often referred to as inertia \citep{kuppens2010}. If inertia is high a development tends to stay in a certain state or move towards a certain direction. Because high inertia is resistant to change, in emotion dynamics high inertia has been found to be indicative of underreactive systems and to be characteristic of psychological maladjustment \citep{kuppens2010}. In a similar vein, high inertia at baseline was even predictive of the initial onset of depression \citep{kuppens2012}. Conceptually, inertia is more broadly connected to research questions such as: ``Do patients stay in a depressed mood for several measurements?'' One way to measure inertia in time series data is through the use of autocorrelations or autoregressions with a lag-1, which look at the correlation between a given time series and itself at the previous measurement occasion. High autocorrelation values can indicate high levels of inertia, while low autocorrelation values may indicate a more unpredictable or volatile time series \citep{dejonckheere2019}.

Seasonality is the self-similarity of a measurement that returns in a cyclical manner and is often also referred to as periodicity \citep{gregson1983}. Periodicity in psychological time series data, thus, refers to the presence of recurring patterns at regular intervals. In psychological time series, these cyclical patterns are not only dangerous for models that assume stationarity \citep{beal2015} but can be interesting psychological properties in their own regard \citep{epskamp2018, schmittmann2013}. Humans generally live in seasonal environments, including, for example, exam periods at universities \citep{fuller2003}. But also more internally, personality manifestations, for example, have long been assumed to develop cyclically \citep{cattell1957}. The associated research questions can be driven by an understanding of external seasonality factors such as: ``Do participants drink more alcohol on Fridays?'' But research questions can also address periodic patterns as they may be the result of underlying psychological processes: ``Does stress increase with fatigue cycles?'' Similar to inertia, periodicity is often measured using autocorrelations but then with the lag of the seasonality or cycle (e.g., daily, seven days, one month, one semester). Periodicity can additionally be captured using spectral analysis or decomposition, such as with wave patterns \citep[e.g., fourier or wavelet transformation; e.g.,][]{mayor2022}.

\paragraph{Linear Trend} Linear trends in time series data refer to the presence of a straight-line development of a variable being measured. This type of trend can be observed when there is a consistent increase or decrease in the data over time \citep{nyblom1986}. For psychological time series, researchers have, for example, pointed out linear trends in interpersonal communications \citep{vasileiadou2014}, and emotion dynamics \citep{oravecz2016}. Linear trends are often the simplest way of assessing whether a psychological theory of change is appropriate \citep{gottman1969}. In empirical practice, linear trends are, thus, a common research interest for psychological time series data: ``Do patient symptoms improve consistently?'' or ``Does worker productivity decline continuously?'', are familiar types of research questions. Mathematically, such linear trends are commonly captured using (piecewise) linear regression coefficients, but also using differential equations \citep{chow2019}. 

\paragraph{Nonlinearity} Changes in psychology are not always linear \citep{hayes2007}

Nonlinear trends are important in two regards. Firstly, nonlinearity is a deviation from linear trends and secondly, the shape and structure of the nonlinear trend. 

``Some examples of nonlinear DEs have already been proposed in studies of ovulatory regulation (Boker, Neale, \& Klump, 2014), circadian rhythms (E. N. Brown \& Luithardt, 1999), cerebral development (Thatcher, 1998), substance use (Boker & Graham, 1998), cognitive aging (Chow \& Nesselroade, 2004), parent-child interactions (Thomas \& Martin, 1976), dyadic relationships (Chow, Ferrer, \& Nesselroade, 2007); and sudden transitions in attitudes (van der Maas, Kolstein, \& van der Pligt, 2003)." \citep{chow2019}

Questions like: ``Is the development of anxiety a nonlinear process?'' are mathematically captured using nonlinearity parameters such as the bicoherence metrics \citep{cuddy2009}. 

The structure of nonlinear trends is often mathematically more difficult to capture \citep{castro-alvarez2022a}. One common way is to capture the trends using polynomial coefficients or more broadly by how `wiggly' the line is (e.g., estimated degrees of freedom of GAM spline models; similar to the number of spikes \citealp[]{caro-martin2018}). 

depression most likely non-linear  \citep{hosenfeld2015}. Patients in recovery showed sudden changes in the improvement of depression \citep{helmich2020a}

\subsection{Analysis Steps}

------------------------

NOTES:

\noindent sort of feature-based clustering in psych \citep{heylen2016} as well as \citep{krone2018}

------------------------

The feature-based clustering can be structured into four main steps \citep{rasanen2009, wang2006}. (1) The selection and preparation of the input variables, (2) the extraction of the features that describe the time series, with an optional feature reduction step if there are too many data points for the clustering algorithms, (3) the actual clustering of the time series features, and (4) the evaluation and interpretation of the clusters. We provide a conceptual overview that can be used alongside this section in \fgrref{fig:TSCFlow}. 

\begin{figure}[!ht] % \begin{figure*}[hbtp] <-- on its own page
  \caption{Flowchart Feature-Based Time Series Clustering in Psychology}
  \label{fig:TSCFlow}
  \centering\includegraphics[width=\textwidth]{figures/TS Cluster Flow/TimeSeriesClusterFlowSelection.pdf}
  \caption*{Note: \\
  Choices selected for illustration in this manuscript are marked in bold.}
\end{figure}

\paragraph{Input variables}
Time series clustering starts with the selection and preparation of the variables of interest. While the selection will necessarily be field- and concept-specific, there are a few conceptual and methodological issues that should be considered. Conceptually, the included variables should adequately capture the concept of interest and should be meaningful to the understanding of the time series. There are, for example, calls that emotion dynamics should be assessed with a repertoire of positive and negative emotions \citep[e.g.,][]{dejonckheere2019}, migration experiences are fully captured with affect, behavior, cognition, and desire measurements \citep[e.g.,][]{Kreienkamp2022d}, and many health developments are commonly captured within the biopsychosocial domains \citep[e.g.,][]{suls2004}. At the same time, however, the added number of variables can become a methodological concern. Not only can redundant and irrelevant variables diminish the quality of the analyses, but with intensive longitudinal data the number of data points compounds across participants, measurement occasions, and variables so that additional variables can make many of the following steps substantially more difficult (also see \fgrref{fig:TSCFlowN}). 

\begin{figure}[!ht] %hbtp
  \caption{Exemplary Flowchart of Data Points in Feature-Based Time Series Clustering}
  \label{fig:TSCFlowN}
  \centering\includegraphics[width=\textwidth]{figures/TS Cluster Flow/tsClustFlowN.pdf}
  \caption*{Note: \\
  The presented number of participants, variables, and measurement occasions are somewhat arbitrary but generally represent common sample sizes found within the literature. Also the number of extracted clusters is presented for illustrative purposes only.}
\end{figure}

Once the important variables have been selected, the data needs to be prepared for the analysis steps. Importantly, this not only means validating and cleaning the data (e.g., re-coding, removing duplicate or unwanted measurements) but also making the time series comparable. Two important steps are making the time-frames and response scales comparable across participants. Extracting features that describe the development of a month for one participant but 90 days for another participant might not be comparable for some psychological phenomena. This comparability is important on a conceptual level but the difference in data availability might also influence the clustering steps, where the participant with the shorter measurement period would, for example, have a larger number of missing values. It is thus generally advised to reduce the data to a common and comparable time frame. 

\paragraph{Feature extraction}
more technical description model + descriptive stats
% The main aim of feature extraction is to describe the most important and meaningful aspects of a time series. There are, however, hundreds of possible features to describe psychological time series \citep[e.g., tsfresh][]{christ2018}. One approach to choosing relevant features would be to extract a large number of features and then assess which features are most effective at capturing differences in the time series. However, such an approach is not always advisable for psychological time series. For one, features should reduce the data dimensionality --- it would thus not necessarily be advisable to describe 60 ESM measurements with 180 time series features. More importantly, however, careful feature extraction can be crucial in the interpretability and explainability of the results. This is particularly the case when features have a conceptual or psychological meaning. Taking the concept of well-being as an example, psychologists might be interested in whether certain participants tend to have higher well-being in general (i.e., mean) or whether some participants fluctuate between extremely high and low well-being (i.e., variance). But psychologists might not necessarily be interested in the exact time point after which 50\% of the summed well-being values lie (i.e., relative mass quantile index) or how much different sine wave patterns within the well-being data correlate with one another (i.e., cross power spectral density). We would, thus, strongly advocate for a careful selection of time series features that are meaningful to the field and concepts.

\input{tables/esmPsychFeatures}

\paragraph{Feature reduction}
Once a meaningful selection of time series features has been extracted for each variable and participant, the total number of data points usually remains too large for most clustering algorithms. As an example, a relatively common scenario would include 10 variables of interest, where 8 time series features are extracted, resulting in 80 features per participant (with a common sample size of 100 participants that would result in a total of 8,000 data points in this hypothetical example). We offer an illustration of the compounding numbers of data points in \fgrref{fig:TSCFlowN}. The difficulty of working with such a large number of dimensions is sometimes termed the `dimensionality curse' \citep[e.g.,][]{altman2018}. To deal with this dimensionality issue, two main approaches have been proposed --- feature selection and feature projection (a full overview of the approaches, methods, and common algorithms is available in \tblref{tab:featureReduction}). 

Feature selections seek to create a subset of the most important features. The many available approaches differ in how they seek to determine the importance of the individual features. Generally speaking, selection methods can be categorized as `filter', `wrapper', or `hybrid' methods. The filter methods, broadly speaking determine important features by identifying irrelevant features (e.g., because features do not capture much information), and identifying redundant features (e.g., because features capture the same information). Wrapper methods on the other hand avoid the feature-based evaluation and focus on the performance of the later model to identify the most important features. A wrapper, thus, compares the performance of models with different feature combinations. Traditional examples of wrappers are forward selection or backward elimination procedures. Because filter methods might not always perform well and wrapper methods are computationally intensive\footnote{Computationally \textit{k} features could be considered in \(2^k – 1\) possible combinations --- for the example of 80 features that would allow for \(1.20 \times 10^{24}\) (over one septillion) combinations.}, hybrid methods seek to combine the two methods and find a balance between computational effort (i.e., efficiency) and performance (i.e., effectiveness). Methods might, for example, use a filter step to reduce the size of features considered in a later wrapper step.

The second general approach to the dimensionality curse has been feature projection. Projection methods are relatively common in psychological research --- including, factor- and principal component analyses. Generally speaking, projection methods seek to transform the many features in such a way that a much smaller number of new variables can accurately capture the variance and structure of the original features (i.e., the data is projected to a lower dimensional space). Commonly, this is achieved using linear transformations (e.g., principal component analysis), or more complex nonlinear transformations (e.g., t-SNE). Generally speaking, feature selection procedures have the benefit that they retain the interpretable feature labels directly and immediately indicate which features were most informative in the sample. Feature projection methods, on the other hand, tend to be more generalized and are more readily available.

\input{tables/FeatureReduction}

\paragraph{Feature clustering}
For feature-based time-series clustering, the main aim is to organize participants into groups so that the features of participants within a group are as similar as possible, while the features of people in different groups are as different as possible. The crux of clustering is, thus, to have clearly defined and effective measurements of (dis)similarity. Most of the clustering algorithms used today use some form of distance measurement to optimize group assignment (or similarity measurement for qualitative features). While others have produced excellent overviews of the many clustering approaches available \citep[e.g.,][]{xu2015}, we will briefly introduce some of the more readily available approaches suitable for most time series feature data. The well-established and readily available clustering approaches can, broadly speaking, be categorized as based on (1) centroids, (2) distributions, (3) density, (4) hierarchies, or (5) a combination thereof (see \tblref{tab:clusterApproaches}). 

Each of these approaches can be a valuable clustering approach for time series feature data and users will usually have to make an informed decision based on the structure of their data as well as an appropriate weighing of accuracy and efficiency. As an example, under ideal conditions, all approaches are likely to suggest very similar clusters. However, when the shapes of clusters, for example, become more complex in a multi-dimensional space, density-based or hierarchy-based approaches that allow for more bottom-up clustering are likely to be more accurate. Yet, with higher numbers of participants and features, both density- and hierarchy-based approaches may perform less well and the more efficient centroid-based methods might be more effective. Similarly, several of the proposed time series features are statics that are generated from Gaussian distributions and a distribution-based algorithm might be ideally suited to separate such distributions \hl{(Corduas \& Piccollo, 2008)}. Yet, in cases where mis-specifications are costly, a method with fewer assumptions might be more advisable. 

There is thus, unfortunately, no one-size-fits-all solution to clustering. On the bright side, however, the thriving methodological developments in the clustering literature offer a wide variety of options that offer prospects even for extremely large and complex data sets. We provide a short intuitive explanation for common approaches, together with some of their characteristics and example algorithms in \tblref{tab:clusterApproaches}. For our own illustration, we have chosen that centroid-based k-means clustering. While k-means comes at the expense of high accuracy with more complex cluster shapes, we specifically chose k-means because it is an extremely efficient method that works well with large participant- and feature numbers without making too many restrictive assumptions about the shape of the data. K-means is also well-established within the research community and has been readily implemented in many statistical software packages. Additionally, many of the feature selection methods have specifically been designed for the well-established k-means algorithm. As such, the k-means offers a good starting point for many psychological researchers and the method should be generalizable across a relatively wide variety of projects.

\paragraph{Cluster Performance Evaluation}

PARAGRAPH ON PERFORMANCE METRICS (incl., cross-validation?)

incl., finding cluster prototypes:
1. The medoid sequence of the set. 2. The average sequence of the set. 3. The local search prototype

External evaluation measures: These measures require knowledge of the true class labels and are used to compare the cluster assignments to the true class labels. Examples include accuracy, F-measure, adjusted Rand index, mutual information, and normalized mutual information.

Internal evaluation measures: These measures do not require knowledge of the true class labels and are used to evaluate the quality of the clusters based on the characteristics of the data itself. Examples include silhouette coefficient, Calinski-Harabasz index, Davies-Bouldin index, and the Elbow method.

PARAGRAPH ON REPORTING \citep[e.g.,][]{vandeschoot2017}

\input{tables/clusterApproaches}

\section{Empirical Illustration}
To illustrate common use cases of feature-based time series clustering with psychological ESM data, we apply the clustering process to a recent set of studies that collected diverse concepts with context-specific measurements. The illustration specifically uses data from research on migration experiences, where researchers have started using ESM data to follow the daily interactions of migrants with the cultural majority groups \citep[e.g.,][]{Keil2020}. This new type of research comes as a response to prominent reviews, which have called for more longitudinal \citep[e.g.,][]{Ward2019} and real-world data \citep[e.g.,][]{McKeown2017}. At the same time, conceptual works have pushed for more diverse assessments of migration experiences, including motivational, affective, cognitive, and behavioral aspects \citep[e.g.,][]{Kreienkamp2022d}. The migration ESM research, thus, offers a good example of the modern ESM data that introduces diverse conceptualizations and more event-specific missingness patterns.

% Methods and Results from RMarkdown render
\input{methods-and-results}
%\input{tables/descrWide}
\input{tables/descrLong}

\section{Discussion}
% aims re-iterated
We sought to introduce a flexible and transparent analysis that aids researchers in describing, exploring, and understanding psychological time series data. Such an analysis needs to address the demands of the growing variety of ESM research. In particular, modern describe- and explore analyses should be able to deal with high dimensionality, structural missingness, and diverse time-varying developments. We have argued that feature-based time series clustering is one option that meets this challenge. 

We have methodologically deconstructed the feature extraction, feature reduction and feature clustering steps and offered methodological as well as conceptual examples for each step. Within this step-wise approach, our article shows that feature-based clustering offers an excellent fit for psychological research practice as both the features as well as the analysis steps are well-established within the field and statistical packages are readily available. Time series features (such as means or linear trends) are not only easy to extract but also hold conceptual meaning for psychological data and can be chosen to address specific research questions (also see \tblref{tab:esmFeatures}). 

To illustrate the practical utility of the approach, we applied the method to real-life empirical data with diverse conceptualizations, missingness, and nonlinear trends. We make available the full code, custom functions, and illustrations to aid the approachability of the method. In the interpretation step we also show how the clusters can be compared across the original feature set as well as other meaningful situational and person-specific differences (e.g., see \fgrref{fig:clusterFeatVar}). We also find that for our example of migration experiences, the method was useful in discerning adaptive from more stressful experiences and helped to contextualize the diverging experiences.

As with any statistical method, feature-based time series clustering is not without limitations. 
In particular, feature-based clustering has both a usability as well as robustness limitation in its multiple steps. In terms of convenience, each of the three steps requires users to make an informed decision about the choice of method and algorithm. These additional steps of decision-making and transparency heighten the initial barrier to entry. We hope that our empirical illustration offers a relatively generalizable procedure that showcases the ease of use but clustering unfortunately does not offer a universal one-size-fits-all solution. 

In terms of methodological robustness, the variety of methods in each of the steps also brings with it the potential uncomparable results across methods. While the variety and diversity of methods is helpful in finding options even for more complex types of data, different algorithms often offer different results. And even when patterns produce robust clustering solutions across algorithms, individual methods might still have their ideosyncratic shortcomings and might, for example, be sensitive to outliers. 

Notwithstanding the limitations, we believe that feature-based clustering offers an exciting new potential for psychological time series. Across a variety of scientific fields, feature-based time series clustering has been valued for its structuring and simplifying utilities. Within the growing field of psychological time series, modern ESM data is becoming increasingly more complex in terms of participants, variables, and time points. Feature-based time series clusters can aid in reducing these complexities to important and meaningful patterns, an increasingly important task for researchers and practitioners. 



% Tables
% Example
%\input{Tables/descrFullWide}


% Figures


\printbibliography

\appendix

\section{ESM Data Challenges}
\label{app:ChallengesAppendix}
We will briefly consider which challenges modern ESM data introduce and what qualities are called for in an extension of the clustering repertoire. We particularly highlight issues of dimensionality, non-equidistant or missing measurements, an interest in non-stationary trends, as well as inconsistent/diverse time scales. 

Concerning dimensionality issues, especially more abstract psychological experiences often need a wider variety of measurements to be captured adequately. Today, few clinical conditions are captured with a single symptom measure \citep[e.g.,][]{cramer2016}, emotions are rarely assessed in isolation \citep[e.g.,][]{reitsema2022}, and socio-cultural experiences are now widely considered to be multimodal \citep[e.g.,][]{Kreienkamp2022d}. This also means that modern analysis techniques increasingly need be able to accommodate an increased focus on multivariate developments. At the same time, however, an increase in the number of considered variables tends to come at the expense of computational load for model estimations, and clustering models may not converge \citep[the aforementioned dimensionality curse;][]{altman2018}. A modern time series clustering technique should consequently be able to summarize and structure multivariate phenomena without running into computational load issues.

Another common type of data that are measurement regiments that collect data in irregular time intervals (i.e., non-equidistant measurements). Common are, for example, procedures where participants are asked to respond at random times throughout the day (i.e., signal-contingent) or following specific natural events of interest \citep[i.e., event-contingent; see][]{shiffman2008, myin-germeys2018}. Under such conditions data tends to violate the equidistance assumption that is expected by many time series models \citep[][]{hamaker2017}. Smaller issues of non-equidistant data can be avoided with transformations \citep[e.g., dynamic time warping,][]{berndt1994} or newer modeling procedures \citep[e.g., continuous-time models;][]{dehaan-rietdijk2017} but for many analyses, including some cluster approaches, non-equidistant measurements remain a prevalent issue. 

Structural missingness remains an even more strenuous challenge. Structural missingness occurs when data is missing because it logically cannot be collected \citep[as opposed to probabilistically missing data;][]{little2020, mclean2017}. Often, however, researchers might want to include variables in their models that are not available under all conditions. Follow-up and event-contingent questions are a common example in ESM studies. Researchers, for example, ask about the frequency, intensity, or duration of symptoms --- but only if a symptom was present \citep{kivela2022}. Such approaches become specifically critical in cases of sensitive questions such as questions about suicidal ideation or other potentially trauma-inducing questions \citep[e.g.,][]{glenn2022}. The most common practice for structurally missing data is to either exclude the variable or any measurement that has no structurally missing data \citep[e.g.,][]{lavori2008}\footnote{This is the case because the most commonly used models require complete data \citep{schafer2002} and structurally missing data cannot be imputed as it logically does not exist \citep[e.g.,][]{lavori2008}.} --- neither option suits a research question that wishes to include variables with common structural missingness, such as event-specific or follow-up questions. In short, new clustering approaches should be able to deal with structurally missing data in order to address modern ESM data.

When it comes to studying developmental trajectories, psychological researchers are often also interested in nonstationary processes because they are more representative of the complex, dynamic behavior of the human mind. In psychology, nonstationary processes are typically used to study phenomena such as cognitive development \citep[][]{quartz1997}, decision-making \citep[][]{ratcliff2016}, and emotion dynamics \citep[][]{bringmann2018b}. These processes are often characterized by changes in the underlying statistical properties of the data over time, such as changes in the mean or variance \citep[][]{molenaar2009}. Especially when considering changes in mean levels, researchers are often interested in nonlinear changes because they describe human functioning better. For example, in decision making people might switch between choices \citep[][]{ratcliff2016}, or patients reducing medication might experience mood swings \citep[][]{helmich2020a}. Similarly, psychologists are often also interested in how variances change over time. This is especially the case because several changes in an individual's variance have been found to be indicative of critical changes, including depression relapses and symptom shifts more generally \citep[e.g.,][]{schreuder2020, wichers2020}. There is, thus, also a need for time series clustering algorithms that capture nonstationary processes, including nonlinear trends.

Psychological time series often exhibit complex patterns and relationships that can change over different time scales. For example, a time series of daily mood ratings may show a weekly pattern, with higher ratings on the weekends and lower ratings during the week. At the same time, the series may also exhibit a longer-term trend, with overall mood levels increasing or decreasing over the course of several months or years \citep[e.g.,][]{Ram2014}. \sout{These different time scales can be studied separately or in combination, using different statistical techniques and modeling approaches \citep[][]{bertenthal2007, jeronimus2019a}.} Different time scales can become an even more difficult issue when different variables in a model develop on different time scales \citep{bringmann2022b}. Different time scales are thus also a concern clustering approaches should be able to address.


\end{document}
