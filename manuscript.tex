% define document type (i.e., template. Here: A4 APA manuscript with 12pt font)
\documentclass[man, 12pt, a4paper, mask, floatsintext]{apa7}

% change margins (e.g., for margin comments):
%\usepackage{geometry}
% \geometry{
% a4paper,
% marginparwidth=30mm,
% right=50mm,
%}

% add packages
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage[style=apa, sortcites=true, sorting=nyt, backend=biber, natbib=true, uniquename=false, uniquelist=false, useprefix=true]{biblatex}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{setspace,caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{fourier}
\usepackage{stackengine}
\usepackage{scalerel}
\usepackage{fontawesome5}
\usepackage[normalem]{ulem}
% \usepackage{longtable}
\usepackage{amsmath, nccmath}
\usepackage{mdframed}
\usepackage{ntheorem}
\usepackage{afterpage}
\usepackage{float}
\usepackage{array}
\usepackage{censor}
\usepackage{pdflscape}
\usepackage{lscape}
\usepackage{pdfpages}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{tabu}


% make warning with red triangle
\newcommand\Warning[1][2ex]{%
  \renewcommand\stacktype{L}%
  \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries !}}{#1}}%

% make question with red triangle
\newcommand\Question[1][2ex]{%
  \renewcommand\stacktype{L}%
  \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries ?}}{#1}}%
  
% add definition sections
\theoremstyle{break}
\newtheorem{definition}{Definition}

% add hypothesis sections
\theoremstyle{plain}
\theoremseparator{:}
\newtheorem{hyp}{Hypothesis}

\newtheorem{subhyp}{Hypothesis}
   \renewcommand\thesubhyp{\thehyp\alph{subhyp}}

% add quote section
\usepackage{csquotes}

% framed box section
\usepackage{framed}
\emergencystretch=1em

% formatting links in the PDF file
\definecolor{myurlcolor}{HTML}{1a0dab}
\hypersetup{
pdfpagemode={UseOutlines},
bookmarksopen=true,
bookmarksopenlevel=0,
hypertexnames=false,
colorlinks   = true, %Colours links instead of ugly boxes
urlcolor     = myurlcolor, %blue,Colour for external hyperlinks
linkcolor    = black, %blue, Colour of internal links
citecolor   = black, % cyan, Colour of citations
pdfstartview={FitV},
unicode,
breaklinks=true,
}

% ref labels
\newcommand{\fgrref}[2][]{\hyperref[#2]{Figure \ref*{#2}#1}}
\newcommand{\tblref}[2][]{\hyperref[#2]{Table \ref*{#2}#1}}
\newcommand{\appref}[2][]{\hyperref[#2]{Appendix \ref*{#2}#1}}
\newcommand{\equatref}[2][]{\hyperref[#2]{Equation \ref*{#2}#1}}

% custom open science badge height
\newlength{\badgeheight}
\setlength{\badgeheight}{1em}

% prevent multipage footnotes
\interfootnotelinepenalty=10000

% language settings
\DeclareLanguageMapping{american}{american-apa}

% add reference library file
\addbibresource{referencesZotero.bib}

% Title and header
%\title{Describe and Explore: Using feature-based time series clustering to identify meaningful structures in intensive longitudinal data}
\title{A Gentle Introduction and Application of Feature-Based Clustering with Psychological Time Series}

\shorttitle{feature-based time series clustering}

% Authors
\author[*,1,3]{Jannis Kreienkamp}
\author[1,3]{Maximilian Agostini}
\author[1,3]{Kai Epstude}
\author[2,3]{Rei Monden}
\author[1,3]{Peter de Jonge}
\author[1,3]{Laura F. Bringmann}
\affiliation{\hfill}

\affil[1]{University of Groningen, Department of Psychology}
\affil[2]{Hiroshima University, Graduate School of Advanced Science and Engineering}
\affil[3]{Author order TBD}

\authornote{   
   \addORCIDlink{* Jannis Kreienkamp}{0000-0002-1831-5604}\\
   \addORCIDlink{Laura F. Bringmann}{0000-0002-8091-9935}\\
   \addORCIDlink{Kai Epstude}{0000-0001-9817-3847}\\
   \addORCIDlink{Maximilian Agostini}{0000-0001-6435-7621}\\
   \addORCIDlink{Peter de Jonge}{0000-0002-0866-6929}\\
   \addORCIDlink{Rei Monden}{0000-0003-1744-5447}

We have no known conflict of interest to declare. The authors received no specific funding for this work. Materials and  software is available at \url{https://janniscodes.github.io/migration-trajectories/}  \citep{KreienkampTBD}. Protocols, materials, data, and code are available at \url{https://osf.io/TBA} \citep{KreienkampTBD}. The preregistration of our analysis can be accessed as part of our Open Science Framework repository \citep{KreienkampTBD}.

Correspondence concerning this article should be addressed to Jannis Kreienkamp, Department of Psychology, University of Groningen, Grote Kruisstraat 2/1, 9712 TS Groningen (The Netherlands).  E-mail: \href{mailto:j.kreienkamp@rug.nl}{j.kreienkamp@rug.nl}
}

\leftheader{Kreienkamp}

% Abstract
\abstract{
Psychological time series data has not only become more common but also more diverse and complex. Researchers and practitioners are increasingly interested in identifying important differences in the developments of their patients, or participants. Past research has proposed a number of `dynamic measures' that describe meaningful developmental patterns for psychological data (e.g., instability, inertia, linear trend). Yet, commonly used clustering approaches are often not able to include these meaningful measures (e.g., due to model assumptions). We propose feature-based time series clustering as a flexible, transparent, and well-grounded approach that clusters participants based on the dynamic measures directly using common clustering algorithms. We introduce the approach and illustrate the utility of the method with real-world empirical data that highlight common ESM challenges of multivariate conceptualizations, structural missingness, and nonlinear trends. We use the data to showcase the main steps of input selection, feature extraction, feature reduction, feature clustering, and cluster evaluation. We also provide practical algorithm overviews and readily available code for data preparation, analysis, and interpretation.

}

\keywords{
    time series analysis, feature-based clustering, intensive longitudinal data, ESM\\
    \vspace{1em}
    \textit{Open Science Practices:}
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/material-color.png}} Open Materials, 
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/data-color.png}} Open Data,
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/code-color.png}} Open Code, \break
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/supplements-color.png}} Open Supplements
}


% set indentation size
\setlength\parindent{1.27cm}

% Start of the main document:
\begin{document}

% add title information (incl. title page and abstract)
\maketitle

% **CHEAT SHEET / LEGEND**

% Comments:
% '%' starts a comment in LaTeX (not printed)
% '\todo[inline]{} makes orange boxes in PDF
% '\marginpar{}' notes in margins
% '\footnote{}' footnote
% '\Warning' important note indicator in PDF (triangle with exclamation mark)
% '\Question' question note indicator in PDF (triangle with question mark)

% Citation (with Natbib citation style):
% '\citep[e.g.][p. 15]{CitationKey}' citation in parentheses "(e.g., Berry, 2003, p. 15)"
% '\citet{CitationKey}' citation in text "Berry (2003)"
% '\citealt' and '\citealp' alternate citation without parentheses
% '\citeauthor' and '\citeyear' only year or author

% Headings:
% '\part{}' and '\chapter{}' only relevant for multi-part or multi-chapter documents
% '\section{}' heading level 1
% '\subsection{}' heading level 2
% '\subsubsection{}' heading level 3
% '\paragraph{}' heading level 4
% '\subparagraph{}' heading level 5

% formatting:
% '\textbf{}' text bold font
% '\textit{}' text italic font
% '\underline{}' text underline
% '\sout{}' text strike out
% '\textsc{}' text small caps
% '\vspace{1em}' add vertical space
% '\hspace{1em}' add horizontal space
% '\\' new line (i.e., line break)
% '\pagebreak' start new page (i.e., page break)
% '\noindent' do not indent current line (e.g., current paragraph)
% 'begin{center}...end{center}' center text or object

% Math mode:
% '$\alpha = .8$' mathematical equation inline
% '$$\hat{y} = b_0 + b_1x$$' mathematical equation in its own line
% '\begin{equation}...\end{equation}' multi-line equation
% '\approx' approximate symbol
% '\neq' not equal
% '\bar' mean bar over letter
% '\pm' plus minus sign 
% '^{}' superscript
% '_{}' subscript
% '\fraq{numerator}{denominator}' fraction
% '\sqrt[n]{}' square root
% '\sum_{k=1}^n' sum for 1 through n

% Insert things from elsewhere:
% '\input{filename}' inputs the raw (tex) file as a command (e.g., tables and R-Markdown imports)
% '\include{filename}' includes section on new page (incl. possible auxiliary info)
% '\includegraphics[settings]{filename}' add a figure or graph
% '\caption{}' adds a caption to a table or figure
% '\label{}' labels sections, tables, figures, etc. so that they can be referred to.
% '\ref{}' refer to a labelled sections, tables, figures, etc.
% '\begin{enumerate}...\end{enumerate}' numbered list
% '\begin{itemize}...\end{itemize}' bullet-ed list
% '\item' item in list section 

% Symbols:
% '\&' and sign
% '\%' percent sign
% '\_' three dotes
% '\#' hash symbol
% ------------------------------------------------------------------

\newlength{\mdfmar}
\setlength{\mdfmar}{1.5em}
\mdfdefinestyle{mdfbox}{
    innerleftmargin = +\mdfmar, %
    innerrightmargin = +2\mdfmar, 
    innertopmargin = +0\mdfmar, 
    innerbottommargin = +1.5\mdfmar, 
    skipabove = -12pt
}

% \begin{mdframed}[style=mdfbox]
% \noindent\center\textit{Structure}:
% \begin{itemize}[nosep]
%     \item introduction
%         \begin{itemize}[nosep]
%             \item increase in number and variety of ESM data + ts clustering to understand complex developmental differences.
%             \item ESM literature: features to capture meaningful aspects
%             \item current clustering based on common model parameters, which only include a small selection of features and often have strong assumptions
%             \item Instead of building more complicated models (e.g., address violated assumptions or add different types of parameters), we propose to cluster based on important and relevant features directly.
%             \item Called feature-based clustering. Common procedure in digital phenotyping and the broader ml literature. Used in many fields
%             \item in this paper we provide a practical introduction to the method and illustrate its utility with real-world ESM data.
%         \end{itemize}
%     \item Data Used for Illustration
%     \item Analysis Steps + Application
%         \begin{itemize}[nosep]
%             \item input variables
%             \item feature extraction
%             \item (feature reduction)
%             \item feature clustering
%             \item cluster evaluation
%         \end{itemize}
%     \item Discussion (summary, limitation, implications, conclusion, etc.)
%     \newpage
%     \item\ [...]
%     \item Feature-based Time Series Clustering
%     \begin{itemize}[nosep]
%         \item why features: flexible similarity (b/c more variety), less assumptions, easy to calculate/extract, theory-based selection (as per RQ), psychological interpretability, ...
%         \item Features in Psychological Time Series
%         \begin{itemize}[nosep]
%             \item central tendency
%             \item variability
%             \item (in)stability
%             \item self-similarity
%             \item linear trend
%             \item nonlinearity
%         \end{itemize}
%         \item Analysis Steps
%         \begin{itemize}[nosep]
%             \item input variables
%             \item feature extraction
%             \item (feature reduction)
%             \item feature clustering
%             \item cluster evaluation
%         \end{itemize}
%     \end{itemize}
%     \item Empirical Illustration
%     \item Discussion (summary, limitation, implications, conclusion, etc.)
% \end{itemize}
% \vspace{1em}
% \end{mdframed}
% \newpage

Recent years have seen a striking increase in the number and variety of research studies that follow participants' everyday experiences and collect real-world psychological time series \citep[e.g.,][]{hamaker2017}. These intensive longitudinal data sets come with new forms of heterogeneity, where researchers have to consider differences across large numbers of participants, time points, and variables \citep[e.g.,][]{cattell1966, wardenaar2013}. Oftentimes, however, researchers are interested in precisely this complexity and wish to understand how people differ in their developments across several variables \citep[e.g.,][]{ernst2021}. Researchers and practitioners are, for example, asking: ``Do the symptoms of different patients develop in contrasting ways?'' \citep{Monden2015} or ``How do migrants differ in the development of their self-reported needs as they arrive in a new country?'' \citep{Kreienkamp2022d}. There is, thus, a clear need for analysis techniques that identify between-subject differences in developmental patterns for psychological data.

Recently, one promising way of identifying between-subject developmental patterns has been \textit{time series clustering} --- the idea of inductively grouping participants based on similarities of their time series \citep{ariens2020}. This analysis type essentially seeks to capture comparable within-person developments --- such as whether a variable remains stable over time, consistently increases, or exhibits cyclical patterns --- and then groups the persons based on these patterns  \citep[][]{liao2005}\footnote{It should be noted in some cases the time series do not need to be summarized and can be compared directly. Such analyses are however only possible with highly regular and controlled data, such as EEG data \citep{huang1985}, or when clustering variables within the person rather than identifying differences between persons \citep{haslbeck2022}. Multivariate data from intensive psychological survey studies, as we discuss here, are seldomly directly comparable across persons \citep[e.g., ][]{faloutsos1994}. We provide a broader embedding of the current methods for psychological time series in the discussion section.}. Time series clustering, thus, crucially depends on identifying meaningful summaries of the time series developments, which can be used to compare participants \citep[][]{Aghabozorgi2015}. 

Fortunately, past conceptual and empirical works in the experience sampling (ESM) literature have collected a number of meaningful aspects of psychological time series\footnote{In psychology, intensive longitudinal data collection methods are often referred to as experience sampling method (ESM), ecological momentary assessment (EMA), or ambulatory assessment (AA) studies. While the terms come from different conceptual backgrounds, they share a focus on collecting data over an extended period of time to capture people's behaviors and experiences as they vary over time and in response to different situations and events. In this article, we will use the experience sampling (ESM) term as it has the strongest footing within the clustering literature.}. Important aspects might include concerns over whether a symptom consistently stays at a certain level without much variability or whether some emotions develop together. For many of the most important developmental aspects, researchers have assembled numeric measures that capture these patterns. These summary statistics are often called ``dynamic measures'', ``principles of change'', or ``dynamic features'' of the psychological time series \citep{dejonckheere2019, kuppens2017, krone2018}. For most conceptualizations, the proposed number of aspects ranges from four to twelve key features \citep[][]{wang2006, dejonckheere2019}. Each of these statistics not only captures a distinct aspect of psychological time series but also holds conceptual value --- inertia, for example, describes a resistance to change that can be indicative of psychological maladjustment \citep{kuppens2010} or a higher within-person variability can signal an erratic state \citep{myin-germeys2018}.

Yet, despite this rich diversity of meaningful time series features in psychology, most clustering of ESM data has only focused on a small and restrictive selection of time series characteristics. Thus far, the most common approach has been to cluster participants based on person-specific model parameters --- notably intercepts and slopes from vector autoregression models \citep[VAR; e.g.,][]{ariens2020, bulteel2016, stefanovic2022}. While such model parameters are familiar to researchers in the field, they are often restricted to autocorrelations and cross-lagged partial correlations \citep[e.g.,][]{bringmann2018c} --- only two of the many potentially important time series features. Additionally, the validity of model parameters is traditionally restricted by the assumptions of the particular statistical model. To take the common VAR model as an example, the model explicitly assumes that the time intervals between measurements are consistent (i.e., equidistant measurement assumption), it does not allow for missing values, and it assumes that means or variances do not change over time \citep[i.e., stationarity assumption;][]{lutkepohl2005}. These assumptions however stand in contrast to types of data researchers commonly collect to address important questions of erratic, and context-specific phenomena \citep[][]{myin-germeys2018, hamaker2017, kivela2022, helmich2020a}. Model parameters might, thus, not always accurately capture the time series and are often restricted in the features they capture.

% There is, thus, a resolute need for more flexible and meaningful time series summaries that can capture diverse measurement and missingness patterns and can capture non-stationary data, including non-linear trends, also across different time scales.

%Recent efforts to address the shortcomings of model parameters for time series clustering have primarily proposed more complicated models aimed at addressing either accuracy or interpretability concerns. Accuracy concerns have particularly been addressed by building more advanced models that relax specific assumptions \citep[e.g.,][]{denteuling2021} or by switching to different types of models with fewer assumptions, including spline models \citep{axen2011}. To expand interpretability, researchers have proposed to model additional types of parameters, including variability or granularity parameters \citep[e.g., see][]{krone2018}. 
%\citet{krone2018} even built a parametric model to tentatively cluster study participants.

Recent efforts to address the shortcomings of model parameters for time series clustering have primarily proposed more complicated models, which either seek to relax specific assumptions \citep[e.g.,][]{denteuling2021} or include additional time series features \citep[e.g., see][]{krone2018, gates2017}. In this manuscript, we instead, propose to directly cluster based on important and relevant features. Adequately named \textit{feature-based time series clustering}, such an approach has been a common procedure in digital phenotyping \citep[][]{loftus2022} and the broader machine learning literature \citep[][]{maharaj2019}. As such, the analysis has been applied to a variety of data, including analyses of astronomical, meteorological, and aviation pathways, biological and medical developments, as well as energy and finance patterns \citep{Aghabozorgi2015}. We argue that for psychological time series data, feature-based clustering offers a flexible variety, fewer strict assumptions, easy and intuitive analysis, as well as meaningful psychological interpretability. 

In the sections below we particularly seek to provide a practical introduction to the method. To do so, we illustrate the utility of the method with real-world ESM data. We use this data to discuss which psychological time series features are well-suited for a clustering approach, introduce the individual analysis steps, and provide practical guidance on common algorithms and analysis code. We first introduce the empirical case study data and then follow the individual analysis steps to discuss key decisions and approaches.

\section{Data Used for Illustration}

To illustrate the functioning and utility of feature-based time series clustering with psychological ESM data, we apply the clustering process to a recent set of studies that collected data on migration experiences. Researchers have recently started using ESM data to follow the daily interactions of migrants with the cultural majority groups. We have seen both new technological advances to capture such interactions \citep[e.g.,][]{Keil2020} as well as a rise in empirical studies that look at aspects such as the well-being of migrants \citep[e.g.,][]{hendriks2016} and the distress of intergroup contact \citep[e.g.,][]{doucerain2023}. 

This type of research comes as a response to a long-standing theoretical tradition highlighting the dynamic and developmental nature of cultural adaptation \citep[e.g.,][]{berry1986}. Importantly, such developmental trajectories are often difficult and stressful \citep{Ward2016} --- so that some may have an adaptive trajectory while others face a more difficult and grievous trajectory \citep{kim2017}. Prominent reviews within the migration literature have, thus, called for more longitudinal \citep[e.g.,][]{Ward2019} and real-world data \citep[e.g.,][]{McKeown2017}. By the same token, there is a crucial need to distinguish (mal-)adaptive clusters within these developmental trajectories and to relate these clusters to individual differences and contextual variables to make them understandable \citep[e.g.,][]{choi2009}. 

At the same time, conceptual works on cultural adaptation have highlighted the multidimensional perspective necessary to understand migration experiences. A recent scoping review has particularly highlighted that cultural adaptation is best understood as a joint process of motivational, affective, cognitive, and behavioral aspects \citep[e.g.,][]{Kreienkamp2022d}. The data on migration experiences are, thus, explicitly multidimensional, mirroring the increase of complex data within the ESM literature \citep[][]{wardenaar2013}.

In short, the data on migration experience we are using for this illustration, address a pressing societal issue of identifying and understanding diverging trajectories. And importantly for our illustration, the migration ESM research, also, exemplifies the real-world data issues that ESM data commonly face, including a multivariate conceptualization with event-specific missingness patterns (also see \appref[]{app:ChallengesAppendix} for an expanded discussion of the current challenges within ESM data that are addressed within the data).

\input{latex-results/01DataDescription.tex}

\section{Analysis Steps and Application} 

To introduce and illustrate the feature-based clustering analysis, we will follow the conceptual steps of the procedure in sequential order and discuss key issues for each step. To do so we will follow the common separation that has structured feature-based clustering into four main steps \citep{rasanen2009, wang2006}. (1) The selection and preparation of the input variables, (2) the extraction of the features that describe the time series, with an optional feature reduction step if there are too many data points for the clustering algorithms, (3) the actual clustering of the time series features, and (4) the evaluation and interpretation of the clusters. 
While this is a common conceptual separation of procedural elements, it is important to note that these steps are a general outline, and the specific details of the analysis will depend on the nature of the data and the research question being addressed. Nonetheless, the conceptual nature of these steps allows us to introduce the major elements of the analysis. We also provide a conceptual overview that can be used alongside this section in \fgrref{fig:TSCFlow}.

\begin{figure}[!ht] % \begin{figure*}[hbtp] <-- on its own page
  \caption{Flowchart Feature-Based Time Series Clustering in Psychology}
  \label{fig:TSCFlow}
  \centering\includegraphics[width=\textwidth]{figures/TS Cluster Flow/TimeSeriesClusterFlowSelection.pdf}
  \caption*{Note: \\
  Choices selected for illustration in this manuscript are marked in bold.}
\end{figure}

\subsection{Input Variables}
Time series clustering starts with the selection and preparation of the variables of interest. While the selection will necessarily be field- and concept-specific, there are a few conceptual and methodological issues that should be considered. Conceptually, the included variables should adequately capture the concept of interest and should be meaningful to the understanding of the time series. One of the advantages of feature-based clustering is that it is inherently adept to accomodating multi-variate concepts --- a common aim in ESM research. There are, for example, calls that emotion dynamics should be assessed with a repertoire of positive and negative emotions \citep[e.g.,][]{dejonckheere2019}, many health developments are captured within the biopsychosocial domains \citep[e.g.,][]{suls2004}, and migration experiences are fully captured with affect, behavior, cognition, and desire measurements \citep[e.g.,][]{Kreienkamp2022d}. At the same time, however, the added number of variables can become a methodological concern. Not only can redundant and irrelevant variables diminish the quality of the analyses, but with intensive longitudinal data the number of data points compounds across participants, measurement occasions, and variables so that additional variables can make many of the following steps substantially more difficult (also see \fgrref{fig:TSCFlowN}). 

\begin{figure}[!ht] %hbtp
  \caption{Exemplary Flowchart of Data Points in Feature-Based Time Series Clustering}
  \label{fig:TSCFlowN}
  \centering\includegraphics[width=\textwidth]{figures/TS Cluster Flow/tsClustFlowN.pdf}
  \caption*{Note: \\
  The presented number of participants, variables, and measurement occasions are somewhat arbitrary but generally represent common sample sizes found within the literature. Also, the number of extracted clusters is presented for illustrative purposes only.}
\end{figure}

\input{latex-results/02InputVariables.tex}

\subsection{Feature Extraction}
Armed with a relevant selection of key variables, the main aim of the feature extraction is to describe the most important and meaningful aspects of a time series. In its most general approach, feature extraction can include any numeric summary of the time series \citep[e.g.,][]{maharaj2019}. Given this flexibility, a staggering variety of time series features have been proposed across different disciplines. For example, \citet{wang2006} proposed 9 structural characteristics \citep[also see][]{fulcher2013}, \citet{adya2001} collected 28 features relevant for forecasting, and a commonly used software package for feature extraction `tsfresh' allows users to extract a total of 794 features of a time series \citep[][]{christ2018}. 

However, not all time series features might be relevant to psychological time series or any particular research question. For example, a psychologist interested in well-being might not necessarily be interested in the exact time point after which 50\% of the summed well-being values lie (i.e., relative mass quantile index) or how much different sine wave patterns within the well-being data correlate with one another (i.e., cross power spectral density). Instead, we advocate that we look at time series features that have a strong backing within the ESM literature and offer meaningful interpretability. 

Fortunately, past conceptual and empirical efforts offer valuable discussions of common time series features in psychological research. To understand emotion dynamics, \citet{kuppens2017} originally proposed four features: (1) within-person variability, (2) co-variance or ICC, (3) inertia or autocorrelation, and (4) cross-lagged correlations. These features were then extended by \citet{krone2018}, adding (5) innovation variance, and (6) mean intensity. \citet{krone2018} even built a parametric model to tentatively cluster study participants. From a slightly different perspective \citet{dejonckheere2019} later added three additional features for psychological time series: (7) instability (8) interdependence (i.e., network density), and (9) diversity \citep[i.e., Gini coefficient; also see][]{wendt2020}\footnote{It should be noted that also within the psychological literature, alternative summaries have been proposed that, for example, include measurement distribution, nonlinear developments, or categorical states. As an example, \citet{kiwuwa-muyingo2011} proposed to extract clinically meaningful states for medical adherence data and suggests these states as meaningful time series features.}. 

Some of the features found in the psychological literature are not necessarily well-suited to summarize time series for feature-based clustering and some key conceptual features are not well represented in the literature. In particular, covariances and cross-lagged correlations often produce a large number of parameters and might not necessarily summarize the existing data enough \citep{ernst2021}. Others such as network density parameters, used to summarize variable interdependence, might not always be meaningful for psychological data \citep{bringmann2019a}. At the same time, linear and nonlinear trends are not captured by the features commonly proposed for psychological time series, because the features are often developed for stationary models \citep[e.g.,][]{krone2018}. 

Thus, while the final selection of features should always be driven by the research questions and field-specific conventions, for our illustration we chose six features that relate to common psychological research questions and recent works within the field: (1) central tendency, (2) variability, (3) instability, (4) self-similarity, (5) linear trend, and (6) nonlinearity. An overview of the selected features, their substantive interpretations, and mathematical operationalizations is also available in \tblref{tab:esmFeatures}. For each of the six time series features we chose, we selected a mathematical representation that was appropriate for our type of data. We provide a short introduction of each feature below. Beyond the operationalizations we chose for our case study, we provide an R-function that automatically extracts and prepares a large selection of the time series feature operationalizations presented in \tblref{tab:esmFeatures} in our GitHub repository (see the \texttt{featureExtractor} function).

\input{tables/esmPsychFeatures}

\input{latex-results/03FeatureExtraction.tex}

It is important to reiterate that the six selected time-series features are in no way exhaustive or imperative. Both using a more data-driven approach to the selection of features or selecting entirely different aspects to summarize the time series are legitimate options \citep[e.g., see][]{heylen2016}. Our choice seeks to offer a practical toolbox of features that are common and meaningful to psychological research questions and -practice but are also easy to extract and summarize a broad range of developments without asserting strict assumptions.

\subsection{Feature Reduction}
Once a meaningful selection of time series features has been extracted for each variable and participant, the total number of data points sometimes remains too large for the desired clustering algorithm. As an example, a relatively common scenario would include 10 variables of interest, where eight time series features are extracted, resulting in 80 features per participant (with a common sample size of 100 participants that would result in a total of 8,000 data points in this hypothetical example). We offer an illustration of the compounding numbers of data points in \fgrref{fig:TSCFlowN}. The difficulty of finding stable clusters for data with a large number of dimensions is sometimes termed the `dimensionality curse' \citep[e.g.,][]{altman2018}. 

To deal with this dimensionality issue, two main approaches have been proposed --- feature selection and feature projection \citep[e.g.,][]{erdogmus2008}. While feature selection refers to the process of identifying and selecting a subset of relevant features from the original feature set \citep{alelyani2014}, feature projection refers to the process of transforming the original feature set into a new feature set of lower dimensionality \citep[][]{carreira-perpinan1997}. In general, feature selection procedures have the benefit that they retain the interpretable feature labels directly and immediately indicate which features were most informative in the sample. Feature projection methods, on the other hand, have been popular because they are efficient, widely available, and applicable to a wide range of data types. We provide an overview of the common approaches, an intuitive introduction to common methods, and exemplar algorithms in \tblref{tab:featureReduction}. 

%Feature selections seek to create a subset of the most important features. The many available approaches differ in how they seek to determine the importance of the individual features \citep{alelyani2014}. Generally speaking, selection methods can be categorized as `filter', `wrapper', or `hybrid' methods \citep{hoque2014}. The filter methods, broadly speaking determine important features by identifying irrelevant features (e.g., because features do not capture much information), and identifying redundant features \citep[e.g., because features capture the same information; e.g., see][]{yu2004}. Wrapper methods, on the other hand, avoid the feature-based evaluation and focus on the performance of the later model to identify the most important features \citep{kohavi1997}. A wrapper, thus, compares the performance of models with different feature combinations \citep[e.g., see][]{tang2014}. Traditional examples of wrappers are forward selection or backward elimination procedures. Because filter methods might not always perform well and wrapper methods are computationally intensive\footnote{Computationally \textit{k} features could be considered in \(2^k – 1\) possible combinations --- for the example of 80 features that would allow for \(1.20 \times 10^{24}\) (over one septillion) combinations.}, hybrid methods seek to combine the two methods and find a balance between computational effort (i.e., efficiency) and performance \citep[i.e., effectiveness; e.g.,][]{alelyani2014}. Methods might, for example, use a filter step to reduce the size of features considered in a later wrapper step \citep{hsu2011}. Feature selection procedures have the benefit that they retain the interpretable feature labels directly and immediately indicate which features were most informative in the sample.

%The second general approach to the dimensionality curse has been feature projection. Broadly speaking, projection methods seek to transform the many features in such a way that a much smaller number of new variables can accurately capture the variance and structure of the original features \citep[i.e., the data is projected to a lower dimensional space; e.g.,][]{carreira-perpinan1997}. Projection methods are relatively common in psychological research --- including, factor- and principal component analyses \citep{vandermaaten2009}. Commonly, this is achieved using linear transformations \citep[e.g., principal component analysis; see][]{cunningham2015}, or more complex nonlinear transformations \citep[e.g., t-SNE; see]{lee2007}. Feature projection methods have been popular because of their widespread availability as well as their high efficiency.

\input{tables/FeatureReduction}

\input{latex-results/04FeatureReduction.tex}

\subsection{Feature Clustering}
For the actual clustering of the time-series features, the main aim is to organize participants into groups so that the features of participants within a group are as similar as possible, while the features of people in different groups are as different as possible \citep{liao2005}. The crux of clustering is, thus, to have clearly defined and effective measurements of (dis)similarity. Most of the clustering algorithms used today use some form of distance measurement to optimize group assignment \citep[or similarity measurement for qualitative features; see][]{Aghabozorgi2015}. While others have produced excellent overviews of the many clustering approaches available \citep[e.g.,][]{xu2015}, the more readily available approaches suitable for most time series feature data can, broadly speaking, be categorized as based on (1) centroids, (2) distributions, (3) density, (4) hierarchies, or (5) a combination thereof \citep[see \tblref{tab:clusterApproaches} for an overview; also see][for a broader review]{jain1999}. 

There is, unfortunately, no one-size-fits-all solution to clustering and users will usually have to make an informed decision based on the structure of their data as well as an appropriate weighing of accuracy and efficiency. We provide a short intuitive explanation for common approaches, together with some of their characteristics and example algorithms in \tblref{tab:clusterApproaches}. For our own illustration, we have chosen the centroid-based k-means clustering. While k-means comes at the expense of high accuracy with more complex cluster shapes, we specifically chose k-means because it is an extremely efficient method that works well with large participant- and feature numbers without making too many restrictive assumptions about the shape of the clusters \citep{jain2010}. K-means is also well-established within the research community and has been readily implemented in many statistical software packages \citep{hand2005}. Additionally, many of the feature selection methods have specifically been designed for the well-established k-means algorithm \citep[e.g.,][]{boutsidis2010}. As such, the k-means offers a good starting point for many psychological researchers and the method should be generalizable across a relatively wide variety of projects.

%Each of these approaches can be a valuable clustering approach for time series feature data and users will usually have to make an informed decision based on the structure of their data as well as an appropriate weighing of accuracy and efficiency. As an example, under ideal conditions, most approaches are likely to provide a similar cluster solution \citep[e.g., for well-separated groups with little noise and few outliers; e.g., see][]{peng2022}. However, when the shapes of clusters, for example, become more complex in a multi-dimensional space, density-based or hierarchy-based approaches that allow for more bottom-up clustering are likely to be more accurate \citep[e.g.,][]{langfelder2008}. Yet, with higher numbers of participants and features, both density- and hierarchy-based approaches may perform less well and the more efficient centroid-based methods might be more effective \citep[e.g.,][]{jain2010}. Similarly, several of the proposed time series features are statics that are generated from Gaussian distributions and a distribution-based algorithm might be ideally suited to separate such distributions \citep[e.g.,][]{corduas2008}. Yet, in cases where misspecifications are costly, a method with fewer assumptions might be more advisable \citep[e.g.,][]{ankerst1999}.

\input{tables/clusterApproaches}

\input{latex-results/05FeatureClustering.tex}

\subsection{Cluster Evaluation}
Now that the participants have been assigned to their respective clusters based on the similarity of their time series features, the final evaluation step includes two main elements, (1) evaluating the performance of the clustering analyses to choose an optimal solution and (2) interpreting the extracted clusters conceptually. 

\subsubsection{Performance}
Performance evaluation often means assessing the accuracy, stability, and separation or purity of the clustering \citep{keogh2003}. Importantly, any evaluation of the results depends on the research questions, the data, and the methods used. However, broadly speaking, evaluation methods can be categorized based on whether the true cluster labels are known or not \citep{saxena2017}. If true class labels are known, the cluster assignments can be compared to the true class labels --- using measures such as the F-measure, adjusted Rand index, mutual information, and normalized mutual information \citep[i.e., external evaluation; e.g.,][]{liao2005}. However, if the true cluster assignments are unknown, as with our psychological time series, the quality of the clusters is assessed based on the characteristics of the data itself, such as separation and homogeneity of the clusters, or goodness of fit indices \citep[i.e., internal evaluation; e.g.,][]{Aghabozorgi2015}. 

\input{latex-results/06ClusterEvaluationPerformance.tex}

\subsubsection{Interpretation}
The interpretation of feature-based time series clustering in psychology involves understanding the meaning and implications of the obtained clusters. In order to make sense of the clustering results, we here focus on three general aspects of the results \citep{kaufman1990}. (1) Assessing differences between the clusters in the original time series features, (2) comparing the clusters based on prototype developments, (3) comparing the clusters based on between-person differences that were not included in the initial clustering.

\input{latex-results/07ClusterEvaluationInterpretation.tex}

\newpage
\section{Discussion}
% aims re-iterated
The purpose of this article was to introduce feature-based time series clustering as an amenable and transparent approach to understanding between-person differences in developmental patterns of psychological time series data. Rather than relying on person-specific model parameters, which can be restrictive and assumption-bound, we argue for the more flexible and theoretically grounded approach of directly clustering on important and relevant features of the time series data. By leveraging the rich array of dynamic measures, our approach offers the advantages of flexibility, fewer strict assumptions, and improved interpretability, thus potentially enriching our understanding of heterogeneous psychological processes in intensive longitudinal studies.

% summary
To illustrate the practical utility of the approach, we applied the method to real-world empirical data that highlight common ESM issues of multivariate conceptualizations, structural missingness, and nonlinear trends \citep[e.g.,][]{ariens2020}. With the real-world data, we followed a stepwise approach to discuss key issues during input selection, feature extraction, feature reduction, feature clustering, and cluster evaluation. Within this step-wise approach, our article shows that feature-based clustering offers an excellent fit for psychological research practice, as both the features and the analysis steps are well established within the field, and statistical packages are readily available. Time series features (such as means or linear trends) are not only easy to extract, but also hold conceptual meaning for psychological data and can be chosen to address specific research questions (also see \tblref{tab:esmFeatures}).

Importantly, we show that feature-based clustering is not only approachable but provides interpretable and transparent insights about the grouped patterns. For our example of migration experiences, the method was useful to discern adaptive from more stressful experiences and helped to contextualize divergent experiences. We found that some variables, such as interaction quality perceptions or need fulfillment were particularly important in distinguishing the groups (see \fgrref[A]{fig:clusterFeatVar}). Similarly, we found that the central tendency (\textit{median}), variability (\textit{MAD}), and linear trend (\textit{slope}) were the most impactful dynamic features in discerning the trajectory clusters (see \fgrref[B]{fig:clusterFeatVar}). Jointly these two approaches allowed us to identify a cluster that had generally positive and improving experiences while the other cluster had more negative and deteriorating experiences. We were even able to further contextualize the results with out-of-feature comparisons, where we found that the group with the more difficult experiences also reported substantially more discrimination experiences during the post-test (see, e.g., \fgrref[B]{fig:clusterFeatVar}). In short, the feature-based approach allowed us to identify directly interpretable and meaningful groups, where we transparently know what data input the clusters are based on.

\subsection{Limitations}
While feature-based time series clustering offers a promising approach to understanding psychological time series data, it is not without limitations. In particular, feature-based clustering has both usability- and robustness limitations across its multiple steps. 

In terms of convenience, each of the steps requires users to make an informed decision about the choice of method and algorithm. These additional steps of decision-making and transparency increase the initial barrier to entry. We hope that our empirical illustration, the sample code, and the custom functions, offer a relatively generalizable procedure that showcases the ease of use, but clustering, unfortunately, does not offer a universal one-size-fits-all solution. 

In terms of methodological robustness, the variety of methods in each of the steps also brings with it the potential inconsistent results between methods \citep[e.g.,][]{bastiaansen2019}. A different set of variables, features, or a different clustering algorithm, might have resulted in substantially different cluster assignments. While the variety and diversity of methods are helpful in finding options even for more complex types of data, different algorithms often offer different results \citep[e.g.,][]{keogh2005}. And even when patterns produce robust clustering solutions across algorithms, individual methods might still have their idiosyncratic shortcomings \citep{xu2015}. 

As an illustration, the choice of features to extract from the time series data is a critical step that can significantly influence the results of the clustering process. In the current example, we chose to extract features such as means, autocorrelations, and linear trends, which are psychologically and conceptually meaningful in interpreting our time series clusters. However, this selection is not exhaustive and may not capture all relevant aspects of the time series data. For example, we did not consider attributes like periodicity or spectral density, which could shed light on the data's cyclical patterns. The choice of features largely hinges on the researcher's specific research question and assumptions about the data, thereby injecting a level of subjectivity into the process. Similar challenges arise with the choice of the clustering algorithm or the cluster illustration. These challenges are not unique to feature-based clustering, rather they are common to all clustering approaches \citep{liao2005, horne2020}. However, it is important to remember that multi-stepped data-driven approaches are particularly vulnerable to the impact of the researchers' degrees of freedom.

One potential remedy to many of the limitations of feature-based clustering lies in transparently and reproducibly reporting the decisions of the user for each of the analysis steps. In our own description of the method, we have provided a range of options and have motivated our own choices to facilitate the transparency of the individual steps and decision moments. Beyond the structures proposed here, \citet{vandeschoot2017} have proposed an extensive checklist for latent trajectory studies. Most of their recommendations and reporting guidelines also apply to feature-based clustering and might even offer a template for researchers who hope to preregister their analysis procedures \citep[also see][]{kirtley2021}.

\subsection{Implications}
Notwithstanding the limitations, we believe that feature-based clustering offers exciting new potential for researchers and practitioners assessing psychological time series. 

For researchers, the feature-based time series clustering approach offers a number of compelling implications. The flexibility and interpretability mean that feature-based time series clustering can be applied to a wide range of data types and research questions. 
The method can be used to contextualize preexisting groups by extracting their time series features and comparing a data-driven approach with existing group labels. Furthermore, the feature-based approach can also be used as an exploratory, descriptive, or predictive approach to intensive longitudinal data. By reducing the complexities of ESM data to important and meaningful patterns, a bottom-up approach can aid in the creation of more embedded theories and interventions, or simply in describing the often complex and heterogeneous data researchers collect during ESM studies.

Looking ahead, the feature-based time series clustering approach opens up new avenues for future research. While the approach has shown promise in dealing with the challenges of dimensionality, missingness, and time scales, there is potential for further refinement and expansion. To showcase the exciting potential for future methodological integrations, we will briefly consider the broad range of alternative approaches to time series clustering (see \fgrref{fig:tsClustTax}).

For instance, given that the approach does not assume the stationarity restrictions of many model-based approaches, future research can now more easily integrate many of the (non-)linear trend features. Research on capturing nonlinear trends has been growing over the past years and there are exciting possibilities to bring these developments to ESM data \citep{bringmann2023}. For example, bicoherence metrics, polynomial-, and differential equation parameters may be used to capture the type and structure of nonlinear developments \citep[e.g. shape-based approaches in \fgrref{fig:tsClustTax}; see also][]{caro-martin2018, mayor2022}. New features capturing nonlinear structures would add to the under-studied (non-)linear trend features of ESM data.

% Additionally, feature-based clustering algorithms traditionally do not provide a cluster-level prototype trajectory. Instead, users have to extract, often suboptimal, approximations of the group-level patterns (e.g., see the section on \textit{Cluster Evaluation}). Iterative or embedded clustering approaches, such as mixture models or group-based trajectory models, often estimate a group-based model and variance around that group-based model \citep[see \fgrref{fig:tsClustTax} and \tblref{tab:clusterApproaches}; e.g.,][]{denteuling2021, lane2019}. To provide similar functionality for feature-based models, researchers may evaluate the utility of using recurrent neural networks, such as long short-term memory (LSTM) models to generate cluster-level trajectories. One could, for example, train an LSTM based on the feature matrix and the original time series as the target \citep[e.g., see][]{fraley2002, nagin1999}. Alternatively, rule-based classifiers could extract rule sets that narrow down the conditions under which a given feature set is assigned to a specific cluster \citep[][]{benard2019}. This could potentially allow us to generate cluster-level trajectories, giving us an estimate of the typical trend for each group.

\begin{figure}[!hbtp] %hbtp
  \caption{Time Series Clustering Taxonomy}
  \label{fig:tsClustTax}
  \centering\includegraphics[width=\textwidth]{figures/TS Cluster Flow/tsClustTax.pdf}
  \caption*{Note: \\
  The taxonomy only exemplifies some of the basic differences between a number of common time series clustering approaches. As such, the taxonomy and the notes are neither exhaustive nor complete in distinguishing different approaches. Additionally, terms and labels are used inconsistently across different types of literature and are chosen to avoid overlapping labels.}
\end{figure}

Beyond the direct academic use, the feature-based time series clustering approach also addresses practical and applied uses. For practitioners, the approach offers a practical and grounded method for dealing with the challenges of complex and messy data from multiple patients, customers, or users. Not only does the approach directly deal with dimensionality, missingness, and time scales in the time series, but the interpretability and transparency aspects offer particular utility in applied settings, where the costs of misspecification are high. Additionally, the approach is also more readily accessible to practitioners who may not have extensive training in complex data analysis techniques. We provide practical algorithm overviews and readily available code for data preparation, analysis, and interpretation. The ability to identify and interpret meaningful patterns in time series data can have significant implications for practice, particularly in fields such as clinical, organizational, or social psychology, where understanding individual differences and developmental patterns can inform interventions and decision-processes.

% conclusion
In conclusion, we show that feature-based time series clustering can effectively reduce the complexities of psychological time series data to important and meaningful patterns. It does so with more flexibility, versatility, and less strict assumptions than many of the commonly used approaches to date. As such the feature-based time series clustering approach addresses key challenges in the field and aids researchers and practitioners in describing and exploring patterns across participants. We hope that the method adds to the methodological toolkit of ESM researchers and promotes the creation of more embedded methods, theories, and interventions.


% Tables
% Example
%\input{Tables/descrFullWide}


% Figures


\printbibliography

\appendix

\section{ESM Data Challenges and Promises}
\label{app:ChallengesAppendix}

\subsection{Promises}

Time series clustering has a number of conceptual use cases with psychological data. Prime among them is the ability to reduce the time, variable, and person complexity by extracting and organizing participant-level structures. These reduction and structuring qualities can be essential in detecting phenomena and extracting more abstract functional principles \citep[][]{eronen2021a}. These phenomena and principles can be meaningful differences that distinguish participants in different clusters, as well as important patterns, trends, and relationships that participants share within a cluster \citep[e.g.,][]{schrodt2000}. Once distinct groups and patterns have been identified, researchers can examine the extent to which these within-group and between-group structures are associated with other variables of interest, such as personality traits, demographic characteristics, or other psychological constructs \citep[e.g.,][]{monden2022}. By detecting meaningful and robust structures and patterns, time series clustering can, thus, be used to inform the development of robust theories as well as targeted interventions and therapies for individuals, for example, with mood disorders and other psychological conditions \citep[e.g.,][]{borsboom2021, eronen2020}.

However, while clustering can be incredibly useful, arriving at these clusters critically depends on two core challenges. First, time series need to be made comparable in order to identify key (dis)similarities and second, comparable (dis)similarities need to be accurately distinguishing into different groups \citep[e.g.,][]{Aghabozorgi2015}. In practice, most psychological time series cannot be compared based on the raw data itself. This is the case because in most cases the raw time series include too many data points --- sometimes referred to as the dimensionality curse \citep[e.g.,][]{altman2018} --- and, more importantly, individual time points are oftentimes not directly comparable between participants in psychological data and would lead to misspecifications \citep[e.g., ][]{faloutsos1994}. While such issues can be avoided with transformations for highly regular, controlled, and comparable time series such as EEG data \citep[e.g.,][]{huang1985}, most ESM researchers are usually not interested in directly comparing individual timepoints between participants but are interested in developmental patterns and relationships. 

As a result, most psychological time series are summarized via a numerical representation and these numerical summaries are then comparable and used to cluster participants (e.g., \citealp[]{timmerman2013}; see \fgrref{fig:tsClustTax}). Ideally, the representations that summarize the original time series data should (1) capture the original data accurately without loosing too much information, and (2) should be conceptually meaningful \citep[][]{vandermaaten2009}. Extracting accurate and meaningful representations of the time series can be essential for understanding what goes into the clustering algorithm (i.e., assists with explainability) and can be crucial in making sense of the final cluster output \citep[i.e., assists with interpretability; e.g.,][]{Kennedy2021}. 

\subsection{Challenges}

We will briefly consider which challenges modern ESM data introduce and what qualities are called for in an extension of the clustering repertoire. We particularly highlight issues of dimensionality, non-equidistant or missing measurements, an interest in non-stationary trends, as well as inconsistent/diverse time scales. 

Concerning dimensionality issues, especially more abstract psychological experiences often need a wider variety of measurements to be captured adequately. Today, few clinical conditions are captured with a single symptom measure \citep[e.g.,][]{cramer2016}, emotions are rarely assessed in isolation \citep[e.g.,][]{reitsema2022}, and socio-cultural experiences are now widely considered to be multimodal \citep[e.g.,][]{Kreienkamp2022d}. This also means that modern analysis techniques increasingly need be able to accommodate an increased focus on multivariate developments. At the same time, however, an increase in the number of considered variables tends to come at the expense of computational load for model estimations, and clustering models may not converge \citep[the aforementioned dimensionality curse;][]{altman2018}. A modern time series clustering technique should consequently be able to summarize and structure multivariate phenomena without running into computational load issues.

Another common type of data are measurement regiments that collect data in irregular time intervals (i.e., non-equidistant measurements). Common are, for example, procedures where participants are asked to respond at random times throughout the day (i.e., signal-contingent) or following specific natural events of interest \citep[i.e., event-contingent; see][]{shiffman2008, myin-germeys2018}. Under such conditions data tends to violate the equidistance assumption that is expected by many time series models \citep[][]{hamaker2017}. Smaller issues of non-equidistant data can be avoided with transformations \citep[e.g., dynamic time warping,][]{berndt1994} or newer modeling procedures \citep[e.g., continuous-time models;][]{dehaan-rietdijk2017} but for many analyses, including some cluster approaches, non-equidistant measurements remain a prevalent issue. 

Structural missingness remains an even more strenuous challenge. Structural missingness occurs when data is missing because it logically cannot be collected \citep[as opposed to probabilistically missing data;][]{little2020, mclean2017}. Often, however, researchers might want to include variables in their models that are not available under all conditions. Follow-up and event-contingent questions are a common example in ESM studies. Researchers, for example, ask about the frequency, intensity, or duration of symptoms --- but only if a symptom was present \citep{kivela2022}. Such approaches become specifically critical in cases of sensitive questions such as questions about suicidal ideation or other potentially trauma-inducing questions \citep[e.g.,][]{glenn2022}. The most common practice for structurally missing data is to either exclude the variable or any measurement that has no structurally missing data \citep[e.g.,][]{lavori2008}\footnote{This is the case because the most commonly used models require complete data \citep{schafer2002} and structurally missing data cannot be imputed as it logically does not exist \citep[e.g.,][]{lavori2008}.} --- neither option suits a research question that wishes to include variables with common structural missingness, such as event-specific or follow-up questions. In short, new clustering approaches should be able to deal with structurally missing data in order to address modern ESM data.

When it comes to studying developmental trajectories, psychological researchers are often also interested in nonstationary processes because they are more representative of the complex, dynamic patterns of the human mind. In psychology, nonstationary processes are typically used to study phenomena such as cognitive development \citep[][]{quartz1997}, decision-making \citep[][]{ratcliff2016}, and emotion dynamics \citep[][]{bringmann2018b}. These processes are often characterized by changes in the underlying statistical properties of the data over time, such as changes in the mean or variance \citep[][]{molenaar2009}. Especially when considering changes in mean levels, researchers are often interested in nonlinear changes because they describe human functioning better. For example, in decision making people might switch between choices \citep[][]{ratcliff2016}, or patients reducing medication might experience mood swings \citep[][]{helmich2020a}. Similarly, psychologists are often also interested in how variances change over time. This is especially the case because several changes in an individual's variance have been found to be indicative of critical changes, including depression relapses and symptom shifts more generally \citep[e.g.,][]{schreuder2020, wichers2020}. There is, thus, also a need for time series clustering algorithms that capture nonstationary processes, including nonlinear trends.

Psychological time series often exhibit complex patterns and relationships that can change over different time scales. For example, a time series of daily mood ratings may show a weekly pattern, with higher ratings on the weekends and lower ratings during the week. At the same time, the series may also exhibit a longer-term trend, with overall mood levels increasing or decreasing over the course of several months or years \citep[e.g.,][]{Ram2014}. These different time scales can be studied separately or in combination, using different statistical techniques and modeling approaches \citep[][]{bertenthal2007, jeronimus2019a}. Different time scales can become an even more difficult issue when different variables in a model develop on different time scales \citep{bringmann2022b}. Different time scales are thus also a concern clustering approaches should be able to address.

It is this background of the common challenges of current ESM data, upon which we propose to consider feature-based clustering. The flexibility of using a wide variety of features that represent the important developmental patterns allows users to circumvent many of the issues with multi-dimensionality, non-equidistant or missing measurements, non-stationary trends, as well as diverse time scales.

%These shortcomings, however, stand in sharp contrast with the types of data researchers commonly collect and do not align with common research interests in the field. Psychological researchers might, for example, collect data based on the occurrence of natural events, which tends to result in non-equidistant measurements \citep[e.g.,][]{myin-germeys2018, hamaker2017} and context-specific missingness that cannot be imputed, for instance, interaction quality perceptions \citep[e.g.,][]{kivela2022, lavori2008}. At the same time, researchers are often specifically interested in non-stationary developments, looking, for example, at how symptom levels and -variations change over time --- often abruptly and non-linearly \citep[e.g.,][]{bringmann2018b, helmich2020a}.


\end{document}
