% define document type (i.e., template. Here: A4 APA manuscript with 12pt font)
\documentclass[man, 12pt, a4paper, mask]{apa7}

% change margins (e.g., for margin comments):
%\usepackage{geometry}
% \geometry{
% a4paper,
% marginparwidth=30mm,
% right=50mm,
%}

% add packages
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage[style=apa, sortcites=true, sorting=nyt, backend=biber, natbib=true, uniquename=false, uniquelist=false, useprefix=true]{biblatex}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{setspace,caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{fourier}
\usepackage{stackengine}
\usepackage{scalerel}
\usepackage{fontawesome5}
\usepackage[normalem]{ulem}
% \usepackage{longtable}
\usepackage{amsmath, nccmath}
\usepackage{mdframed}
\usepackage{ntheorem}
\usepackage{afterpage}
\usepackage{float}
\usepackage{array}
\usepackage{censor}
\usepackage{pdflscape}
\usepackage{lscape}
\usepackage{pdfpages}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{tabu}

% make warning with red triangle
\newcommand\Warning[1][2ex]{%
  \renewcommand\stacktype{L}%
  \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries !}}{#1}}%

% make question with red triangle
\newcommand\Question[1][2ex]{%
  \renewcommand\stacktype{L}%
  \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries ?}}{#1}}%
  
% add definition sections
\theoremstyle{break}
\newtheorem{definition}{Definition}

% add hypothesis sections
\theoremstyle{plain}
\theoremseparator{:}
\newtheorem{hyp}{Hypothesis}

\newtheorem{subhyp}{Hypothesis}
   \renewcommand\thesubhyp{\thehyp\alph{subhyp}}

% add quote section
\usepackage{csquotes}

% framed box section
\usepackage{framed}
\emergencystretch=1em

% formatting links in the PDF file
\hypersetup{
pdfpagemode={UseOutlines},
bookmarksopen=true,
bookmarksopenlevel=0,
hypertexnames=false,
colorlinks   = true, %Colours links instead of ugly boxes
urlcolor     = blue, %Colour for external hyperlinks
linkcolor    = blue, %Colour of internal links
citecolor   = cyan, %Colour of citations
pdfstartview={FitV},
unicode,
breaklinks=true,
}

% ref labels
\newcommand{\fgrref}[2][]{\hyperref[#2]{Figure \ref*{#2}#1}}
\newcommand{\tblref}[2][]{\hyperref[#2]{Table \ref*{#2}#1}}
\newcommand{\appref}[2][]{\hyperref[#2]{Appendix \ref*{#2}#1}}

% custom open science badge height
\newlength{\badgeheight}
\setlength{\badgeheight}{1em}

% language settings
\DeclareLanguageMapping{american}{american-apa}

% add reference library file
\addbibresource{referencesZotero.bib}

% Title and header
\title{Describe and Explore: Using feature-based time-series clustering to understand modern intensive longitudinal data (Working Title)}
\shorttitle{Migration Experience Trajectories}

% Authors
\author[*,1,3]{Jannis Kreienkamp}
\author[1,3]{Laura F. Bringmann}
\author[1,3]{Kai Epstude}
\author[1,3]{Maximilian Agostini}
\author[1,3]{Peter de Jonge}
\author[2,3]{Rei Monden}
\affiliation{\hfill}

\affil[1]{University of Groningen, Department of Psychology}
\affil[2]{Hiroshima University, Graduate School of Advanced Science and Engineering}
\affil[3]{Author order TBD (currently in first name alphabetical order)}

\authornote{   
   \addORCIDlink{* Jannis Kreienkamp}{0000-0002-1831-5604}\\
   \addORCIDlink{Laura F. Bringmann}{0000-0002-8091-9935}\\
   \addORCIDlink{Kai Epstude}{0000-0001-9817-3847}\\
   \addORCIDlink{Maximilian Agostini}{0000-0001-6435-7621}\\
   \addORCIDlink{Peter de Jonge}{0000-0002-0866-6929}\\
   \addORCIDlink{Rei Monden}{0000-0003-1744-5447}

We have no known conflict of interest to declare. The authors received no specific funding for this work. Materials and  software is available at \url{https://janniscodes.github.io/migration-trajectories/}  \citep{KreienkampTBD}. Protocols, materials, data, and code are available at \url{https://osf.io/TBA} \citep{KreienkampTBD}. The preregistration of our analysis can be accessed as part of our Open Science Framework repository \citep{KreienkampTBD}.

Correspondence concerning this article should be addressed to Jannis Kreienkamp, Department of Psychology, University of Groningen, Grote Kruisstraat 2/1, 9712 TS Groningen (The Netherlands).  E-mail: \href{mailto:j.kreienkamp@rug.nl}{j.kreienkamp@rug.nl}
}

\leftheader{Kreienkamp}

% Abstract
\abstract{
Abstract to be written.

\noindent\textbf{Public significance statement}: To be written.

}

\keywords{
    TBD\\
    \vspace{1em}
    \textit{Open Science Practices [?]:}
    \noindent \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/registration-color.png}} Preregistration, 
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/material-color.png}} Open Materials, 
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/data-color.png}} Open Data, \break
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/code-color.png}} Open Code, 
    \href{https://osf.io}{\includegraphics[height=\badgeheight]{assets/open-badges-small/supplements-color.png}} Open Supplements
}


% set indentation size
\setlength\parindent{1.27cm}

% Start of the main document:
\begin{document}

% add title information (incl. title page and abstract)
\maketitle

% **CHEAT SHEET / LEGEND**
%
% Comments:
% '%' starts a comment in LaTeX (not printed)
% '\todo[inline]{} makes orange boxes in PDF
% '\marginpar{}' notes in margins
% '\footnote{}' footnote
% '\Warning' important note indicator in PDF (triangle with exclamation mark)
% '\Question' question note indicator in PDF (triangle with question mark)
%
% Citation (with Natbib citation style):
% '\citep[e.g.][p. 15]{CitationKey}' citation in parentheses "(e.g., Berry, 2003, p. 15)"
% '\citet{CitationKey}' citation in text "Berry (2003)"
% '\citealt' and '\citealp' alternate citation without parentheses
% '\citeauthor' and '\citeyear' only year or author
% 
% Headings:
% '\part{}' and '\chapter{}' only relevant for multi-part or multi-chapter documents
% '\section{}' heading level 1
% '\subsection{}' heading level 2
% '\subsubsection{}' heading level 3
% '\paragraph{}' heading level 4
% '\subparagraph{}' heading level 5
%
% formatting:
% '\textbf{}' text bold font
% '\textit{}' text italic font
% '\underline{}' text underline
% '\sout{}' text strike out
% '\textsc{}' text small caps
% '\vspace{1em}' add vertical space
% '\hspace{1em}' add horizontal space
% '\\' new line (i.e., line break)
% '\pagebreak' start new page (i.e., page break)
% '\noindent' do not indent current line (e.g., current paragraph)
% 'begin{center}...end{center}' center text or object
%
% Math mode:
% '$\alpha = .8$' mathematical equation inline
% '$$\hat{y} = b_0 + b_1x$$' mathematical equation in its own line
% '\begin{equation}...\end{equation}' multi-line equation
% '\approx' approximate symbol
% '\neq' not equal
% '\bar' mean bar over letter
% '\pm' plus minus sign 
% '^{}' superscript
% '_{}' subscript
% '\fraq{numerator}{denominator}' fraction
% '\sqrt[n]{}' square root
% '\sum_{k=1}^n' sum for 1 through n
%
% Insert things from elsewhere:
% '\input{filename}' inputs the raw (tex) file as a command (e.g., tables and R-Markdown imports)
% '\include{filename}' includes section on new page (incl. possible auxiliary info)
% '\includegraphics[settings]{filename}' add a figure or graph
% '\caption{}' adds a caption to a table or figure
% '\label{}' labels sections, tables, figures, etc. so that they can be referred to.
% '\ref{}' refer to a labelled sections, tables, figures, etc.
% '\begin{enumerate}...\end{enumerate}' numbered list
% '\begin{itemize}...\end{itemize}' bullet-ed list
% '\item' item in list section 
%
% Symbols:
% '\&' and sign
% '\%' percent sign
% '\_' three dotes
% '\#' hash symbol
% ------------------------------------------------------------------

\newlength{\mdfmar}
\setlength{\mdfmar}{1.5em}
\mdfdefinestyle{mdfbox}{
    innerleftmargin = +\mdfmar, %
    innerrightmargin = +2\mdfmar, 
    innertopmargin = +0\mdfmar, 
    innerbottommargin = +1.5\mdfmar, 
    skipabove = -12pt
}

\begin{mdframed}[style=mdfbox]
\noindent\center\textit{Structure}:
\begin{itemize}[nosep]
    \item introduction
    \begin{itemize}[nosep]
        \item\ [Relevance]: increase in number and variety of ESM data
        \item\ [Problem]: no descriptive / understand structure analyses
        \item\ [Proposal]: feature based clusterting = flexible +  transparent
    \end{itemize}
    \item Modern ESM data: new type of data and issues for analysis:
    \begin{itemize}[nosep]
        \item incomplete data (complete data models vs. structural missingness)
        \item non-equidistant
        \item non-stationary
        \item changes/trends on different time scales
        \item interest in multivariate developments
        \item dimensionality curse
    \end{itemize}
    \item Time series clustering
    \begin{itemize}[nosep]
        \item shape-based
        \item model-based
        \item \textbf{feature-based}
    \end{itemize}
    \item Feature-Based Clustering
    \begin{itemize}[nosep]
        \item input variables
        \item feature extraction
        \item feature reduction
        \item feature clustering
    \end{itemize}
    \item Empirical Illustration
    \begin{itemize}[nosep]
        \item Data
        \item Analysis and Results
    \end{itemize}
    \item Discussion (summary, limitation, implications, conclusion, etc.)
\end{itemize}
\vspace{1em}
\end{mdframed}

% Relevance Pragraph and issue
Recent years have seen a striking increase in the number and variety of research studies using experience sampling data \citep[][; also see \fgrref{fig:ScopusEsm}]{hamaker2017}. 
With the increased availability of technologies to easily collect large amounts of experience sampling data using mobile devices \citep[e.g.,][]{Keil2020} and web-based applications \citep[e.g.,][]{Arslan2020}, the experience-sampling method (ESM) has been applied to a host of new types of psychological data. 
This move toward a broader application of the experience-sampling method, however, stands in stark contrast with recent developments within the psychometric literature on ESM analyses, which have become more specialized and model-focused. 
While generally speaking analytical methods for ESM data have become more readily available \citep[e.g.,][]{ODonnell2021}, flexible descriptive and exploratory analyses have largely been neglected and remain crucially understudied within the experience sampling literature. 

% Problem Illustration Paragraph
Importantly, the understudied `describe and explore' analyses are not only important for contextualizing inferential model tests, but are powerful methods for theoretical and applied users in their own regard. Extensive longitudinal data sets come with new forms of heterogeneity, where researchers have to consider differences across large numbers of participants, time points, and variables. Data-driven models not only describe and summarize this complex data but can help us understand the data by uncovering patterns that might not be detectable otherwise. As an example, understanding how people differ in their trajectories can be crucial in understanding adaptive and maladaptive patterns. By identifying and contextualizing distinct trajectories, we might for example determine which situation- or person-specific differences discern positive from negative developments. Outcomes from such an approach might inform both practical early-warning signals as well as academic theory-building efforts. There is, thus, a clear need to assess the utility of data-driven/bottom-up analysis procedures that are flexible enough to address the growing variety of intensive longitudinal psychological data. 

% Aim / Solution / Proposal Paragraph:
In this article, we propose to look at recent developments within the wider time-series clustering literature to identify an exploratory data analysis that is flexible enough to apply to the growing variety of ESM data. In particular, we propose that feature-based clustering is ideally suited because it offers a flexible variety of validated and intuitive components while also allowing for adequate control by the researcher. To showcase the utility of this technique, we apply feature-based clustering to a set of three ESM studies with complex conceptualizations and missingness patterns. 

\section{Modern ESM Data}
\centerline{<--- \textit{still horrible, probably restructure section} --->}

Two important developments within the psychological sciences have been (1) a shift towards more comprehensive assessments of psychological phenomena and (2) a growing focus on intensive longitudinal real-life data. Multivariate experience-sampling approaches allow researchers to test theories more exhaustively and help build a more embedded understanding of human experience processes. However, such data also deviates from past intensive longitudinal data sets that have focused on psychological states that are more homogeneous in their developmental characteristics. In particular, the new applications of ESM tend to bring new challenges for data dimensionality, -missingness, and time scales. 

Concerning dimensionality issues, especially more abstract psychological experiences often need a wider variety of measurements to be captured adequately. Today, few clinical conditions are captured with a single symptom measure, emotions are rarely assessed in isolation, and socio-cultural experiences are now widely considered to be multimodal. This also means that modern analysis techniques increasingly need be able to accommodate an increased focus on multivariate developments. The recent rise in psychological network models has highlighted this need for more multivariate outcome measurements \citep[e.g.,][]{decancq2013}. Additionally, the increased number of variables also heightens the computational load for model estimations and, especially when the number of variables and measurements compounds, models may not converge --- an issue sometimes referred to as the dimensionality curse. A modern descriptive and exploratory analysis technique should consequently be able to summarize and structure multivariate phenomena without running into computational load issues.

The push for more contextualized models also presents new issues with missing data. More event-based assessments tend to encounter issues when models require time intervals between measurements to be identical (i.e., equidistant measurements) and an increased focus on context-specific phenomena has lead to issues when data is only available under certain conditions, including follow-up questions (i.e., structurally missing data). Smaller issues of non-equidistant data can be avoided with transformations \citep[e.g., dynamic time warping,][]{berndt1994} or newer modeling procedures \citep[e.g., continuous-time models][]{dehaan-rietdijk2017} but it's prevalence is indicative of the structure of real-world issues researchers seek to investigate. Structural missingness remains a much more strenuous challenge. Many of the most commonly used models need complete data \citep{schafer2002} and structurally missing data cannot be imputed as it logically does not exist \citep[e.g.,][]{lavori2008}. This type of missingness becomes a colossal issue when researchers are interested in how two things co-develop and one of the concepts has structural missingness under certain conditions. The most common practice for structurally missing data is to either exclude the variable or any measurement that has no structurally missing data \citep[e.g.,][]{lavori2008} --- neither option suits a research question that includes a developmental relationship between the two variables. In short, adequate descriptive approaches should be able to deal with non-equidistant and structurally missing data in order to address modern ESM data.

Finally, both the push for more multivariate conceptualizations and contextualized measurements have highlighted issues with how temporal developments are conceptualized. Firstly, researchers are increasingly interested in how mean levels of variables change over time (i.e., non-stationary), such trend data is often assumed to be non-linear, and with the increased focus on more diverse conceptualizations researchers are interested in relationships between variables that change on fundamentally different time scales. Many of the commonly used analyses for ESM data are stationary lagged regression models that assume stable means and variances over time (incl., vector autoregressive models, dynamic structural equation models, autoregressive integrated moving average models, and cross-lagged panel analyses) or use basic trajectory models (e.g., mixed effects models, spline regression models, and latent growth curve modeling). And even fewer analysis techniques allow for substantially different time scales between variables. A modern descriptive and exploratory methodology should thus be able to flexibly capture and comparable non-stationary and non-linear trends, also across different time scales.

\section{Time Series Clustering}
One promising approach to describing and exploring time-series data has been the idea of structuring data using clustering analyses. Clustering seeks to organize data into groups that are maximally different from one another while minimizing differences within the groups. Time-series clustering has been extremely common in other empirical fields, including analyses of astronomical, meteorological, and aviation pathways, biological and medical developments, as well as energy and finance patterns \citep{Aghabozorgi2015}. But also for psychological data time-series clustering is not entirely novel \citep[e.g.,][]{ernst2021}. However, most clustering introduced for psychological data has faced the dimensionality-, missingness-, and time scale issues described above.

Many of the common time-series clustering algorithms used today have been developed within the machine learning literature and have commonly been separated into three different approaches: (1) shape-based, (2) model-based, and (3) feature-based clustering \citep{hautamaki2008, liao2005}. Shape-based clustering analyses use the raw time series and seek to match time series by non-linearly stretching and contracting the time axes. The other two approaches do not use the raw time series directly but convert the data first. The model-based clustering has been the most commonly used method within the psychological literature and clusters the parameters of a parametric model fitted for each participant \citep[e.g.,][]{ernst2021}. The flexibility of such clustering is restricted to the assumptions of the model used, so that stationary models, for example, do not capture non-stationary data well. Finally, feature-based clustering analyses are the most flexible, as they allow users to summarize the time series data with a wide variety of features that capture the data characteristics of interest. The participants are then clustered on basis of the extracted features. To showcase the flexibility of this approach, a commonly used software package for this approach allows users to extract a total of 794 time series features \citep[][]{christ2018}. Time series features can be chosen by the user based on their research question and can, for example, include the mean and standard deviation of a time series, but also linear and non-linear trends, inertia-related features such as auto-correlations, or the stability of the time series (e.g., mean absolute change). Given the flexibility of the feature-based approach, users are able to apply the analysis across multivariate data, feature extraction mitigates issues of non-equidistant and structurally missing data, and the features can capture and compare across non-linear trends at different time scales.

\section{Feature-based Time Series Clustering}

The feature-based clustering can be structured into four main steps. (1) The selection and preparation of the input variables, (2) the extraction of the features that describe the time series, (3) an optional feature reduction step if there are too many data points for the clustering algorithms, and (4) the actual clustering of the time series features. We provide a conceptual overview that can be used alongside this section in \fgrref{fig:TSCFlow}. 

\subsection{Input variables}
Time series clustering starts with the selection and preparation of the variables of interest. While the selection will necessarily be field- and concept-specific, there are a few conceptual and methodological issues that should be considered. Conceptually, the included variables should adequately capture the concept of interest and should be meaningful to the understanding of the time series. There are, for example, calls that emotion dynamics should be assessed with a repertoire of positive and negative emotions \citep[e.g.,][]{dejonckheere2019}, migration experiences are fully captured with affect, behavior, cognition, and desire measurements \citep[e.g.,][]{Kreienkamp2022d}, and many health developments are commonly captured within the biopsychosocial domains \citep[e.g.,][]{suls2004}. At the same time, however, the added number of variables can become a methodological concern. Not only can redundant and irrelevant variables diminish the quality of the analyses, but with intensive longitudinal data the number of data points compounds across participants, measurement occasions, and variables so that additional variables can make many of the following steps substantially more difficult (also see \fgrref{fig:TSCFlowN}). 

Once the important variables have been selected, the data needs to be prepared for the analysis steps. Importantly, this not only means validating and cleaning the data (e.g., re-coding, removing duplicate or unwanted measurements) but also making the time series comparable. Two important steps are making the time-frames and response scales comparable across participants. Extracting features that describe the development of a month for one participant but 90 days for another participant might not be comparable for some psychological phenomena. This comparability is important on a conceptual level but the difference in data availability might also influence the clustering steps, where the participant with the shorter measurement period would, for example, have a larger number of missing values. It is thus generally advised to reduce the data to a common and comparable time frame. 

\subsection{Feature extraction}
The main aim of feature extraction is to describe the most important and meaningful aspects of a time series. There are, however, hundreds of possible features to describe psychological time series \citep[e.g., tsfresh][]{christ2018}. One approach to choosing relevant features would be to extract a large number of features and then assess which features are most effective at capturing differences in the time series. However, such an approach is not always advisable for psychological time series. For one, features should reduce the data dimensionality --- it would thus not necessarily be advisable to describe 60 ESM measurements with 180 time series features. More importantly, however, careful feature extraction can be crucial in the interpretability and explainability of the results. This is particularly the case when features have a conceptual or psychological meaning. Taking the concept of well-being as an example, psychologists might be interested in whether certain participants tend to have higher well-being in general (i.e., mean) or whether some participants fluctuate between extremely high and low well-being (i.e., variance). But psychologists might not necessarily be interested in the exact time point after which 50\% of the summed well-being values lie (i.e., relative mass quantile index) or how much different sine wave patterns within the well-being data correlate with one another (i.e., cross power spectral density). We would, thus, strongly advocate for a careful selection of time series features that are meaningful to the field and concepts.

Fortunately, past conceptual and empirical efforts offer valuable discussions of common time series features in psychological research. While the final selection of features should always be driven by the research questions and field-specific conventions, we can build a practical toolbox of meaningful time series features for psychological data. In particular, we propose to focus on six features based on common psychological research questions and recent works on affect dynamics \citep[e.g.,][]{dejonckheere2019, kuppens2017}. An overview of the proposed features, their substantive interpretations, and mathematical operationalizations is available in \tblref{tab:esmFeatures}.

The first two features are a person's \textit{central tendency} and \textit{variability}. Familiar statistics from probability theory, the two features sit at the heart of many fundamental psychological questions. ``Are some people happier than others?'' (i.e., difference in central tendency), and "Are people settled in their attitudes towards migrants or do they fluctuate over time?'' (i.e., difference in variability). Mathematically, both features have parametric and robust options to choose from (e.g., parametric: mean and standard deviation; robust: median and median absolute deviation). 

The third and fourth features describe the structure of the variability within the time series. In particular, \textit{(in)stability} captures the average change between two consecutive measurements. Do changes tend to be slow and gradual or fast and abrupt? Mathematically, (in)stability measurements often look at the average change between two consecutive measurements. \textit{Inertia} describes how much a measurement carries over to future measurements. There are two main ways in which experiences tend to be connected to future experiences --- resistance to change and seasonality. Both forms of inertia relate to different types of research questions. ``Do patients stay in a depressed mood for several measurements?'' (i.e., stability-inertia), and ``Do participants drink more alcohol on Fridays?'' (i.e., periodicity-inertia). Both forms of inertia are often measured using autocorrelations with either a lag-1 (stability-inertia) or the lag of the seasonality (e.g., seven days). Periodicity-inertia can additionally be captured using wave patterns (e.g., fourier or wavelet transformation).

The final two features are linear and nonlinear trends. \textit{Linear trends} are a common research interest for longitudinal data. ``Do patient symptoms improve?'' or ``Does worker productivity decline?'', are familiar types of research questions. Mathematically, such linear trends are commonly captured using (piecewise) linear regression coefficients. \textit{Nonlinear trends} are important in two regards. Firstly, nonlinearity is a deviation from linear trends and secondly, the shape and structure of the nonlinear trend. Questions like ``Is the development of anxiety a nonlinear process?'' are mathematically captured using nonlinearity parameters such the bicoherence metrics \citep{cuddy2009}. The structure of nonlinear trends is often mathematically captured using polynomial coefficients or more broadly by how wiggly the line is (e.g., estimated degrees of freedom of GAM spline models; similar to the number of spikes \citealp[]{caro-martin2018}). 

This selection of the proposed six time-series features is in no way exhaustive or imperative. Both using a purely data-driven approach or selecting other aspects to summarize the time series are legitimate options. The list seeks to offer a practical toolbox of features that are common and meaningful to psychological research questions and -practice (see \tblref{tab:esmFeatures} for a more complete overview).

\input{tables/esmPsychFeatures}

\subsection{Feature reduction}
Once a meaningful selection of time series features has been extracted for each variable and participant, the total number of data points usually remains too large for most clustering algorithms. As an example, a relatively common scenario would include 10 variables of interest, where 8 time series features are extracted, resulting in 80 features per participant (with a common sample size of 100 participants that would result in a total of 8,000 data points in this hypothetical example). We offer an illustration of the compounding numbers of data points in \fgrref{fig:TSCFlowN}. The difficulty of working with such a large number of dimensions is sometimes termed the `dimensionality curse' \citep[e.g.,][]{altman2018}. To deal with this dimensionality issue, two main approaches have been proposed --- feature selection and feature projection (a full overview of the approaches, methods, and common algorithms is available in \tblref{tab:featureReduction}). 

Feature selections seek to create a subset of the most important features. The many available approaches differ in how they seek to determine the importance of the individual features. Generally speaking, selection methods can be categorized as `filter', `wrapper', or `hybrid' methods. The filter methods, broadly speaking determine important features by identifying irrelevant features (e.g., because features do not capture much information), and identifying redundant features (e.g., because features capture the same information). Wrapper methods on the other hand avoid the feature-based evaluation and focus on the performance of the later model to identify the most important features. A wrapper, thus, compares the performance of models with different feature combinations. Traditional examples of wrappers are forward selection or backward elimination procedures. Because filter methods might not always perform well and wrapper methods are computationally intensive\footnote{Computationally \textit{k} features could be considered in \(2^k – 1\) possible combinations --- for the example of 80 features that would allow for \(1.20 \times 10^{24}\) (over one septillion) combinations.}, hybrid methods seek to combine the two methods and find a balance between computational effort (i.e., efficiency) and performance (i.e., effectiveness). Methods might, for example, use a filter step to reduce the size of features considered in a later wrapper step.

The second general approach to the dimensionality curse has been feature projection. Projection methods are relatively common in psychological research --- including, factor- and principal component analyses. Generally speaking, projection methods seek to transform the many features in such a way that a much smaller number of new variables can accurately capture the variance and structure of the original features (i.e., the data is projected to a lower dimensional space). Commonly, this is achieved using linear transformations (e.g., principal component analysis), or more complex nonlinear transformations (e.g., t-SNE). Generally speaking, feature selection procedures have the benefit that they retain the interpretable feature labels directly and immediately indicate which features were most informative in the sample. Feature projection methods, on the other hand, tend to be more generalized and are more readily available.

\input{tables/FeatureReduction}

\subsection{Feature clustering}
For feature-based time-series clustering, the main aim is to organize participants into groups so that the features of participants within a group are as similar as possible, while the features of people in different groups are as different as possible. The crux of clustering is, thus, to have clearly defined and effective measurements of (dis)similarity. Most of the clustering algorithms used today use some form of distance measurement to optimize group assignment (or similarity measurement for qualitative features). While others have produced excellent overviews of the many clustering approaches available \citep[e.g.,][]{xu2015}, we will briefly introduce some of the more readily available approaches suitable for most time series feature data. The well-established and readily available clustering approaches can, broadly speaking, be categorized as based on (1) centroids, (2) distributions, (3) density, (4) hierarchies, or (5) a combination thereof (see \tblref{tab:clusterApproaches}). 

Each of these approaches can be a valuable clustering approach for time series feature data and users will usually have to make an informed decision based on the structure of their data as well as an appropriate weighing of accuracy and efficiency. As an example, under ideal conditions, all approaches are likely to suggest very similar clusters. However, when the shapes of clusters, for example, become more complex in a multi-dimensional space, density-based or hierarchy-based approaches that allow for more bottom-up clustering are likely to be more accurate. Yet, with higher numbers of participants and features, both density- and hierarchy-based approaches may perform less well and the more efficient centroid-based methods might be more effective. Similarly, several of the proposed time series features are statics that are generated from Gaussian distributions and a distribution-based algorithm might be ideally suited to separate such distributions. Yet, in cases where mis-specifications are costly, a method with fewer assumptions might be more advisable. 

There is thus, unfortunately, no one-size-fits-all solution to clustering. On the bright side, however, the thriving methodological developments in the clustering literature offer a wide variety of options that offer prospects even for extremely large and complex data sets. We provide a short intuitive explanation for common approaches, together with some of their characteristics and example algorithms in \tblref{tab:clusterApproaches}. For our own illustration, we have chosen that centroid-based k-means clustering. While k-means comes at the expense of high accuracy with more complex cluster shapes, we specifically chose k-means because it is an extremely efficient method that works well with large participant- and feature numbers without making too many restrictive assumptions about the shape of the data. K-means is also well-established within the research community and has been readily implemented in many statistical software packages. Additionally, many of the feature selection methods have specifically been designed for the well-established k-means algorithm. As such, the k-means offers a good starting point for many psychological researchers and the method should be generalizable across a relatively wide variety of projects.

\input{tables/clusterApproaches}

\section{Empirical Illustration}
To illustrate common use cases of feature-based time series clustering with psychological ESM data, we apply the clustering process to a recent set of studies that collected diverse concepts with context-specific measurements. The illustration specifically uses data from research on migration experiences, where researchers have started using ESM data to follow the daily interactions of migrants with the cultural majority groups \citep[e.g.,][]{Keil2020}. This new type of research comes as a response to prominent reviews, which have called for more longitudinal \citep[e.g.,][]{Ward2019} and real-world data \citep[e.g.,][]{McKeown2017}. At the same time, conceptual works have pushed for more diverse assessments of migration experiences, including motivational, affective, cognitive, and behavioral aspects \citep[e.g.,][]{Kreienkamp2022d}. The migration ESM research, thus, offers a good example of the modern ESM data that introduces diverse conceptualizations and more event-specific missingness patterns.

% Methods and Results from RMarkdown render
\input{methods-and-results}
%\input{tables/descrWide}
\input{tables/descrLong}

\section{Discussion}
% aims re-iterated
To be written.

\subsection{Limitations}
To be written.

\subsection{Implications}
To be written.


\subsection{Conclusion}
To be written.




\section{Notes}

\citep[e.g., see][]{Ram2014} [Processes Across Multiple Time-Scales]

"The goal of clustering is to identify structure in an unlabeled data set by objectively organizing data into homogeneous groups where the within-group-object dissimilarity is minimized and the between-group-object dissimilarity is maximized." --- \citep[][p.1857]{liao2005}

\subsection{Migration Parts}
In the example of the migration experiences, researchers might for example be interested in how well-being develops in relations the quality of interactions with the majority group. Because rating of interaction quality are only possible when an interaction actually happened, one cannot impute interaction quality ratings for measurements that did not include an interaction. 



% Tables
% Example
%\input{Tables/descrFullWide}


% Figures

\begin{figure}[hbtp]
  \caption{Scopus ESM Development}
  \label{fig:ScopusEsm}
  \centering\includegraphics[width=\textwidth]{figures/Scopus-ESM-Development.png}
  \caption*{Note: \\
  Search Terms: `( "ecological momentary assessment"  OR  "experience sampling"  OR  "intensive longitudinal")' in Title, Abstract, or Keyword.}
\end{figure}

\begin{figure*}[hbtp]
  \caption{Flowchart Feature-Based Time Series Clustering in Psychology}
  \label{fig:TSCFlow}
  \centering\includegraphics[width=\textwidth]{figures/TS Cluster Flow/TimeSeriesClusterFlowSelection.pdf}
  \caption*{Note: \\
  Choices selected for illustration in this manuscript are marked in bold.}
\end{figure*}

\begin{figure*}[hbtp]
  \caption{Exemplary Flowchart of Data Points in Feature-Based Time Series Clustering}
  \label{fig:TSCFlowN}
  \centering\includegraphics[width=\textwidth]{figures/TS Cluster Flow/tsClustFlowN.pdf}
  \caption*{Note: \\
  The presented number of participants, variables, and measurement occasions are somewhat arbitrary but generally represent common sample sizes found within the literature. Also the number of extracted clusters is presented for illustrative purposes only.}
\end{figure*}

\begin{figure*}[hbtp]
  \caption{Cluster Group Comparisons based on Features and Variables}
  \label{fig:clusterFeatVar}
  \centering\includegraphics[width=\textwidth]{figures/clusterFeatVarComb.pdf}
  \caption*{Note: \\
  same data}
\end{figure*}

\begin{figure*}[hbtp]
  \caption{Cluster Group Comparisons over time}
  \label{fig:clusterTs}
  \centering\includegraphics[width=\textwidth]{figures/clusterTsComb.pdf}
  \caption*{Note: \\
  same data}
\end{figure*}

\printbibliography

\appendix

\section{Placeholder Title}
\label{app:AppendixTitle}
To be written, if necessary.

\end{document}
