\subsection{Data}

To illustrate the

\subsubsection{Participants}

\subsubsection{Timepoints}

\subsubsection{Variables}

\subsection{Procedure}

feature engineering: feature extraction + feature selection

\subsubsection{Feature-Extraction}

use domain knowledge to extract new variables from raw data (summarize)

options: tsfresh \citep[][]{christ2018}, spikes
\citep[][]{caro-martin2018}

Our selection: density, inertia, instability, diversity
\citep[][]{dejonckheere2019, kuppens2017}

\begin{itemize}
  \item 
\end{itemize}

\subsubsection{Feature Selection}

select most consistent, relevant, and non-redundant features.

\textit{k} features means \(2^k â€“ 1\) possible models

maximize relevance and minimize redundancy

``However, most of the existing UFS methods primarily focus on the
significance of features in maintaining the data structure while
ignoring the redundancy among features. Moreover, the determination of
the proper number of features is another challenge.''

\begin{itemize}
  \item feature selection
  \begin{itemize}
    \item filter: evaluate individual features' impact / importance / relevance. Univariate (ranking-based) or multivariate (preferred bc. can handle redundant and irrelevant features)
    \begin{itemize}
      \item unvariate, often based on information or spectral analysis (univariate inclusion test \citep[i.e., alpha-corrected t-tests][]{christ2018})
    \end{itemize}
    \item wrapper: search problem --- compare different models based on evaluation criterion (i.e., model performance; e.g., $R^2$, accuracy, ...). "high computational cost, and they are limited to be used in conjunction with a particular clustering algorithm."
    \begin{itemize}
      \item Exhaustive Feature Selection (i.e., brute force)
      \item forward selection (e.g., greedy Forward Search \citep[][]{wang2006})
      \item backward elimination
      \item Bi-directional elimination (Stepwise Selection)
    \end{itemize}
    \item hybrid: first filter then wrapper
    \item [embedded: feature selection as part of the model construction process. E.g., LASSO Regularization L1, or Random Forest Importance] MOSTLY FOR SUPERVISED / LABELED DATA (e.g., regressions)
  \end{itemize}
  \item dimension reduction
\end{itemize}

random forest search \citep[][]{}
