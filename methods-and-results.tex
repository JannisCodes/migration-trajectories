\subsection{Data}

We used a data set following migration experiences collected by
\citet[][]{Kreienkamp2022b}. The data set consists of three studies that
followed migrants who had recently arrived in the Netherlands in their
daily interactions with the Dutch majority group members. After a
general migration-focused pre-questionnaire, participants were invited
twice per day to report on their (potential) interactions with majority
group members for at least 30 days. The short ESM surveys were sent out
at around lunch (12pm) and dinner time (7pm). After the 30 day study
period participants filled in a post-questionnaire that mirrored the
pre-questionnaire. Participants received either monetary compensation or
partial course credits based on the number of surveys they completed.
For our empirical example we focus on the variables that were collected
during the ESM surveys and were available in all three studies. Full
methodological details are available in the empirical article by
\citet[][]{Kreienkamp2022b}.

\subsubsection{Sample}

The original studies included 207 participants (\(N_{S1}=\) 23,
\(N_{S2}=\) 113, \(N_{S3}=\) 71) with a total of 10,297 measurements.
The studies differed substantially in the maximum length of
participation (\(\text{max}(t_{S1})=\) 63, \(\text{max}(t_{S2})=\) 69,
\(\text{max}(t_{S3})=\) 155). This was likely due to the option to
continue participation without compensation in the later two studies. To
make the three studies comparable in participation and time frames, we
iteratively removed all measurement occasions and participants that had
more than 45\% missingness
\citep[which was in line with the general rcecommendation for data that might still need to rely on imputations for later model testing][]{Madley-Dowd2019}.
This procedure let to a final sample of 157 participants, who jointly
produced 8,132 measurements. Importantly, both the participant
repsonse-patterns and the time frame were now a lot more comparable
(\(\text{max}(t_{S1})=\) 61, \(\text{max}(t_{S2})=\) 60,
\(\text{max}(t_{S3})=\) 67). For a full overview of the data reduction
procedure and study-specific exclusions see Online Supplemental Material
A.

\subsubsection{Variables}

Included 12 variables that were available across all three studies and
captured information about the participant's interactions, as well as
cognitive-, emotional-, and motivational self in relationship with the
majority group. We chose these two aspects in particular because (1) the
interaction-specific information exemplified the structural missingness
issue of modern ESM data and (2) the motivational, emotional, and
cognitive experience offered a diverse conceptualization of migration
experience (beyond behavioral measurements) that is becoming more common
in the literature \citep[][]{Kreienkamp2022d}. Full methodological
details are available in Online Supplemental Material A, but basic item
information, descriptives, and correlations are available in
\tblref{tab:descrWide}.

\subsection{Analysis and Results}

To perform the main analysis on our selected variables, we follow the
three extract--reduce--cluster steps in sequential order. For each of
the steps fully reproducible and annotated code is available in Online
Supplemental A as well as in the accompanying OSF- and GitHub
repositories (REFERENCES).

\subsubsection{Feature extraction}

For the features, we selected and extracted one of each of the features
proposed in \tblref{tab:esmFeatures}. We particularly chose the more
robust median and median absolute deviation (MAD) for central tendency
and variability. Within the variability structure, we chose the slightly
more intuitive mean absolute change (MAC) for stability and stayed with
the common lag--1 autocorrelation for inertia. For the trend summaries,
we chose the overall linear regression slope to describe the linear
trend and we summarized the nonlinear trend with the estimated degrees
of freedom of an empty GAM spline model (edf) --- the edf summarizes the
\textit{wiggliness} of the spline trend line. We chose these features in
particular because we consider them to be the most broadly applicable
features. We also extracted the participant's number of completed ESM
measurements to ensure that the clusters are comparable in that regard.
We include a function that automatically extracts and prepares all
features we extracted as well as a large selection of the time series
feature operationalizations presented in \tblref{tab:esmFeatures} in our
GitHub repository (see the \texttt{featureExtractor} function).

After the feature extraction, we found that about 1.40\% of the
extracted features are missing across participants across the 72
features per participant. This might, for example, happen if
participants do not have two subsequent measurements with outgroup
interactions, so that an autocorrelation with lag-1 cannot be calculated
for the contact-specific variables. The small number of missing values
indicates that the feature-based approach indeed largely avoids the
structural missingness issue. The few missing values can however be an
issue for some feature reduction or feature clustering algorithms. We,
thus, impute the missing feature values with a single predictive mean
matching imputation using the MICE library.

\subsubsection{Feature reduction}

For the feature reduction, we chose the very general
\textit{principal component analysis} (PCA). Some of the more
tailor-made feature selection algorithms can be more accurate in
reducing the feature dimensionality and might retain feature importance
information more directly, depending on the specific data structure.
However, PCAs have the distinct benefit that they are well-established
within the psychometric literature and can broadly be applied to a wide
variety of studies in an automatize manner. As our aim is to present a
general illustration that can also be adopted for more general data
descriptive uses, we present the workflow using a PCA here but we
encourage users to consider more specialized methods as well.

To use the principal component analysis with our extracted time series
features, we first standardize all features across participants to
ensure that all features are weighted equally. We then enter all 72
features into the analysis. The PCA will use linear transformations in
such a way that the first component captures the most possible variance
of the original data (e.g., by finding a vector that maximizes the sum
of squared distances). The following components will then use the same
method to iteratively explain the most of the remaining variance while
also ensuring that the components are linearly uncorrelated. In
practice, this means that the PCA decomposes the 72 features into 72
principal components but now (because of the uncorrelated linear
transformations) the first few principal components will capture a
majority of the variance. We can then decide how much information (i.e.,
variance) we are willing to sacrifice for a reduced dimensionality. A
common rule of thumb is to use the principal components that jointly
explain 70--90\% of the original variance (i.e., cumulative percentage
explained variance). For our illustration we we select the first 27
principal components that explain 80\% of the variance in the original
72 features (reducing the dimensionality by 62.50\%).

We would like to comment on two practical matters when using principal
components --- the amount of dimensionality reduction and the
interpretation of the principal components. As for the expected
dimensionality reduction, given its methodology, PCAs tend to `work
better' with (highly) correlated variables. Thus, with a set of very
homogeneous variables and features users will need less principal
components to explain a large amount of variance, while a more diverse
set of variables and features will tend to require more principal
components to capture the same amount of variance. While our 27
principal components is still a relatively high number of variables this
is not surprising as we chose a diverse conceptualization and a diverse
set of time series features. As for the interpretability, PCA allows
users to extract information on the meaning of the principal components.
In particular, because the principal components are linear combination
of original features, users can extract the relative importance of each
feature for the extracted principal components (i.e., the eigenvectors).
While this can be useful in understanding the variance in the original
data or help with manual feature selection, we use the PCA purely to
reduce the dimensionality for the clustering step. Instead of relying on
the principal components, we use the original features of interest to
interpret the later extracted clusters. We particularly advocate for
such an approach if the original features were chosen for their
psychological meaning in understanding the time series and broader
phenomenon of interest.

\subsubsection{Feature clustering}

maximize relevance and minimize redundancy
