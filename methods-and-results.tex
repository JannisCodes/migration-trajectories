\subsection{Data}

To illustrate the

\subsubsection{Participants}

\subsubsection{Timepoints}

\subsubsection{Variables}

\subsection{Procedure}

feature engineering: feature extraction + feature selection

\subsubsection{Feature-Extraction}

use domain knowledge to extract new variables from raw data (summarize)

options: tsfresh \citep[][]{christ2018}, spikes
\citep[][]{caro-martin2018}

Our selection based on describing time series aspects relevant to human
experiences
\citep[density, inertia, instability, diversity;][]{dejonckheere2019, kuppens2017}

\begin{itemize}
  \item diversity: multiple variables
  \begin{itemize}
    \item field and concept specific: affect, behavior, cognition, desire
  \end{itemize}
  \item (in)stability
  \begin{itemize}
    \item mean of sum squared differences
    \item mean absolute change
  \end{itemize}
  \item central tendency
  \begin{itemize}
    \item mean
    \item median
  \end{itemize}
  \item variance
  \begin{itemize}
    \item variance / standard deviation
    \item median absolute distance
    \item (range / min + max)
    \item (change in SD)
    \item (variation coefficient (SE/mean))
  \end{itemize}
  \item inertia
  \begin{itemize}
    \item autocorrelation (lag-1)
    \item (additional periodicity/seasonality (e.g., day, week))
  \end{itemize}
  \item linear trend
  \begin{itemize}
    \item OLS regression slope
  \end{itemize}
  \item nonlinear trend
  \begin{itemize}
    \item GAM spline coefficient: estimated degrees of freedom (edf; how wiggly the line is)
    \item (number of spikes \citep[e.g.,][]{caro-martin2018})
    \item (Fourier Transform Coefficients (representation in square- and sine waves))
    \item (polynomial coefficients (Langevin model))
    \item (semi-markov model parameters)
  \end{itemize}
\end{itemize}

\subsubsection{Feature Selection}

select most consistent, relevant, and non-redundant features.

\textit{k} features means \(2^k â€“ 1\) possible models

maximize relevance and minimize redundancy

``However, most of the existing UFS methods primarily focus on the
significance of features in maintaining the data structure while
ignoring the redundancy among features. Moreover, the determination of
the proper number of features is another challenge.''

\begin{itemize}
  \item dimension reduction
  \item feature selection
  \begin{itemize}
    \item filter: evaluate individual features' impact / importance / relevance. Univariate (ranking-based) or multivariate (preferred bc. can handle redundant and irrelevant features)
    \begin{itemize}
      \item unvariate, often based on information or spectral analysis (univariate inclusion test \citep[i.e., alpha-corrected t-tests][]{christ2018})
    \end{itemize}
    \item wrapper: search problem --- compare different models based on evaluation criterion (i.e., model performance; e.g., $R^2$, accuracy, ...). "high computational cost, and they are limited to be used in conjunction with a particular clustering algorithm."
    \begin{itemize}
      \item Exhaustive Feature Selection (i.e., brute force)
      \item forward selection (e.g., greedy Forward Search \citep[][]{wang2006})
      \item backward elimination
      \item Bi-directional elimination (Stepwise Selection)
    \end{itemize}
    \item hybrid: (usually) first filter then wrapper (e.g., Bayesian Filter KMeans)
    \item (embedded: feature selection as part of the model construction process. E.g., LASSO Regularization L1, or Random Forest Importance) MOSTLY FOR SUPERVISED / LABELED DATA (e.g., regressions)
  \end{itemize}
\end{itemize}

random forest search \citep[][]{}

\subsection{Clustering}

\begin{itemize}
  \item centroid-based (e.g., k-means)
  \begin{itemize}
    \item non-probabilistic
    \item no assumptions
    \item fast
    \item sensitive to local minima
  \end{itemize}
  \item distribution-based (e.g., gaussian mixture model [GMM])
  \begin{itemize}
    \item probabilistic
    \item (gaussian) distributions assumed
    \item more flexible shape than vanilla centroid-based
  \end{itemize}
  \item density-based (e.g., dbscan)
  \begin{itemize}
    \item no shape assumption
    \item do not assign outliers (i.e., outlier detection)
  \end{itemize}
  \item Hierarchical-based (e.g., Ward hierarchical clustering)
  \begin{itemize}
    \item divisive (top-down) and agglomerative (bottom-up)
    \item no pre-specified number of clusters
    \item easy visual inspection (dendogram)
    \item best for small number of cases
    \item no reversal of assignments
  \end{itemize}
  \item Hybrid (e.g, GMM with kmeans centroids as starting point)
  \begin{itemize}
    \item helps avoid individual shortcomings (e.g., local minima avoidance)
  \end{itemize}
\end{itemize}
