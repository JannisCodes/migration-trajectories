In our own illustration example, we used the \texttt{cluster.stats()}
function from the \texttt{fpc} \textsf{R} package, which calculates a
wide variety of internal cluster validity statistics for each of the
extracted clustering solutions. With real-world data, no single
evaluation measure is likely to be perfect. Different measures may yield
varying results based on the data characteristics and the research
question at hand \citep{kittler1998}. It is therefore important to
consider a variety of evaluation measures and to carefully interpret the
results in the context of the specific analysis \citep{vinh2009}. We
found that across most indices, the analysis with \(k=2\) clusters
performed the best. Three commonly reported indices we would like to
highlight are the comparison of within clusters sum of squares, the
average silhouette score, and the Calinski-Harabasz index. The first
statistic we looked at was the total within-cluster sum of square
\(WCSS\) (see also \equatref{eq:kWCSS}). While the within-cluster
variation will naturally decrease with (more) smaller clusters, we
observed that the decrease in \(WCSS\) was largest until \(k=2\), after
which the decrease was much smaller. This method is also sometimes
referred to as the `elbow method' \citep{syakur2018}. We then looked at
a second, commonly used measure, the average silhouette score. This
statistic measures the degree to which each time feature data point is
similar to other points within the same cluster, compared to points in
other clusters \citep{rousseeuw1987}. In our case, the \(k=2\) solution
maximized the silhouette coefficient (\(s_2=\) 0.09). Finally, the
Calinski-Harabasz index assesses the compactness and separation of the
clusters by assessing the ratio of the sum of between-clusters
dispersion and of inter-cluster dispersion for all clusters --- thus,
the higher the score the better the performances \citep{calinski1974}.
In our case, the \(k=2\) solution also showed the highest
Calinski-Harabasz index (\(CH_2=\) 16.38; a full table of all extracted
validity statistics is available in Supplemental Material
A)\footnote{It is important to note that another commonly assessed aspect of the evaluation is determining the stability and robustness of the clusters \citep{berkhin2006}. This can be assessed by evaluating the sensitivity of the clusters to different feature sets or clustering algorithms, or by using techniques such as bootstrapping to assess the uncertainty of the clusters \citep{vinh2009}. Especially when comparing different clustering algorithms one common index is the Bayesian information criterion (BIC), where a lower BIC indicates that a model is more representative of the data \citep{vandeschoot2017}.}.
In the final \(k=2\) solution the k-means analysis also assigned a
relatively even number of participants to cluster 1 (\(n_{C_1}=\) 76)
and cluster 2 (\(n_{C_1}=\) 80).

To ensure that the clustering is necessary in the first place, we also
compare the performance to a single cluster solution (i.e., a single
centroid). The comparison to this \(k=1\) solution is slightly different
because metrics like the between-cluster separation are not available.
Nonetheless, comparing the within-cluster sums of squares (SS) and the
explained variance, we find that two clusters indeed outperform a single
cluster solution. Specifically, the total within-cluster SS decreased
from 8940.21 for one cluster to 8080.67 for two clusters. Additionally,
the variance explained increased from \textless.001 to 0.096 when the
cluster count was increased to two
\citep[e.g., ][; also see \situtorial\ for full results]{beijers2022}.
