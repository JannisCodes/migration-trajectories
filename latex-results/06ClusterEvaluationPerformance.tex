In our own illustration example, we used the \texttt{cluster.stats()}
function from the \texttt{fpc} \textsf{R} package, which calculates a
wide variety of internal cluster validity statistics for each of the
extracted clustering solutions. With real-world data no single
evaluation measure is likely perfect, and different measures may produce
different results depending on the characteristics of the data and the
research question being addressed \citep{kittler1998}. It is therefore
important to consider a variety of evaluation measures and to carefully
interpret the results in the context of the specific analysis
\citep{vinh2009}. We found that across most indices, the analysis with
\(k=2\) clusters performed the best. Three commonly reported indices we
would like to highlight are the comparison of within clusters sum of
squares, the average silhouette score, and the Calinski-Harabasz index.
The first statistic we looked at was the total within-cluster sum of
square \(WCSS\) (see also \equatref{eq:kWCSS}). While the within-cluster
variation will naturally decrease with (more) smaller clusters, we
observed that the decrease in \(WCSS\) was largest until \(k=2\) after
which the decrease was much smaller. This method is also sometimes
referred to as the `elbow method' \citep{syakur2018}. We then looked at
a second, commonly used measure, the average silhouette score. This
statistic measures the degree to which each time feature data point is
similar to other points within the same cluster, compared to points in
other clusters \citep{rousseeuw1987}. In our case, the \(k=2\) solution
maximized the silhouette coefficient (\(s_2=\) 0.09). Finally, the
Calinski-Harabasz index assesses the compactness and separation of the
clusters by assessing the ratio of the sum of between-clusters
dispersion and of inter-cluster dispersion for all clusters --- thus,
the higher the score the better the performances \citep{calinski1974}.
In our case, the \(k=2\) solution also showed the highest
Calinski-Harabasz index (\(CH_2=\) 16.38; a full table of all extracted
validity statistics is available in Supplemental Material
A)\footnote{It is important to note that another commonly assessed aspect of the evaluation is determining the stability and robustness of the clusters \citep{berkhin2006}. This can be assessed by evaluating the sensitivity of the clusters to different feature sets or clustering algorithms, or by using techniques such as bootstrapping to assess the uncertainty of the clusters \citep{vinh2009}. Especially when comparing different clustering algorithms one common index is the Bayesian information criterion (BIC), where a lower BIC indicates that a model is more representative of the data \citep{vandeschoot2017}.}.
In the final \(k=2\) solution the k-means analysis also assigned a
relatively even number of participants to cluster 1 (\(n_{C_1}=\) 76)
and cluster 2 (\(n_{C_1}=\) 80).
