During the k-means clustering itself, the analysis seeks to minimize the
total within-cluster variation. The analysis is designed to optimize the
clustering of the feature data into \(k\) groups, where \(k\) is a
pre-defined number of clusters. We used the Hartigan and Wong algorithm,
which is a widely used algorithm in k-means clustering
\citep{hartigan1979}. The algorithm starts by randomly separating the
data points into k clusters and then iteratively updates the assignment
of each point to the nearest cluster center until convergence. To do so,
the Hartigan and Wong algorithm specifically calculates the
within-cluster variation (\(W\)) of cluster \(C_i\) as the summed
squared Euclidean distances of the feature \(x\) to the closest cluster
centroid \(\mu_i\):

\begin{equation} \label{eq:kWCi}
  W(C_i) = \sum_{x \in C_i}(x-\mu_i)^2
\end{equation}

By summing the within-cluster sum of squares from all \(k\) clusters, we
can then derive the total within-cluster sum of square \(WCSS\):

\begin{equation} \label{eq:kWCSS}
  WCSS = \sum_{i=1}^k W(C_i) = \sum_{i=1}^k \sum_{x \in C_i} (x - \mu_i)^2
\end{equation}

It is this \(WCSS\) that becomes the objective function to be minimized,
by iteratively moving features from one cluster to another
\citep{hartigan1979}. In particular, the algorithm (1) calculates the
cluster centroids of the initial partitioning, (2) checks whether any
feature has a centroid that is closer than that of the currently
assigned cluster (3) updates the centroids based on any reassigned
features, and then iterates between steps two and three until \(WCSS\)
is minimized (i.e., locally optimal convergence) or a maximum number of
iterations is reached. Given the iterative nature of the algorithm, the
initial partitioning is often important because the algorithm might
arrive at a suboptimal clustering where the \(WCSS\) cannot be further
reduced by moving any feature to another cluster, despite a better
solution existing (i.e., a local minimum). It is, therefore often
recommended to run the k-means clustering with several different
starting positions.

In our case we used the Elbow and the Silhouette method to determine the
recommended number of \(k\) clusters, which for both methods was two
clusters (see Supplemental Material A). We then entered the
participants' PC-scores from the feature reduction step into the k-means
algorithm. To avoid local minima we used 100 random initial centroid
positions. In the final solution the k-means analysis assigned 80
participants to cluster 1 and 76 participants to cluster 2.
