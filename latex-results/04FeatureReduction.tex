For our own illustration data, we chose a feature projection method to
reduce the dimensionality of our extracted features. We particularly
chose the feature projection method for its broad applicability. We,
specifically, selected the commonly used
\textit{principal component analysis} (PCA). Some of the more
tailor-made feature selection algorithms can be more accurate in
reducing the feature dimensionality and might retain feature importance
information more directly, depending on the specific data structure.
However, PCAs have the distinct benefit that they are well-established
within the psychometric literature \citep{jolliffe2011} and can broadly
be applied to a wide variety of studies in an automatized manner
\citep{abdi2010}. As our aim is to present a general illustration that
can also be adopted across use cases, we present the workflow using a
PCA here but we encourage users to consider more specialized methods as
well.

To use the PCA with our extracted time series features, we first
standardize all features across participants to ensure that all features
are weighted equally. We then enter all 72 features into the analysis.
The PCA uses linear transformations in such a way that the first
component captures the most possible variance of the original data
\citep[e.g., by finding a vector that maximizes the sum of squared distances][]{jolliffe2002, abdi2010}.
The following components will then use the same method to iteratively
explain the most of the remaining variance while also ensuring that the
components are linearly uncorrelated \citep{shlens2014}. In practice,
this meant that the PCA decomposed the 72 features into 72 principal
components but now (because of the uncorrelated linear transformations)
the first few principal components will capture a majority of the
variance. We can then decide how much information (i.e., variance) we
are willing to sacrifice for a reduced dimensionality. A common rule of
thumb is to use the principal components that jointly explain 70--90\%
of the original variance
\citep[i.e., cumulative percentage explained variance; e.g.,][]{jackson2003}.
For our illustration, we select the first 27 principal components that
explain 80\% of the variance in the original 72 features (reducing the
dimensionality by 62.50\%). For the extracted principal components we
save the 27 principal component scores for each participant (i.e., the
participants' coordinates in the reduced dimensional space; PC-scores).

We would like to comment on two practical matters when using principal
components --- the amount of dimensionality reduction and the
interpretation of the principal components. As for the expected
dimensionality reduction, given its methodology, PCAs tend to `work
better' at reducing dimensions with (highly) correlated variables
\citep[e.g.,][]{jolliffe2002}. Thus, with a set of very homogeneous
variables and features users will need fewer principal components to
explain a large amount of variance, while a more diverse set of
variables and features will tend to require more principal components to
capture the same amount of variance \citep[e.g.,][]{abdi2010}. Our 27
principal components are still a relatively high number of variables but
this is not surprising as we chose a diverse conceptualization and a
diverse set of time series features. As for interpretability, PCA allows
users to extract information on the meaning of the principal components.
In particular, because the principal components are linear combinations
of the original features, users can extract the relative importance of
each feature for the extracted principal components (i.e., the
eigenvectors). While this can be useful in understanding the variance in
the original data or help with manual feature selection, we use the PCA
purely to reduce the dimensionality for the clustering step. Instead of
relying on the principal components, we use the original features of
interest to interpret the later extracted clusters. We particularly
advocate for such an approach if all original features are considered
meaningful in understanding the time series and users would like to
retain the features for interpretation (irrespective of the features'
importance).
