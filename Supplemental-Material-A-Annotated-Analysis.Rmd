---
title: "Supplemental Information A: Full Annotated Analysis and Reproducible Code"
subtitle: "Supplemental Material for 'Migration Trajectories [working title]'"
author:
- Jannis Kreienkamp^1,3^
- Kai Epstude^1,3^
- Laura F. Bringmann^1,3^
- Maximilian Agostini^1,3^
- Peter de Jonge^1,3^
- Rei Monden^2,3^
- ^1^University of Groningen, Department of Psychology
- ^2^Hiroshima University, Graduate School of Advanced Science and Engineering
- ^3^Author order TBD [currently in first name alphabetical order]
- "Author Information:"
- "Correspondence concerning this article should be addressed to Jannis Kreienkamp, Department of Psychology, University of Groningen, Grote Kruisstraat 2/1, 9712 TS Groningen (The Netherlands).  E-mail: j.kreienkamp@rug.nl"
- 'The main manuscript is available at <a href="https://www.doi.org/ToBePublished" target="_blank">doi.org/ToBePublished</a>'
- 'The data repository for this manuscript is available at <a href="https://osf.io/TBA" target="_blank">osf.io/TBA</a>'
- 'The GitHub repository for this manuscript is available at <a href="https://github.com/maskedForPeerReview" target="_blank">github.com/maskedForPeerReview</a>'
date: "Last updated: `r format(Sys.time(), '%d %B, %Y')`"
output: 
  bookdown::html_document2:
    fig_caption: yes
    md_extensions: +footnotes
    code_folding: hide
    mathjax: default
    theme: yeti
    toc: yes
    toc_float: yes
    number_sections: false
    css: style.css
    includes:
      in_header: "_includes/head-custom-rmd.html" 
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: apa.csl
header-includes:
   - \usepackage{amsmath, nccmath}
---

```{=html}
<style type="text/css">
.main-container {
  max-width: 1300px;
  margin-left: auto;
  margin-right: auto;
}
.table {
  margin-left:auto; 
  margin-right:auto;
}
</style>
```
```{r setup, include=FALSE}
# R Studio Clean-Up
cat("\014") # clear console
# rm(list=ls()) # clear workspace - use restart R instead [cmd/alt + shift + F10]
gc() # garbage collector

# Install and Load Packages
# !IMPORTANT!
# BEFORE FIRST RENDER:
# To install all relevant packages please run "renv::restore()" (or renv::init() and then initiate from lockfile) in the console before the first use to ensure that all packages are using the correct version.
# to store the packages in a contained library within the project folder: renv::settings$use.cache(FALSE) and add 'RENV_CONFIG_SANDBOX_ENABLED = FALSE' to an '.Renviron' file
lib <- c(
  "rmarkdown",
  "knitr",
  "remedy",
  "bookdown",
  "brms",
  "psych",
  "ggplot2",
  "ggthemes",
  "haven",
  "RColorBrewer",
  "plotly",
  "gridExtra",
  "ggpattern",
  "lme4",
  "nlme",
  "jtools",
  "gtsummary",
  "sessioninfo",
  "tibble",
  "pander",
  "devtools",
  "mada",
  "data.table",
  "plyr",
  "dplyr",
  "tidyr",
  "Hmisc",
  "kableExtra",
  "papaja",
  "stringr",
  "stringi",
  "reshape2",
  "lubridate",
  "purrr",
  "metafor",
  "dygraphs",
  "readxl", 
  "reshape",
  "factoextra",
  "Amelia", 
  "ThreeWay"
)
invisible(lapply(lib, library, character.only = TRUE))
rm(lib)

# Load Custom Packages
source("./scripts/functions/fun.panel.R")
source("./scripts/functions/themes.R")
source("./scripts/functions/binaryCor.R")
source("./scripts/functions/MlCorMat.R")
source("./scripts/functions/MlTbl.R")
source("./scripts/functions/metaLmer.R")
source("./scripts/functions/meanViz.R")

# Markdown Options
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # set working directory
knitr::opts_knit$get("root.dir") # check working directory
options(
  scipen = 999,
  digits = 4,
  width = 400
) # removes scientific quotation
# knitr::opts_chunk$set(echo = TRUE, cache = F, cache.path = rprojroot::find_rstudio_root_file('cache/')) # cache settings
knitr::knit_hooks$set(
  error = function(x, options) {
    paste('\n\n<div class="alert alert-danger">',
      gsub("##", "\n", gsub("^##\ Error", "**Error**", x)),
      "</div>",
      sep = "\n"
    )
  },
  warning = function(x, options) {
    paste('\n\n<div class="alert alert-warning">',
      gsub("##", "\n", gsub("^##\ Warning:", "**Warning**", x)),
      "</div>",
      sep = "\n"
    )
  },
  message = function(x, options) {
    paste('\n\n<div class="alert alert-info">',
      gsub("##", "\n", x),
      "</div>",
      sep = "\n"
    )
  }
)
htmltools::tagList(rmarkdown::html_dependency_font_awesome())

# Global Chunk Options
knitr::opts_chunk$set(
  fig.width = 12,
  fig.height = 8,
  fig.path = "Figures/",
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

</br>

------------------------------------------------------------------------

</br>

<i class="fas fa-exclamation-circle"></i> Note. Boxplots display the interquartile range (IQR, center box), and the whiskers extend 1.5\*IQR from the lower and upper hinge. The white point indicates the mean and the white center line indicates the median.

</br>

------------------------------------------------------------------------

</br>

# **Data Preparation**

In an initial preparatory step, we import the data into the R project environment and prepare the variables for further processing and later analyses.

## Data Import

The data were collected using two different survey tools. For the study with sojourners (Study 1: worker) we used the survey platform Qualtrics XM, whereas the studies with international students (Study 2: student), and the international medical professionals (Study 3: medical) were conducted using the survey framework FormR. This means that the datasets had inconsistent file formats and naming conventions. For the Qualtrics study we pre-processed some variables to ease the import process (for the syntax files see the SPS files in '*data/S1_Workers/processed/cleaned*' and for the raw data files see '*data/S1_Workers/raw*'). For the two other studies, we import the raw csv files from their respective folders.

```{r formrImport}
# Load variable name lookup table
varNames <- readxl::read_excel("preregistration/varNames.xlsx")

# rename function
reNam <- function(data = NA, names = varNames, study = "S1", survey = "pre") {
  # for testing
  # data = dtS1$raw.pre
  # names = varNames
  # study = "S1"
  # survey = "pre"
  
  var <- paste0("survey", study)
  varNamOld <- paste0("varNam", study)
  
  nameTbl <- names %>%
    filter((!!sym(var)) == survey) %>%
    select(varNam, varNamOld = (!!sym(varNamOld)))
  
  data.table::setnames(data, as.character(nameTbl$varNamOld), as.character(nameTbl$varNam), skip_absent=TRUE)
  
  return(data)
}

# workers
# initial data cleaning was done in SPSS (syntax files are available in "")
dtS1 <- list(
  raw.pre = read_spss("data/S1_Workers/processed/cleaned/MT - Pre-Measure - 06-15-2018.sav") %>% 
    reNam(data = ., names = varNames, study = "S1", survey = "pre"),
  raw.post = read_spss("data/S1_Workers/processed/cleaned/MT - Post-Measure - 06-15-2018.sav") %>% 
    reNam(data = ., names = varNames, study = "S1", survey = "post"),
  raw.morning = read_spss("data/S1_Workers/processed/cleaned/MT - Morning - 06-15-2018.sav") %>% 
    reNam(data = ., names = varNames, study = "S1", survey = "daily"),
  raw.afternoon = read_spss("data/S1_Workers/processed/cleaned/MT - Afternoon - 06-15-2018.sav") %>% 
    reNam(data = ., names = varNames, study = "S1", survey = "daily")
)

# students
dtS2 <- list(
  raw.pre = read.csv(file = "data/S2_Students/raw/AOTS_Pre.csv", header = T, sep = ",") %>% 
    reNam(data = ., names = varNames, study = "S2", survey = "pre"),
  raw.post = read.csv(file = "data/S2_Students/raw/AOTS_Post.csv", header = T, sep = ",") %>% 
    reNam(data = ., names = varNames, study = "S2", survey = "post"),
  raw.daily = read.csv(file = "data/S2_Students/raw/AOTS_Daily.csv", header = T, sep = ",") %>% 
    reNam(data = ., names = varNames, study = "S2", survey = "daily")
)

# young medical professionals
dtS3 <- list(
  raw.eligibility = read.csv("data/S3_Medical/raw/AOTM_Eligibility.csv") %>% 
    reNam(data = ., names = varNames, study = "S3", survey = "pre_entry"), # works but should be 'survey = "pre_elig"'
  raw.pre = read.csv("data/S3_Medical/raw/AOTM_Pre.csv") %>% 
    reNam(data = ., names = varNames, study = "S3", survey = "pre_entry"),
  raw.post = read.csv("data/S3_Medical/raw/AOTM_Post.csv") %>% 
    reNam(data = ., names = varNames, study = "S3", survey = "post"),
  raw.daily = read.csv("data/S3_Medical/raw/AOTM_Daily.csv") %>% 
    reNam(data = ., names = varNames, study = "S3", survey = "daily")
)

# # FROM INITIAL VAR NAME EXPORT [LEGACY]
# # export var names
# S1Nam <- rbind(
#   data.frame(
#     study = "S1",
#     survey = "pre",
#     varNam = names(dtS1$raw.pre)
#   ),
#   data.frame(
#     study = "S1",
#     survey = "daily",
#     varNam = names(dtS1$raw.afternoon)
#   ),
#   data.frame(
#     study = "S1",
#     survey = "post",
#     varNam = names(dtS1$raw.post)
#   )
# )
# S2Nam <- rbind(
#   data.frame(
#     study = "S2",
#     survey = "pre",
#     varNam = names(dtS2$raw.pre)
#   ),
#   data.frame(
#     study = "S2",
#     survey = "daily",
#     varNam = names(dtS2$raw.daily)
#   ),
#   data.frame(
#     study = "S2",
#     survey = "post",
#     varNam = names(dtS2$raw.post)
#   )
# )
# S3Nam <- rbind(
#   data.frame(
#     study = "S3",
#     survey = "pre_elig",
#     varNam = names(dtS3$raw.eligibility)
#   ),
#   data.frame(
#     study = "S3",
#     survey = "pre_entry",
#     varNam = names(dtS3$raw.pre)
#   ),
#   data.frame(
#     study = "S3",
#     survey = "daily",
#     varNam = names(dtS3$raw.daily)
#   ),
#   data.frame(
#     study = "S3",
#     survey = "post",
#     varNam = names(dtS3$raw.post)
#   )
# )
# write.csv(
#   x = rbind(
#     S1Nam,
#     S2Nam,
#     S3Nam
#   ),
#   file = "preregistration/varNames.csv"
# )
```

## Data Cleaning & Data Exclusions

### Study 1

For the sojourner sample data was collected in four separate surveys: (1) the pre-measurement, (2) the daily morning survey, (3) the daily afternoon survey, as well as (4) a post-measurement. We combine the four individual surveys into one cohesive dataframe and drop superfluous variables that are not relevant to the analyses relevant here. We then format the time and date variables and add person- and measurement indices (for easy and meaningful addressing of the data). We also exclude our own test data.\
<i class="fas fa-info-circle"></i> *Note:* All data preparation steps are saved in the '*dtS1*' list.

```{r cleanWorker}
# Create reduced data sets for morning and afternoon
dat.mo <- dtS1$raw.morning %>%
  select(-starts_with("t_"))
#setdiff(names(dtS1$raw.morning), names(dat.mo))
dat.mo$daytime <- "morning"

dat.af <- dtS1$raw.afternoon %>%
  select(-starts_with("t_"))
dat.af$daytime <- "afternoon"

# merge morning and afternoon measurements with indicator [+ clean up]
daily.dat <- plyr::rbind.fill(dat.mo, dat.af)
daily.dat <- daily.dat[daily.dat$last_outside_referrer != 55951, ]
dtS1$daily <- daily.dat
rm(dat.mo, dat.af, daily.dat)

# reduced data set for pre measurement
dat.pre.red <- dtS1$raw.pre %>%
  select(-starts_with("t_"))
names(dat.pre.red) <- paste(names(dat.pre.red), "pre", sep = ".")

# merge with daily data [+ clean up]
df.pre <- merge(
  x = dtS1$daily,
  y = dat.pre.red,
  by.x = "last_outside_referrer",
  by.y = "platformId.pre",
  all = T
)

# adjust duplicate names to fit to indicate daily or pre measurement
names(df.pre) <- gsub("[[:punct:]]x", ".daily", names(df.pre))
names(df.pre) <- gsub("[[:punct:]]y", ".pre", names(df.pre))

# reduced data set for post-measurement
dat.post.red <- dtS1$raw.post %>%
  select(-starts_with("t_"))
names(dat.post.red) <- paste(names(dat.post.red), "post", sep = ".")

# merge post measurement with pre- and daily data
df <- merge(
  x = df.pre,
  y = dat.post.red,
  by.x = "last_outside_referrer",
  by.y = "last_outside_referrer.post",
  all = T
)

# adjust duplicate names to indicate pre or post
names(df) <- gsub("[[:punct:]]x", ".pre", names(df))
names(df) <- gsub("[[:punct:]]y", ".post", names(df))

# add to list
dtS1$combined <- df
rm(df, df.pre, dat.post.red, dat.pre.red)

# create data frame with cleaned data
df <- dtS1$combined %>%
  filter(
    Finished.pre == 1,
    Finished == 1,
    !is.na(last_outside_referrer)
  )

# add running number as measurement ID within participants
df$measureID <- data.table::rowidv(df, cols = c("last_outside_referrer"))

dtS1$clean <- df

# clean up
rm(df)

# Export reduced Data
# write.csv(dtS1$clean, "data/processed/MT_clean-merged_07-05-2018.csv", row.names = F)
# save(dtS1$clean, file = "data/processed/MT_clean-merged_07-05-2018.RData")
```

### Study 2

For the student sample data was, similarly, collected in three separate surveys: (1) the pre-measurement, (2) the daily survey sent out at lunch and dinner time, and (3) a post-measurement. We combine the three individual surveys into one large dataframe and drop superfluous variables that are not relevant to the analyses relevant here. We exclude our own test data as well as one participant who entered the study twice (but gave different responses during the pre-measurement). We also reformat missing values and format core ID variables.\
<i class="fas fa-info-circle"></i> *Note:* All data preparation steps are saved in the '*dtS2*' list.

```{r cleanStudents}
# our own test IDs
ownIDs <- c(
  "beautifulLionfishXXXR5rcgVBzGu8hPvOqrK8UBJBw4owvi9nfRFSFu3lMzYhE",
  "niceDogoXXXmB8JI5SFu78SF3DVof84mGUPPNUr14p2HYFTtp31a6D1OwAzM6F-K",
  "amusedQuailXXXmhuc_fpTp8vPkMwDH1BzjaH1d1kHSO1bsPEfsnaEYk4WeVBfPi",
  "juwGAbtXX0_1kmZtSVqKh3PGaHOICqUyU4iBkrT3nDsI_uifuD1gzKcZerxaM5FL"
)

# Prepare dfs for Cleaning
df.pre <- dtS2$raw.pre %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>%
  filter(!is.na(ended)) %>% # remove all who did not finish
  filter(!email %in% .$email[duplicated(.$email)]) %>% # remove all who did the pre questionnaire multiple times (b/c inconsistent ratings scales)
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)
names(df.pre) <- paste(names(df.pre), "pre", sep = ".")

df.post <- dtS2$raw.post %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>%
  filter(!is.na(ResponseId)) %>% # remove own test runs
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  filter(ResponseId %in% df.pre$ResponseId) %>% # remove anyone who wasn't in the pre
  filter(!is.na(ended)) %>% # remove all who never finished
  filter(!ResponseId %in% .$ResponseId[duplicated(.$ResponseId)]) %>% # remove all duplicate ResponseIds
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)
names(df.post) <- paste(names(df.post), "post", sep = ".")

df.daily <- dtS2$raw.daily %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>%
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  filter(ResponseId %in% df.pre$ResponseId) %>% # remove anyone who wasn't in the pre
  filter(!is.na(ended)) %>% # remove all who never finished
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)

# merge daily with pre
dfPreDaily <- merge(
  x = df.daily,
  y = df.pre,
  by.x = "ResponseId",
  by.y = "ResponseId.pre", 
  suffixes = c(".daily", ".pre"),
  all = FALSE
)

# merge daily with post
dfCombined <- merge(
  x = dfPreDaily,
  y = df.post,
  by.x = "ResponseId",
  by.y = "ResponseId.post", 
  suffixes = c(".pre", ".post"),
  all = FALSE
)

# add to list
dtS2$clean <- dfCombined

# clean up workspace
rm(df.pre, df.daily, df.post, dfPreDaily, dfCombined, ownIDs)
```

### Study 3

For the medical professionals sample data was, again, collected in three separate surveys: (1) the pre-measurement, (2) the daily survey sent out at lunch and dinner time, and (3) a post-measurement. We combine the three individual surveys into one large dataframe. We exclude our own test data. We also reformat missing values and format core ID variables.\
<i class="fas fa-info-circle"></i> *Note:* All data preparation steps are saved in the '*dtS3*' list.

```{r cleanMedical}
# our own test IDs
ownIDs <- c(
  "test_LeonieXXXSklxecPLW0-FBPM4796o3pUwUhAY5jb9KGw8jQsKxWmGpa1Jiy", 
  "test_MaxXXXtOp_5dTNefIq0yKXtXt2IN6eEKxeHoPY9mlyvdsqPpLp1B0NGg4UL",
  "test_JannisXXXBsNqk62fOpX6chbd2tMWPptUdjjnhAqnQ3uBqckZ7gLIEoPlfZ",
  "quaintLeopardCatXXXAJ9cfSj-_SZLwNwMDxv_xv_iyr1Bg5YFLTlYdrjW0UXZY",
  "blue-eyedIndianElephantXXXLf5zPMpQCDGS3umFzIj-YVky7ivTItvvozW49m"
)

# Prepare dfs for Cleaning
df.pre <- dtS3$raw.pre %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>%
  filter(!is.na(ended)) %>% # remove all who did not finish
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)
names(df.pre) <- paste(names(df.pre), "pre", sep = ".")

df.post <- dtS3$raw.post %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>% 
  filter(!is.na(ResponseId)) %>% # remove own test runs
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  filter(ResponseId %in% df.pre$ResponseId) %>% # remove anyone who wasn't in the pre
  #filter(!is.na(ended)) %>% # remove all who never finished [disabled because only relevant if data is missing]
  filter(!ResponseId %in% .$ResponseId[duplicated(.$ResponseId)]) %>% # remove all duplicate ResponseIds
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)
names(df.post) <- paste(names(df.post), "post", sep = ".")

df.daily <- dtS3$raw.daily %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>%
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  filter(ResponseId %in% df.pre$ResponseId) %>% # remove anyone who wasn't in the pre
  #filter(!is.na(ended)) %>% # remove all who never finished [disabled because only relevant if data is missing]
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)

# merge daily with pre
dfPreDaily <- merge(
  x = df.daily,
  y = df.pre,
  by.x = "ResponseId",
  by.y = "ResponseId.pre", 
  suffixes = c(".daily", ".pre"),
  all = F
)

# merge daily with post
dfCombined <- merge(
  x = dfPreDaily,
  y = df.post,
  by.x = "ResponseId",
  by.y = "ResponseId.post", 
  suffixes = c(".pre", ".post"),
  all = F
)

# add to list
dtS3$clean <- dfCombined

# clean up workspace
rm(df.pre, df.daily, df.post, dfPreDaily, dfCombined, ownIDs)
```

## Calculate needed transformations

### Study 1

For the worker sample, the data transformation stage had three main aims:

1.  We first corrected time indicators within the surveys. In some cases participants completed their daily diary surveys for the afternoon after midnight. In these cases the measurement still is in reference to the previous day and is indicated in the corrected variable.\
2.  We then created indices of scales. Some indices were multi-item scales while some indices combine equivalent measurement for different situational circumstances (e.g., competence perceptions after interactions and at measurement occasions without interactions).\
3.  Finally, we calculated several basic participant summaries (averages across all measurement occasions).

```{r newVarsWorkers}
df <- dtS1$clean

# Correct date_period ()
df <- df %>%
  mutate(
    startDate = as.Date(created),
    startTime = format(as.POSIXct(created), format = "%H:%M:%S")
  )
  
# order time
# df$TID <- factor(df$date_period, levels = unique(dtS3$raw.daily$date_period))
# df$TIDnum <- as.numeric(df$TID) # get numeric TID
df <- df %>%
  mutate(
    PID = as.numeric(factor(last_outside_referrer))
  )

# Time and Date Variables
df <- df %>%
  mutate(
    TID = measureID - 1,
    # time ID with t0 = 0 for meaningfull intercept interpretations
    date = substr(created, 1, 10),
    # awkward way of extracting date (best converted to )
    time = substr(created, 12, 19),
    # awkward way of extracting time
    daynum = as.numeric(factor(dateQualtrics)),
    # all days as numeric for ordering
    daycor = ifelse(
      daytime == "morning" &
        period_to_seconds(hms(time)) < period_to_seconds(hms("12:00:00")) |
        daytime == "afternoon" &
          period_to_seconds(hms(time)) < period_to_seconds(hms("19:00:00")),
      daynum - 1,
      daynum
    ),
    daycorTest = ifelse(
      daytime == "morning" &
        period_to_seconds(hms(time)) < period_to_seconds(hms("12:00:00")) |
        daytime == "afternoon" &
          period_to_seconds(hms(time)) < period_to_seconds(hms("19:00:00")),
      1,
      0
    ),
    # correctly identify which date the questionnaire is about
    daycor.lead = sprintf("%02d", daycor),
    daytime.lt = ifelse(daytime == "morning", "a", "b"),
    # morning / afternoon to a / b
    day_time = paste(daycor.lead, daytime.lt, sep = "_"),
    # combine day id with morning / afternoon
    ResponseId = as.numeric(factor(day_time)),
    # day and time identifier as numeric id
    SubTime = chron::times(time.0),
    time.daily = as.character(time),
    PPDate = as.Date(df$dateQualtrics),
    number = replace_na(ContactNum, 0),
    NonDutchNum = replace_na(NonDutchNum, 0)
  )

# remove seconds from afternoon time
df$SubTime[df$daytime == "afternoon"] <- paste0(substring(as.character(df$time.0[df$daytime == "afternoon"]), 4, 8), ":00")
df$time.daily[df$daytime == "afternoon" &
  !is.na(df$time.daily != "<NA>")] <- paste0(substring(as.character(df$time.daily[df$daytime == "afternoon" &
  !is.na(df$time.daily != "<NA>")]), 4, 8), ":00")

# Correct morning / afternoon date where survey was collected the day after to indicate the correct date that was targeted
df$PPDate[df$SubTime < "11:50:00" &
  df$daytime == "morning"] <- df$PPDate[df$SubTime < "11:50:00" &
  df$daytime == "morning"] - 1
df$PPDate[df$SubTime < "18:50:00" &
  df$daytime == "afternoon"] <- df$PPDate[df$SubTime < "18:50:00" &
  df$daytime == "afternoon"] - 1

# Make time IDs consistent with later studies
df$TID <- paste(df$PPDate, str_to_title(df$daytime))
df$TIDnum <- as.numeric(factor(df$TID %>% gsub(" Morning", "-A", .) %>% gsub(" Afternoon", "-B", .)))

df <- df %>%
  rowwise() %>%
  mutate(
    InteractionDum = sum(IntergroupContact, IngroupContact, na.rm = FALSE),
    InteractionDum = if_else(InteractionDum > 0, 1, InteractionDum),
    alertness = sum(alertness1, alertness2, na.rm = TRUE),
    calmness = sum(calmness1, calmness2, na.rm = TRUE),
    valence = sum(valence1, valence2, na.rm = TRUE)
  ) %>%
  ungroup()

# Need scales
df$keyMotiveFulfilled <- rowSums(df[, c("KeyNeedFulfillment", "DaytimeNeedFulfillment")], na.rm = T)
df$autonomy.daily.all <- rowSums(df[, c("autonomy_Int", "autonomy_NoInt")], na.rm = T)
df$competence.daily.all <- rowSums(df[, c("competence_Int", "competence_NoInt")], na.rm = T)
# cor(df$relatednessOther, df$relatedness_self_1,use="complete.obs")
df$relatedness.daily.all <- rowMeans(df[, c(
  "relatednessOther",
  "relatednessSelf",
  "relatednessNoInteraction"
)], na.rm = T)

pairs.panels.new(
  df[c("relatednessSelf", "relatednessOther")],
  labels = c(
    "I shared information about myself.",
    "X shared information about themselves."
  )
)
df$relatedness <- rowMeans(df[, c("relatednessOther", "relatednessSelf")], na.rm = T)

df$autonomy <- df$autonomy.daily.all
df$competence <- df$competence.daily.all
df$relatedness <- df$relatedness.daily.all

varNamIndicesS1 <- c(
  "autonomy",
  "competence",
  "relatedness",
  "education_level.pre",
  "associationMerged.pre",
  "assimilation.pre",
  "separation.pre",
  "integration.pre",
  "marginalization.pre",
  "VIA_heritage.pre",
  "VIA_Dutch.pre",
  "SSAS_surrounding.pre",
  "SSAS_privat.pre",
  "SSAS_public.pre",
  "assimilation.post",
  "separation.post",
  "integration.post",
  "marginalization.post",
  "VIA_heritage.post",
  "VIA_Dutch.post",
  "rosenberg.post",
  "social_support.post",
  "stress.post",
  "discrimination.post",
  "discrimination_month.post",
  "NLE_1month.post",
  "NLE_6month.post",
  "NLE_12month.post"
  )

df$roommate.pre_calc <- df %>%
  select(
    starts_with("roommate")
  ) %>% 
  mutate_all(as_factor) %>%
  unite(., "Comb", sep = ", ", remove = TRUE, na.rm = TRUE) %>%
  pull

df$Reason.pre_calc <- df %>%
  select(
    starts_with("Reason"),
    -Reason_nodesire, 
    -ReasonOther.pre 
  ) %>% 
  mutate_all(as_factor) %>% 
  unite(., "Comb", sep = ", ", remove = TRUE, na.rm = TRUE) %>%
  pull

df$occupation.pre_calc <- df %>%
  select(
    starts_with("occupation"),
    -occupation_8_TEXT.pre
  ) %>%
  mutate_all(as_factor) %>% 
  unite(., "Comb", sep = ", ", remove = TRUE, na.rm = TRUE) %>% 
  pull 
  
df$CurrentEducation.pre_calc <-
  df %>%
  select(starts_with("CurrentEducation"),
         -CurrentEducation_6_TEXT.pre) %>%
  mutate_all(as_factor) %>%
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$groupType.pre_calc <-
  df %>%
  select(starts_with("gr_type"),
          -ends_with("TEXT")) %>%
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$groupContext.pre_calc <-
  df %>%
  select(starts_with("gr_context"),
          -ends_with("TEXT")) %>%
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$dyadType.pre_calc <-
  df %>%
  select(starts_with("dyad_type"),
          -ends_with("TEXT")) %>%
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$context.pre_calc <-
  df %>%
  select(starts_with("Context"),
          -ends_with("TEXT")) %>% 
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$desireType.pre_calc <-
  df %>%
  select(starts_with("desire_type"),
          -ends_with("TEXT")) %>% 
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$desireType.pre_calc <-
  df %>%
  select(starts_with("desire_context"),
          -ends_with("TEXT")) %>% 
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$nonDutchType.pre_calc <-
  df %>%
  select(starts_with("NonDutchType"),
          -ends_with("TEXT")) %>% 
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$autonomySat.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT0[1-4].pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric
df$autonomyFrust.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT0[5-8].pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric

df$relatednessSat.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT(09|1[0-2]).pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric
df$relatednessFrust.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT1[3-6].pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric

df$competenceSat.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT(1[7-9]|20).pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric
df$competenceFrust.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT2[1-4].pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric

df$IntGrAnx.pre_calc <-
  psych::scoreItems(
    keys = c(rep(1,6),rep(-1,6)),
    items = df %>% select(matches("^IntGrAnx[0-9]{2}.?\\.pre")),
    min = 1,
    max = 10
  )$scores %>% 
  as.numeric

df$swl.pre_calc <-
  psych::scoreItems(
    keys = rep(1,5),
    items = df %>% select(matches("^SWL[0-9]{2}\\.pre")),
    min = 1,
    max = 7,
    missing = TRUE,
    impute = "none",
  )$scores %>% 
  as.numeric

df$mdmq.alertness.pre_calc <- 
  psych::scoreItems(
    keys = c(1, -1, -1, 1),
    items = df %>% select(MDMQ02.pre, MDMQ05.pre, MDMQ07.pre, MDMQ10.pre),
    min = 1,
    max = 6,
    missing = TRUE,
    impute = "none",
  )$scores %>% 
  as.numeric
df$mdmq.calmness.pre_calc <- 
  psych::scoreItems(
    keys = c(-1, 1, -1, 1),
    items = df %>% select(MDMQ03.pre, MDMQ06.pre, MDMQ09.pre, MDMQ12.pre),
    min = 1,
    max = 6,
    missing = TRUE,
    impute = "none",
  )$scores %>% 
  as.numeric
df$mdmq.valence.pre_calc <- 
  psych::scoreItems(
    keys = c(1, -1, 1, -1),
    items = df %>% select(MDMQ01.pre, MDMQ04.pre, MDMQ08.pre, MDMQ11.pre),
    min = 1,
    max = 6,
    missing = TRUE,
    impute = "none",
  )$scores %>% 
  as.numeric

df$IntGrAnx.post_calc <-
  psych::scoreItems(
    keys = c(rep(1,6),rep(-1,6)),
    items = df %>% select(matches("^IntGrAnx[0-9]{2}.?\\.post")),
    min = 1,
    max = 10
  )$scores %>% 
  as.numeric

df$swl.post_calc <-
  psych::scoreItems(
    keys = rep(1,5),
    items = df %>% select(matches("^SWL[0-9]{2}\\.post")),
    min = 1,
    max = 7,
    missing = TRUE,
    impute = "none",
  )$scores %>% 
  as.numeric

varNamIndicesS1 <-
  append(varNamIndicesS1,
         df %>% select(ends_with("_calc")) %>% names)


#
df <- df %>%
  mutate(
    exWB = scales::rescale(exWB, from = range(exWB, na.rm = TRUE), to = c(-100, 100)),
    relatedness = relatedness + 50,
    autonomy = autonomy + 50,
    competence = competence + 50,
    KeyNeedFulfillment = KeyNeedFulfillment + 50,
    KeyNeedDueToPartner = KeyNeedDueToPartner + 50,
    InteractionContextAccidental = InteractionContextAccidental + 50,
    InteractionContextvoluntary = InteractionContextvoluntary + 50,
    InteractionContextCooperative = InteractionContextCooperative + 50,
    InteractionContextRepresentativeNL = InteractionContextRepresentativeNL + 50,
    qualityOverall = qualityOverall + 50,
    qualityMeaning = qualityMeaning + 50,
    DaytimeNeedFulfillment = DaytimeNeedFulfillment + 50
  )
dtS1$full <- df
rm(df)
```

### Study 2

For the student sample, the data transformation stage had five main aims:

1.  We first create person, survey type, and measurement ID variables.\
2.  We then created indices of scales. Some indices were multi-item scales while some indices combine equivalent measurement for different situational circumstances (e.g., competence perceptions after interactions and at measurement occasions without interactions).\
3.  We add information about the interaction partner to the beep during which a person was selected as an interaction partner.\
4.  We cluster mean-center key variables within participants.\
5.  Finally, we calculated several basic participant summaries (averages across all measurement occasions).

```{r newVarsStudents}
df <- dtS2$clean

# Add ID variables
df$PID <- as.numeric(factor(df$ResponseId)) # participant ID

# Correct date_period ()
df <- df %>%
  mutate(
    startDate = as.Date(created),
    startTime = format(as.POSIXct(created), format = "%H:%M:%S"),
    date_period_old = date_period,
    date_period = ifelse(
      startTime >= c("12:00") & startTime <= c("18:59"),
      paste(format(as.POSIXct(created), format = "%Y-%m-%d"), "Morning"),
      ifelse(
        startTime >= c("19:00") & startTime <= c("23:59"),
        paste(format(as.POSIXct(created), format = "%Y-%m-%d"), "Afternoon"),
        ifelse(
          startTime >= c("00:00") & startTime <= c("11:59"),
          paste(as.Date(created) - 1, "Afternoon"),
          NA
        )
      )
    )
  )
  
# order time
# df$TID <- factor(df$date_period, levels = unique(dtS3$raw.daily$date_period))
# df$TIDnum <- as.numeric(df$TID) # get numeric TID
df <- df %>%
  mutate(
    dateOrder = stri_replace_all_regex(
      date_period,
      pattern = c(' Morning', ' Afternoon'),
      replacement = c('_A', '_B'),
      vectorize = FALSE
    ),
    TID = factor(date_period),
    TIDnum = as.numeric(factor(dateOrder))
  )

# check whether time ordering worked
df <- df %>%
  arrange(PID, TID) 

# Interaction as Factor
df$interaction.f <-
  factor(df$Interaction,
    levels = c("no interaction", "Dutch", "Non-Dutch")
  )
df$intNL <- ifelse(df$Interaction == "Dutch", 1, 0)
df$intNonNL <- ifelse(df$Interaction == "Non-Dutch", 1, 0)

df$IntergroupContact <- (df$IntergroupContact-2)*-1
df$IngroupContact <- (df$IngroupContact-2)*-1

# -------------------------------------------------------------------------------------------------------------
#                                       Combine Variables
# -------------------------------------------------------------------------------------------------------------
# Relatedness
pairs.panels.new(
  df[c("relatednessSelf", "relatednessOther")],
  labels = c(
    "I shared information about myself.",
    "X shared information about themselves."
  )
)
df$relatednessInteraction <- rowMeans(df[c("relatednessSelf", "relatednessOther")], na.rm = TRUE)
df$relatednessInteraction[df$relatednessInteraction == "NaN"] <- NA
# Relatedness Overall (JANNIS NOT SURE THESE ARE CORRECT, CHANGE ROWS?; JK: Changed "NaN" in df$RelatednessInteraction to NA() should work now)
df$relatedness <-
  rowMeans(df[, c("relatednessInteraction", "relatednessNoInteraction")],
           na.rm = TRUE) %>%
  ifelse(is.nan(.), NA, .)

# Core Need
df$DaytimeNeedFulfillment[df$InteractionDum == 0 & !is.na(df$KeyNeedFulfillment)] <-
  df$KeyNeedFulfillment[df$InteractionDum == 0 & !is.na(df$KeyNeedFulfillment)]

df$InteractionNeedFullfillment <- NA 
df$InteractionNeedFullfillment[df$InteractionDum == 1 & !is.na(df$KeyNeedFulfillment)] <- 
  df$KeyNeedFulfillment[df$InteractionDum == 1 & !is.na(df$KeyNeedFulfillment)]


# Randomly selected variables
df <- df %>%
  rowwise() %>%
  mutate(
    ProSo = mean(c(ProSo1, ProSo2, ProSo3, ProSo4), na.rm = TRUE),
    AntiSo = mean(c(AntiSo1, AntiSo2, AntiSo3, AntiSo4, AntiSo5, AntiSo6, AntiSo7), na.rm = TRUE)
  )

# -------------------------------------------------------------------------------------------------------------
#                                 Add Variables related to interaction partner
# -------------------------------------------------------------------------------------------------------------
# create function for later lapply
createIntPartDf <- function(inp) {
  # prepare the dataframe so that we can forloop over it later
  tmp <- data.frame(
    CC = as.character(inp$CC),
    NewCC = as.character(inp$NewCC),
    NewName = as.character(inp$NewName),
    NewCloseness = inp$NewCloseness,
    NewGender = inp$NewGender,
    NewEthnicity = as.character(inp$NewEthnicity),
    NewRelationship = as.character(inp$NewRelationship)
  )

  tmp$CC2 <- recode(tmp$CC, "SOMEONE ELSE" = "NA")
  tmp$CC2 <-
    ifelse(
      tmp$CC == 1 |
        tmp$CC == "SOMEONE ELSE",
      as.character(tmp$NewName),
      as.character(tmp$CC2)
    )
  # maybe add [[:space:]]\b to remove space before word boundary or ^[[:space:]] to remove space in the beginning of a string
  tmp$CC2 <- gsub("^[[:space:]]", "", tmp$CC2)
  tmp$NewName <- gsub("^[[:space:]]", "", tmp$NewName)

  # open the variables that will be filled up in the foor-loop
  tmp$closeness <- rep(NA, nrow(tmp))
  tmp$gender <- rep(NA, nrow(tmp))
  tmp$ethnicity <- rep(NA, nrow(tmp))
  tmp$relationship <- rep(NA, nrow(tmp))

  # Run the for-loop. It finds the variables related to the name of the interaction partner. If there is a repeating interaction
  # partner (i.e. CC2) it takes the value (i.e. NewCloseness) from the first interaction (i.e. NewName)
  for (i in 1:nrow(tmp)) {
    if (is.na(tmp$CC2[i])) {
      next
    } else {
      tmp$closeness[i] <-
        na.omit(tmp$NewCloseness[as.character(tmp$CC2[i]) == as.character(tmp$NewName)])[1] # find closeness where CC2 matches NewName (na.omit + [1] to get the number)
      tmp$gender[i] <-
        na.omit(tmp$NewGender[as.character(tmp$CC2[i]) == as.character(tmp$NewName)])[1] # (na.omit + [1] to get the number and not the rest of the na.omit list)
      tmp$ethnicity[i] <-
        na.omit(as.character(tmp$NewEthnicity[as.character(tmp$CC2[i]) == as.character(tmp$NewName)]))[1] # PROBLEM IS THAT THERE ARE TOO MANY NA's: Difficult to deal with
      tmp$relationship[i] <-
        na.omit(as.character(tmp$NewRelationship[as.character(tmp$CC2[i]) == as.character(tmp$NewName)]))[1]
    }
  }

  out <- tmp
  out
}

# split df per participants and run function
PP <- split(df, df$PID)
PP <- lapply(PP, createIntPartDf)
rm(createIntPartDf)

# add variables back to df
remergePP <- do.call(rbind.data.frame, PP)
colnames(remergePP) <-
  paste(colnames(remergePP), "_Calc", sep = "")
df <- cbind(df, remergePP)
rm(remergePP, PP)

dtS2$full <- df
rm(df)
```

### Study 3

For the medical professional sample, the data transformation stage had five main aims:

1.  We first create person, survey type, and measurement ID variables.\
2.  We then created indices of scales. Some indices were multi-item scales while some indices combine equivalent measurement for different situational circumstances (e.g., competence perceptions after interactions and at measurement occasions without interactions).\
3.  We cluster mean-center key variables within participants.\
4.  Finally, we calculated several basic participant summaries (averages across all measurement occasions).

```{r newVarsMedical}
df <- dtS3$clean

# Add ID variables
df$PID <- as.numeric(factor(df$ResponseId)) # participant ID

# Correct date_period ()
df <- df %>%
  mutate(
    startDate = as.Date(created),
    startTime = format(as.POSIXct(created), format = "%H:%M:%S"),
    date_period_old = date_period,
    date_period = ifelse(
      startTime >= c("12:00") & startTime <= c("18:59"),
      paste(format(as.POSIXct(created), format = "%Y-%m-%d"), "Morning"),
      ifelse(
        startTime >= c("19:00") & startTime <= c("23:59"),
        paste(format(as.POSIXct(created), format = "%Y-%m-%d"), "Afternoon"),
        ifelse(
          startTime >= c("00:00") & startTime <= c("11:59"),
          paste(as.Date(created) - 1, "Afternoon"),
          NA
        )
      )
    )
  )
  
# order time
# df$TID <- factor(df$date_period, levels = unique(dtS3$raw.daily$date_period))
# df$TIDnum <- as.numeric(df$TID) # get numeric TID
df <- df %>%
  mutate(
    dateOrder = stri_replace_all_regex(
      date_period,
      pattern = c(' Morning', ' Afternoon'),
      replacement = c('_A', '_B'),
      vectorize = FALSE
    ),
    TID = factor(date_period),
    TIDnum = as.numeric(factor(dateOrder))
  )

# check whether time ordering worked
df <- df %>%
  arrange(PID, TID)

# Interaction as Factor
df$interaction.f <-
  factor(df$Interaction,
    levels = c("no interaction", "Dutch", "Non-Dutch")
  )
df$intNL <- ifelse(df$Interaction == "Dutch", 1, 0)
df$intNonNL <- ifelse(df$Interaction == "Non-Dutch", 1, 0)

df <- df %>%
  mutate(
    NonDutchContact = replace_na(NonDutchNum, 0), # make second non-Dutch countable
    NonDutchContact = ifelse(NonDutchContact > 1, 1, 0) # recode (yes = 1 -> 1, no = 2 -> 0)
  ) %>%
  mutate(
    OutgroupInteraction = factor(
      InteractionDumDutch,
      levels = c(0, 1),
      labels = c("No", "Yes")
    ),
    NonOutgroupInteraction = factor(
      rowSums(select(., c(InteractionDumNonDutch, NonDutchContact)), na.rm = TRUE), # combine the two non-Dutch Q.,
      levels = c(0, 1),
      labels = c("No", "Yes")
    )
  )

df$IntergroupContact <- (df$IntergroupContact-2)*-1
df$IngroupContact <- (df$IngroupContact-2)*-1

# -------------------------------------------------------------------------------------------------------------
#                                       Combine Variables
# -------------------------------------------------------------------------------------------------------------
# Relatedness
pairs.panels.new(
  df[c("relatednessSelf", "relatednessOther")],
  labels = c(
    "I shared information about myself.",
    "X shared information about themselves."
  )
)
df$relatednessInteraction <-
  rowMeans(df[c("relatednessSelf", "relatednessOther")], na.rm = TRUE)
df$relatednessInteraction[df$relatednessInteraction == "NaN"] <- NA
# Relatedness Overall (JANNIS NOT SURE THESE ARE CORRECT, CHANGE ROWS?; J: Changed "NaN" in df$RelatednessInteraction to NA() should work now)
df$relatedness <-
  rowMeans(df[, c("relatednessInteraction", "relatednessNoInteraction")],
           na.rm = TRUE) %>%
  ifelse(is.nan(.), NA, .)


df$DaytimeNeedFulfillment[df$InteractionDum == 0 & !is.na(df$KeyNeedFulfillment)] <-
  df$KeyNeedFulfillment[df$InteractionDum == 0 & !is.na(df$KeyNeedFulfillment)]

df$InteractionNeedFullfillment <- NA 
df$InteractionNeedFullfillment[df$InteractionDum == 1 & !is.na(df$KeyNeedFulfillment)] <- 
  df$KeyNeedFulfillment[df$InteractionDum == 1 & !is.na(df$KeyNeedFulfillment)]

df$InteractionNeedImportance <- NA 
df$InteractionNeedImportance[df$InteractionDum == 1 & !is.na(df$KeyNeedImp)] <- 
  df$KeyNeedImp[df$InteractionDum == 1 & !is.na(df$KeyNeedImp)]


# Randomly selected variables
df <- df %>%
  rowwise() %>%
  mutate(
    ProSo = mean(c(ProSo1, ProSo2, ProSo3, ProSo4), na.rm = TRUE),
    AntiSo = mean(c(AntiSo1, AntiSo2, AntiSo3, AntiSo4, AntiSo5, AntiSo6, AntiSo7), na.rm = TRUE),
    agency = mean(c(agency1, agency2, agency3), na.rm = TRUE),
    autoFrust = mean(c(autoFrust1, autoFrust2, autoFrust3, autoFrust4), na.rm = TRUE),
    autoSat = mean(c(autoSat1, autoSat2, autoSat3, autoSat4), na.rm = TRUE),
    relatFrust = mean(c(relatFrust1, relatFrust2, relatFrust3, relatFrust4), na.rm = TRUE),
    relatSat = mean(c(relatSat1, relatSat2, relatSat3, relatSat4), na.rm = TRUE),
    compFrust = mean(c(compFrust1, compFrust2, compFrust3, compFrust4), na.rm = TRUE),
    compSat = mean(c(compSat1, compSat2, compSat3, compSat4), na.rm = TRUE),
    lonely = mean(c(lonely4, lonely4, lonely4, lonely4), na.rm = TRUE),
    emotRegPos = mean(c(emotRegPos01, emotRegPos02), na.rm = TRUE),
    emotRegNeg = mean(c(emotRegNeg01, emotRegNeg02), na.rm = TRUE)
  )


# Allport's Conditions
df %>%
  #filter(OutgroupInteraction == "Yes") %>%
  select(
    InteractionContextEqualStatus,
    KeyNeedShared,
    InteractionContextCooperative,
    InteractionContextvoluntary
  ) %>%
  pairs.panels.new

AlportDescr <- df %>%
  #filter(OutgroupInteraction == "Yes") %>%
  select(
    InteractionContextEqualStatus,
    KeyNeedShared,
    InteractionContextCooperative,
    InteractionContextvoluntary
  ) %>%
  psych::describe(., skew=F,ranges=T) %>%
  as.data.frame() %>%
  select(-vars) %>%
  kable(., caption = "Descriptives of Allport's Condition items") %>% 
  kable_styling("hover", full_width = F, latex_options = "hold_position")



iaWorkerAllport <- 
  df %>%
  #filter(OutgroupInteraction == "Yes") %>%
  select(
    InteractionContextEqualStatus,
    KeyNeedShared,
    InteractionContextCooperative,
    InteractionContextvoluntary
  )

itemScaleAllport01 <- sjPlot::tab_itemscale(iaWorkerAllport)

pca <- parameters::principal_components(iaWorkerAllport)
factor.groups <- parameters::closest_component(pca)

sjPlot::tab_itemscale(iaWorkerAllport, factor.groups, show.kurtosis = TRUE)


AllportAlpha <- ltm::cronbach.alpha(na.omit(iaWorkerAllport), CI = TRUE)


data <- 
  df %>%
  select(
    PID,
    TIDnum,
    InteractionContextEqualStatus,
    KeyNeedShared,
    InteractionContextCooperative,
    InteractionContextvoluntary
  ) %>%
  drop_na %>%
  reshape2::melt(
    ., 
    id.vars = c("PID", "TIDnum")
  )

AllportNestedAlpha <- horst::nestedAlpha(item.level.1 = "value",
                   level.2      = "TIDnum",
                   level.3      = "PID",
                   data         = data)
rm(data)

iaWorkerAllportScale <- 
  iaWorkerAllport %>%
  Scale::Scale() %>%
  Scale::ItemAnalysis()

df$AllportsCondition <-
  scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(
      InteractionContextEqualStatus,
      KeyNeedShared,
      InteractionContextCooperative,
      InteractionContextvoluntary
    ),
    min = 0,
    max = 100
  )$scores

as.data.frame(psych::describe(df$AllportsCondition, skew=T)) %>%
  mutate(vars = "Allport's Conditions Index") %>%
  kable(., caption = "Allport's Conditions: Scale Descriptives", row.names = FALSE) %>% 
  kable_styling("hover", full_width = F, latex_options = "hold_position")

ggplot(df, aes(x = AllportsCondition)) +
  geom_histogram() +
  theme_Publication()

# -------------------------------------------------------------------------------------------------------------
#                                 Add Variables related to interaction partner
# -------------------------------------------------------------------------------------------------------------
# create function for later lapply
createIntPartDf <- function(inp) {
  # prepare the dataframe so that we can forloop over it later
  tmp <- data.frame(
    CC = as.character(inp$CC),
    NewCC = as.character(inp$NewCC),
    NewName = as.character(inp$NewName),
    NewCloseness = inp$NewCloseness,
    NewGender = inp$NewGender,
    NewEthnicity = as.character(inp$NewEthnicity),
    NewRelationship = as.character(inp$NewRelationship)
  )

  tmp$CC2 <- recode(tmp$CC, "SOMEONE ELSE" = "NA")
  tmp$CC2 <-
    ifelse(
      tmp$CC == 1 |
        tmp$CC == "SOMEONE ELSE",
      as.character(tmp$NewName),
      as.character(tmp$CC2)
    )
  # maybe add [[:space:]]\b to remove space before word boundary or ^[[:space:]] to remove space in the beginning of a string
  tmp$CC2 <- gsub("^[[:space:]]", "", tmp$CC2)
  tmp$NewName <- gsub("^[[:space:]]", "", tmp$NewName)

  # open the variables that will be filled up in the foor-loop
  tmp$closeness <- rep(NA, nrow(tmp))
  tmp$gender <- rep(NA, nrow(tmp))
  tmp$ethnicity <- rep(NA, nrow(tmp))
  tmp$relationship <- rep(NA, nrow(tmp))

  # Run the for-loop. It finds the variables related to the name of the interaction partner. If there is a repeating interaction
  # partner (i.e. CC2) it takes the value (i.e. NewCloseness) from the first interaction (i.e. NewName)
  for (i in 1:nrow(tmp)) {
    if (is.na(tmp$CC2[i])) {
      next
    } else {
      tmp$closeness[i] <-
        na.omit(tmp$NewCloseness[as.character(tmp$CC2[i]) == as.character(tmp$NewName)])[1] # find closeness where CC2 matches NewName (na.omit + [1] to get the number)
      tmp$gender[i] <-
        na.omit(tmp$NewGender[as.character(tmp$CC2[i]) == as.character(tmp$NewName)])[1] # (na.omit + [1] to get the number and not the rest of the na.omit list)
      tmp$ethnicity[i] <-
        na.omit(as.character(tmp$NewEthnicity[as.character(tmp$CC2[i]) == as.character(tmp$NewName)]))[1] # PROBLEM IS THAT THERE ARE TOO MANY NA's: Difficult to deal with
      tmp$relationship[i] <-
        na.omit(as.character(tmp$NewRelationship[as.character(tmp$CC2[i]) == as.character(tmp$NewName)]))[1]
    }
  }

  out <- tmp
  out
}

# split df per participants and run function
PP <- split(df, df$PID)
PP <- lapply(PP, createIntPartDf)
rm(createIntPartDf)

# add variables back to df
remergePP <- do.call(rbind.data.frame, PP)
colnames(remergePP) <-
  paste(colnames(remergePP), "_Calc", sep = "")
df <- cbind(df, remergePP)
rm(remergePP, PP)

dtS3$full <- df
```

## Data Availability and Sample Selection {.tabset .tabset-fade}

As one of our main analyses is a three-mode principal component analysis (3MPCA) we begin by assessing the amount of missing data in each of the ESM studies. To assess the missingness in detail, we prepare a data availability table of whether data is available or missing for each possible measurement point (data + morning/afternoon) for all individual participants. We then export the data availability tables as comma separated value files (.csv) to be assessed in detail using a spreadsheet program (such as MS Excel).

For our sample selection we address both the selection of time points and participants. Given that multiple imputation procedures even work well with large proportions of missing data [assuming missingness at random; @Madley-Dowd2019], we decided on a general criterion of less than 45% missingness to balance sample size retention and bias in the multiple imputation model. Thus, we then select the time points for which we have less than 45% missingness and select participants who have less than 45% missingness across the selected time range.

To approximate an ideal sample selection that maximized the number of participants and timepoints that fit the 55% data availability criterion. We select sequentially remove the rows or columns with the lowest data availability rate untill all rows and columns have more than 55% data available (i.e., less than 45% missingness).

::: {.alert .alert-info .alert-dismissible .fade .in role="alert"}
<a class="close" data-dismiss="alert" aria-label="close">×</a> <i class="fas fa-info-circle"></i> <b>Note: </b><br/> This needs to be adjusted current missingness criterion set to more conservative 33%.
:::

### Study 1

```{r workerDataAvailability}
dtS1Availability <- dtS1$full %>%
  select(
    PID,
    TIDnum
  ) %>%
  arrange(PID, TIDnum) %>%
  mutate(data = 1)
dtS1Availability <- reshape::cast(dtS1Availability, PID ~ TIDnum) %>%
  select(-PID) %>%
  mutate_all(function(x) ifelse(x>1,1,x))
# sum(colMeans(dtS1Availability)*100 >= 66)
# sum(rowMeans(dtS1Availability)*100 >= 66)
# sum(colMeans(dtS1Availability[-5,])*100 >= 66)
# sum(rowMeans(dtS1Availability[,-8])*100 >= 66)

rownames(dtS1Availability) <- paste("PP", 1:nrow(dtS1Availability), sep = "_")
colnames(dtS1Availability) <- paste("t", 1:ncol(dtS1Availability), sep = "_")
dtS1AvailabilityKbl <- dtS1Availability

dtS1AvailabilityKbl[1:ncol(dtS1AvailabilityKbl)] <- lapply(dtS1AvailabilityKbl[1:ncol(dtS1AvailabilityKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS1AvailabilityKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 1: Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")

write.csv(dtS1Availability, "data/S1_Workers/processed/workerAvailability.csv")
```

```{r testOptimizer, include=FALSE}
# set.seed(42)
# nr <- 5
# nc <- 10
# df <- as.data.table(matrix(rbinom(nr*nc,1,.66), nrow=nr, ncol=nc))
# df
# 
# best.list.row.df = list()
# for (i in 1:nrow(df)) {
#   # get best subset for rows based on how many columns have more than 66% data
#   rowlist = combn(nrow(df), i)
#   numobs = apply(rowlist, 2, function(x) sum(colMeans(df[x,])*100 >= 66))
#   cat("For subsets of", i, "rows, the highest number of observations is", max(numobs), "out of the", ncol(df), "maximum. Product =", i*max(numobs),"\n")
#   best = which(numobs == max(numobs))[1]
#   best.list.row.df = c(best.list.row.df, list(rowlist[, best]))
# }
# 
# best.list.col.df = list()
# for (i in 1:ncol(df)) {
#   # get best subset for columns based on how many rows have more than 66% data
#   collist = combn(ncol(df), i)
#   numobs = apply(collist, 2, function(x) sum(rowMeans(df[, ..x])*100 >= 66))
#   cat("For subsets of", i, "columns, the highest number of participants is", max(numobs), "out of the", nrow(df), "maximum. Product =", i*max(numobs),"\n")
#   best = which(numobs == max(numobs))[1]
#   best.list.col.df = c(best.list.col.df, list(collist[, best]))
# }
# rm(df, best.list.row.df, best.list.col.df)
```

```{r testOptimizer03, include=FALSE}
# library(dplyr)
# library(ROI)
# library(ROI.plugin.glpk)
# library(ompr)
# library(ompr.roi)
# 
# set.seed(42)
# tf <- matrix(sample(c(TRUE, FALSE), 1488, replace=TRUE), 31)
# M3 <- t(replicate(31, 1969:2016, simplify=TRUE))
# M3[tf] <- NA
# 
# m <- +!is.na(M3) # gets logical matrix; 0 if NA else 1    
# #m <- as.data.frame(dtS1AvailabilityTest)
# nr <- nrow(m)
# nc <- ncol(m)
# n_years <- 15 
# 
# 
# model <- MIPModel() %>% 
#   # keep[i,j] is 1 if matrix cell [i,j] is to be kept else 0
#   add_variable(keep[i,j], i = 1:nr, j = 1:nc, typ = "binary") %>% 
#   # rm_row[i] is 1 if row i is selected for removal else 0
#   add_variable(rm_row[i], i = 1:nr, type = "binary") %>% 
#   # rm_col[j] is 1 if column j is selected for removal else 0
#   add_variable(rm_col[j], j = 1:nc, type = "binary") %>% 
#   # maximize good cells kept
#   set_objective(sum_expr(keep[i,j], i = 1:nr, j = 1:nc), "max") %>% 
#   # cell can be kept only when row is not selected for removal
#   add_constraint(sum_expr(keep[i,j], j = 1:nc) <= 1 - rm_row[i], i = 1:nr) %>%
#   # cell can be kept only when column is not selected for removal
#   add_constraint(sum_expr(keep[i,j], i = 1:nr) <= 1 - rm_col[j], j = 1:nc) %>%
#   # keep at most n_years columns i.e. remove at least (nc - n_years) columns
#   # only non-NA values can be kept
#   add_constraint(m[i,j] + rm_row[i] + rm_col[j] >= 1, i = 1:nr, j = 1:nc) %>% 
#   #add_constraint(sum_expr(m[,j], j = 1:nc)/nc*100 >= 66) %>% 
#   #add_constraint(sum_expr(m[i,], i = 1:nr)/nr*100 >= 66) %>% 
#   # keep cols with more 
#   #add_constraint(colMeans(m[i,j], i = 1:nr, j = 1:nc) >= 0.66) %>%
#   # I used >= instead of == to avoid infeasiblity
#   #add_constraint(sum_expr(rm_col[j], j = 1:nc) >= nc - n_years) %>% 
#   # solve using free glpk solver
#   solve_model(with_ROI(solver = "glpk", verbose = TRUE))
# 
# solver_status(model)
# rm_rows <- model %>% 
#   get_solution(rm_row[i]) %>% 
#   filter(value > 0) %>% 
#   pull(i)
# 
# rm_cols <- model %>% 
#   get_solution(rm_col[j]) %>% 
#   filter(value > 0) %>% 
#   pull(j) 
# 
# result <- m[-rm_rows, -rm_cols, drop = F]
# result
```

```{r testOptimizer05}
cleanM <- function(M, c = 55) { #c = 66
  check <- list()
  rowNamesOut <- c()
  colNamesOut <- c()
  for (i in 1:length(c(row.names(M), names(M)))) {
    rowMeans <- rowMeans(M) * 100
    colMeans <- colMeans(M) * 100
    rcMeans <- c(rowMeans, colMeans)
    rm <- which.min(rcMeans) %>% names
    rc <- ifelse(startsWith(rm, "PP_"), "row", "col")
    
    check[[i]] <- rcMeans
    
    if (!all(rcMeans >= c) && rc == "row") {
      M <- M[!(row.names(M) %in% rm), ]
      rowNamesOut <- append(rowNamesOut, rm)
      cat(i, ": Row ", rm, " had a completion rate of ", format(round(min(rcMeans), 2), nsmall = 2), "% and was removed.\n", sep = "")
    } else if (!all(rcMeans >= c) && rc == "col") {
      M <- M[, !(names(M) %in% rm)]
      colNamesOut <- append(colNamesOut, rm)
      cat(i, ": Column ", rm, " had a completion rate of ", format(round(min(rcMeans), 2), nsmall = 2), "% and was removed.\n", sep = "")
    } else {
      cat(i, ": All row- and column means are over ", c, "%. The final matrix has ", nrow(M), " rows and ", ncol(M), " columns.", sep = "")
      return(
        list(
          reducedMatrix = M,
          rowNamesIn = row.names(M),
          colNamesIn = names(M),
          rowNamesOut = rowNamesOut,
          colNamesOut = colNamesOut,
          rowMeans = rowMeans,
          colMeans = colMeans
        )
      )
    }
  }
}

dtS1RedInfo <- cleanM(M = as.data.frame(dtS1Availability), c = 55)

PIDout <- gsub("PP_", "", dtS1RedInfo$rowNamesOut) %>% as.numeric
TIDout <- gsub("t_", "", dtS1RedInfo$colNamesOut) %>% as.numeric
TIDInRed <- gsub("t_", "", dtS1RedInfo$colNamesIn) %>% as.numeric
TIDIn <- seq(min(TIDInRed), max(TIDInRed), 1)

dtS1Red <- dtS1$full %>%
  filter(
    !PID %in% PIDout,
    TIDnum %in% TIDIn
  ) %>% 
  mutate(TIDnum = TIDnum - min(TIDnum))
rm(PIDout, TIDout, TIDInRed, TIDIn)

# change glitch where survey was sent too early
dtS1Red$TIDnum[dtS1Red$PID == 7 & as.character(dtS1Red$created) == "2018-05-18 11:46:40"] <- 10
dtS1Red$TID[dtS1Red$PID == 7 & as.character(dtS1Red$created) == "2018-05-18 11:46:40"] <- "2018-05-18 Morning"

dtS1AvailabilityRedKbl <- dtS1RedInfo$reducedMatrix
dtS1AvailabilityRedKbl[1:ncol(dtS1AvailabilityRedKbl)] <- lapply(dtS1AvailabilityRedKbl[1:ncol(dtS1AvailabilityRedKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS1AvailabilityRedKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 1: Final Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
```

### Study 2

```{r studentDataAvailability}
dtS2Availability <- dtS2$full %>%
  select(
    PID,
    TIDnum
  ) %>%
  #na.omit %>%
  arrange(PID, TIDnum) %>%
  mutate(data = 1)
dtS2Availability <- reshape::cast(dtS2Availability, PID ~ TIDnum) %>%
  select(-PID)

rownames(dtS2Availability) <- paste("PP", 1:nrow(dtS2Availability), sep = "_")
colnames(dtS2Availability) <- paste("t", 1:ncol(dtS2Availability), sep = "_")

write.csv(dtS2Availability, "data/S2_Students/processed/studentAvailability.csv")

dtS2AvailabilityKbl <- dtS2Availability
dtS2AvailabilityKbl[1:ncol(dtS2AvailabilityKbl)] <- lapply(dtS2AvailabilityKbl[1:ncol(dtS2AvailabilityKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS2AvailabilityKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 2: Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
```

```{r s2SampleReduction}
dtS2RedInfo <- cleanM(M = as.data.frame(dtS2Availability), c = 55)

PIDout <- gsub("PP_", "", dtS2RedInfo$rowNamesOut) %>% as.numeric
TIDout <- gsub("t_", "", dtS2RedInfo$colNamesOut) %>% as.numeric
TIDInRed <- gsub("t_", "", dtS2RedInfo$colNamesIn) %>% as.numeric
TIDIn <- seq(min(TIDInRed), max(TIDInRed), 1)

dtS2Red <- dtS2$full %>%
  filter(
    !PID %in% PIDout,
    TIDnum %in% TIDIn
  ) %>% 
  mutate(TIDnum = TIDnum - min(TIDnum))
rm(PIDout, TIDout, TIDInRed, TIDIn)

# remove glitch entries 
dtS2Red <- dtS2Red[!(dtS2Red$PID == 46 & dtS2Red$TIDnum == 57 & dtS2Red$ended == "2018-12-20 21:58:31"),]
dtS2Red <- dtS2Red[!(dtS2Red$PID == 46 & dtS2Red$TIDnum == 59 & dtS2Red$ended == "2018-12-20 21:58:31"),]

dtS2AvailabilityRedKbl <- dtS2RedInfo$reducedMatrix
dtS2AvailabilityRedKbl[1:ncol(dtS2AvailabilityRedKbl)] <- lapply(dtS2AvailabilityRedKbl[1:ncol(dtS2AvailabilityRedKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS2AvailabilityRedKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 2: Final Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
```

### Study 3

```{r medicalDataAvailability}
dtS3Availability <- dtS3$full %>%
  select(
    PID,
    TIDnum
  ) %>%
  arrange(PID, TIDnum) %>%
  mutate(data = 1)
dtS3Availability <- reshape::cast(dtS3Availability, PID ~ TIDnum, fill = 0) %>%
  select(-PID)

rownames(dtS3Availability) <- paste("PP", 1:nrow(dtS3Availability), sep = "_")
colnames(dtS3Availability) <- paste("t", 1:ncol(dtS3Availability), sep = "_")

write.csv(dtS3Availability, "data/S3_Medical/processed/medicalAvailability.csv")

dtS3AvailabilityKbl <- dtS3Availability
dtS3AvailabilityKbl[1:ncol(dtS3AvailabilityKbl)] <- lapply(dtS3AvailabilityKbl[1:ncol(dtS3AvailabilityKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS3AvailabilityKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 2: Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
rm(dtS3AvailabilityKbl)
```

```{r s3SampleReduction}
dtS3RedInfo <- cleanM(M = as.data.frame(dtS3Availability), c = 55)

PIDout <- gsub("PP_", "", dtS3RedInfo$rowNamesOut) %>% as.numeric
TIDout <- gsub("t_", "", dtS3RedInfo$colNamesOut) %>% as.numeric
TIDInRed <- gsub("t_", "", dtS3RedInfo$colNamesIn) %>% as.numeric
TIDIn <- seq(min(TIDInRed), max(TIDInRed), 1)

dtS3Red <- dtS3$full %>%
  filter(
    !PID %in% PIDout,
    TIDnum %in% TIDIn
  ) %>% 
  mutate(TIDnum = TIDnum - min(TIDnum))
rm(PIDout, TIDout, TIDInRed, TIDIn)

dtS3AvailabilityRedKbl <- dtS3RedInfo$reducedMatrix
dtS3AvailabilityRedKbl[1:ncol(dtS3AvailabilityRedKbl)] <- lapply(dtS3AvailabilityRedKbl[1:ncol(dtS3AvailabilityRedKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS3AvailabilityRedKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 3: Final Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
```

## Time Scales and Timepoint Aggregation

```{r checkVarAvailability}
varNamesPre <- varNames %>%
  filter(
    surveyS1 == "pre",
    surveyS2 == "pre",
    str_detect(surveyS3, "^pre")
  ) %>%
  select(varNam)
varNamesPreFull <-
  data.frame(varNam = Reduce(intersect, list(
    names(dtS1$full), names(dtS3$full), names(dtS3$full)
  ))) %>%
  filter(str_detect(varNam, ".pre$"))

varNamesDaily <- varNames %>%
  filter(
    surveyS1 == "daily",
    surveyS2 == "daily",
    surveyS3 == "daily"
  ) %>%
  select(varNam)
varNamesDailyFull <-
  data.frame(varNam = Reduce(intersect, list(
    names(dtS1Red), names(dtS2Red), names(dtS3Red)
  ))) %>%
  filter(!str_detect(varNam, ".pre$|.post$"))

varNamesPost <- varNames %>%
  filter(
    surveyS1 == "post",
    surveyS2 == "post",
    surveyS3 == "post"
  ) %>%
  select(varNam)
varNamesPostFull <-
  data.frame(varNam = Reduce(intersect, list(
    names(dtS1Red), names(dtS3Red), names(dtS3Red)
  ))) %>%
  filter(str_detect(varNam, ".post$"))
```

```{r analysisDFs}
# Main Analysis: 3MPCA (common variables across studies)
dropVarPcaMain <- c(
  # Meta Data
  "last_outside_referrer",
  "created",
  "ended",
  "ip_address",
  "date",
  "ResponseId",
  "server_language",
  "Meta_Browser",
  # Interaction Dummies (non-continuous)
  "IntergroupContact",
  "IngroupContact",
  "InteractionDum",
  # Interaction Counts (non-continuous)
  "ContactNum",
  "NonDutchNum",
  "groupDutch",
  # Interaction Descriptives (non-continuous)
  "duration",
  "dyadGroup",
  "groupSize",
  # Keyneed Freetexts (non-continuous)
  "KeyNeed",
  "DaytimeNeed",
  # Relatedness individual items
  "relatednessSelf",
  "relatednessOther",
  "relatednessNoInteraction"
)

varNam3mpcaMain <- varNamesDailyFull %>%
  filter(!varNam %in% dropVarPcaMain) %>%
  pull

dtMain3mpca <- rbind(
  dtS1Red %>% select(any_of(varNam3mpcaMain)) %>% mutate(study = "S1"),
  dtS3Red %>% select(any_of(varNam3mpcaMain)) %>% mutate(study = "S2"),
  dtS3Red %>% select(any_of(varNam3mpcaMain)) %>% mutate(study = "S3")
  ) %>%
  group_by(study, PID) %>%
  mutate(ID = cur_group_id()) %>%
  ungroup %>%
  mutate(
    date = as.Date(gsub(" .*", "", TID)),
    week = strftime(date, format = "%Y-W%V")
    ) %>%
  select(
    ID,
    PID,
    TID,
    date,
    week,
    TIDnum,
    study,
    everything()
  ) %>%
  arrange(ID, TIDnum)

dtMain3mpcaDaily <- dtMain3mpca %>%
  group_by(ID, date, study) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup %>%
  group_by(study) %>%
  mutate(TIDnum = as.numeric(factor(date))) %>%
  ungroup %>%
  select(ID, date, TIDnum, everything()) %>%
  mutate_all(~ifelse(is.nan(.), NA, .))

dtMain3mpcaWeekly <- dtMain3mpca %>%
  group_by(ID, week, study) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup %>%
  group_by(study) %>%
  mutate(TIDnum = as.numeric(factor(week))) %>%
  ungroup %>%
  select(ID, week, TIDnum, everything()) %>%
  mutate_all(~ifelse(is.nan(.), NA, .))
```

We aggregate the key variables over time to archive a reasonably interpretable number of time points and remove a first proportion of missing data. Given that little data is available on the meaningful time scales of the selected psychological variables, we chose to determine the appropriate time scales using variance decomposition [e.g., see @Ram2014]. This is to say that we create multi-level unconditional means models (without predictors) that include possible nested time scales as levels. We chose to select time scales that align with common human cycles. We thus compare the variances of bi-daily, daily, and weekly aggregations. Additional aggregations of two weeks or the full four weeks might be possible but would most likely reduce the variance too much for any meaningul further reduction during the 3MPCA. We then chose the time scales that have the most variance.

```{r keyNeedTest}

# WILL USE IMPUTED DATA AND FOR-LOOP ACROSS ALL RELEVANT DVs
# PLOT EFFECT SIZES

dtMain3mpca <- dtMain3mpca %>%
  group_by(study) %>%
  mutate(
    TIDdate = as.numeric(factor(date)),
    TIDweek = as.numeric(factor(week))
  ) %>%
  ungroup

# Not sure whether TIDs as predictors or empty model?
keyNeedMlTest01 <-
  lme4::lmer(KeyNeedFulfillment ~ 1 + TIDnum + TIDdate + TIDweek + (1 + TIDnum + TIDdate + TIDweek | ID),
    data = dtMain3mpca
  ) 

keyNeedMlTest02 <-
  lme(
    KeyNeedFulfillment ~ 1 + TIDnum + TIDdate + TIDweek,
    random = ~ 1 + TIDnum + TIDdate + TIDweek | ID,
    data = dtMain3mpca %>% filter(!is.na(KeyNeedFulfillment)),
    control = list(opt = "nlmimb")
  ) # use optim if it does not converge

# summary(keyNeedMlTest02)
# summ(keyNeedMlTest01, digits = 3, center = FALSE)
# anova(keyNeedMlTest02)

```

## Multiple Imputation

We ultimately aim to create 20 imputed data sets to perform the analyses on. As outlined by @Monden2015, we will use the key variables themselves as well as auxiliary variables to impute missing values. Auxiliary variables are any pre-, post-, or daily dairy variables other than the main variables. If there are any modeling issues we will limit the auxiliary variables to all variables that that are significantly ($p$ \< .01) and meaningfully ($r$ \> .3) correlated with (1) the key variables or (2) missingness on the key variables.

We create the imputed data set for each of the analyses. This includes the main analyses with across all three studies, using the set of common variables, as well as follow-up analyses for study-specific, interaction-specific and time scale specific data sets.

```{r varNames}
varNamS123Aux <- varNames %>%
  filter(
    aux != 0,
    studyS1 == "S1",
    studyS2 == "S2",
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull
varNamS1Aux <- varNames %>%
  filter(
    aux != 0,
    studyS1 == "S1"
  ) %>%
  select(varNam) %>%
  pull
varNamS2Aux <- varNames %>%
  filter(
    aux != 0,
    studyS2 == "S2"
  ) %>%
  select(varNam) %>%
  pull
varNamS3Aux <- varNames %>%
  filter(
    aux != 0,
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull

varNamS123PCA <- varNames %>%
  filter(
    pca != 0,
    studyS1 == "S1",
    studyS2 == "S2",
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull
varNamS1PCA <- varNames %>%
  filter(
    pca != 0,
    studyS1 == "S1"
  ) %>%
  select(varNam) %>%
  pull
varNamS2PCA <- varNames %>%
  filter(
    pca != 0,
    studyS2 == "S2"
  ) %>%
  select(varNam) %>% 
  pull
varNamS3PCA <- varNames %>%
  filter(
    pca != 0,
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull

varNamOut <- c(
  "ResponseId",
  "relatednessNoInteraction",
  "relatednessSelf",
  "relatednessOther",
  "autonomy_Int",
  "autonomy_NoInt",
  "competence_Int",
  "competence_NoInt",
  varNamS1PCA[grepl('^MDMQ', varNamS1PCA)],
  varNamS2PCA[grepl('^ProSo|^AntiSo', varNamS2PCA)],
  varNamS3PCA[grepl(
    '^ProSo|^AntiSo|^agency|^autoFrust|^autoSat|^relatFrust|^relatSat|^compFrust|^compSat|^lonely[0-9]|^emotRegPos|^emotRegNeg',
    varNamS3PCA
  )]
)
varNamCore <- c(
  "PID",
  "TID",
  "TIDnum"
)
varNamNewAll <- c(
  "relatedness",
  "autonomy",
  "competence"
)
varNamNewS1 <- c(
  "relatedness",
  "autonomy",
  "competence",
  "alertness", 
  "calmness", 
  "valence"
)
varNamNewS2 <- c(
  "relatedness",
  "autonomy",
  "competence",
  "ProSo", 
  "AntiSo"
)
varNamNewS3 <- c(
  "relatedness",
  "autonomy",
  "competence",
  "ProSo", 
  "AntiSo",
  "agency",
  "autoFrust",
  "autoSat",
  "relatFrust",
  "relatSat",
  "compFrust",
  "compSat",
  "lonely",
  "emotRegPos",
  "emotRegNeg"
)

varNamNewCat <- c(
  "closeness_Calc",
  "gender_Calc",
  "ethnicity_Calc",
  "relationship_Calc"
)

varNamIntDep <- c(
  varNamS123PCA[grepl('^InteractionContext|AttitudesPartner|KeyNeedDueToPartner|^quality', varNamS123PCA)]
)

varNamIndiceItemsS1 <- varNames %>%
  filter(
    aux == -1,
    studyS1 == "S1"
  ) %>%
  select(varNam) %>%
  pull %>%
  append(., gsub(".pre|.post|_calc", "", varNamIndicesS1)) %>%
  unique

varNamS123MI <- c(varNamCore, varNamNewAll, varNamS123Aux)
varNamS1MI <- c(varNamCore, varNamIndicesS1, varNamS1Aux[!varNamS1Aux %in% varNamIndiceItemsS1]) #
varNamS2MI <- c(varNamCore, varNamNewS2, varNamS2Aux)
varNamS3MI <- c(varNamCore, varNamNewS3, varNamS3Aux)

varNamS123PCA <- c(varNamCore, varNamS123PCA[!varNamS123PCA %in% varNamOut])
varNamS1PCA <- c(varNamCore, varNamNewS1, varNamS1PCA[!varNamS1PCA %in% varNamOut])
varNamS2PCA <- c(varNamCore, varNamNewS2, varNamS2PCA[!varNamS2PCA %in% varNamOut])
varNamS3PCA <- c(varNamCore, varNamNewS3, varNamS3PCA[!varNamS3PCA %in% varNamOut])

idVars <- c("ID", "PID", "TID", "TIDnum", "date", "week", "study")
```

### Main Analysis

We begin by preparing the data for the main analysis, which combines the common variables of all three studies. Here, we:

1.  Select the variables for all relevant analysis (i.e., imputation, 3MPCA, and validation) across all three studies.
2.  Create the combined dataset in the correct format (long format with all missed time points as NAs).
3.  Impute the data and save the imputed datasets for later recall.

```{r MiMainPrep}
#################################################################
##                    Variable Name Vectors                    ##
#################################################################
varNamS123MIPsbl <- c(
  paste(varNamS123MI, rep(".pre", length(varNamS123MI)), sep = ""),
  varNamS123MI,
  paste(varNamS123MI, rep(".post", length(varNamS123MI)), sep = "")
)

varNamS123MiRed <- Reduce(
  intersect,
  list(
    dtS1Red %>% select(PID, TID, TIDnum, any_of(varNamS123MIPsbl)) %>% names,
    dtS2Red %>% select(PID, TID, TIDnum, any_of(varNamS123MIPsbl)) %>% names,
    dtS3Red %>% select(PID, TID, TIDnum, any_of(varNamS123MIPsbl)) %>% names
  )
)

varNamMiMainNoms <- varNames %>%
  filter(
    aux == 1,
    format == "nominal",
    studyS1 == "S1",
    studyS2 == "S2",
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull

#################################################################
##               EXPORT FOR STM MACHINE LEARNING               ##
#################################################################
# bannerCommenter::banner("EXPORT FOR STM MACHINE LEARNING", centre = TRUE, bandChar = "#")
# varNamPosAdd <- c("relatedness", "autonomy", "competence")
# 
# dtAllStm <- rbind(
#   dtS1$full %>% select(KeyNeed, DaytimeNeed, any_of(varNamS123MiRed), any_of(varNamPosAdd)) %>% mutate(study = "S1") %>%
#     mutate(across(!TID & !study & !KeyNeed & !DaytimeNeed, as.numeric)),
#   dtS2$full %>% select(KeyNeed, DaytimeNeed, any_of(varNamS123MiRed), any_of(varNamPosAdd)) %>% mutate(study = "S2"),
#   dtS3$full %>% select(KeyNeed, DaytimeNeed, any_of(varNamS123MiRed), any_of(varNamPosAdd)) %>% mutate(study = "S3")
# ) %>%
#   group_by(study, PID) %>%
#   mutate(ID = cur_group_id()) %>%
#   ungroup %>%
#   mutate(
#     date = as.Date(gsub(" .*", "", TID)),
#     week = strftime(date, format = "%Y-W%V")
#   ) %>%
#   select(ID,
#          PID,
#          study,
#          TID,
#          TIDnum,
#          date,
#          week,
#          IntergroupContact,
#          IngroupContact,
#          everything(),
#          -starts_with("ResponseId")) %>%
#   as.data.frame
# 
# dtAllStm2 <- dtAllStm
# 
# dtAllStm2$IntGrAnx.pre_calc <-
#   psych::scoreItems(
#     keys = c(rep(1,6),rep(-1,6)),
#     items = dtAllStm2 %>% select(matches("^IntGrAnx[0-9]{2}.?\\.pre")),
#     min = 1,
#     max = 10
#   )$scores %>%
#   as.numeric
# 
# dtAllStm2$IntGrAnx.post_calc <-
#   psych::scoreItems(
#     keys = c(rep(1,6),rep(-1,6)),
#     items = dtAllStm2 %>% select(matches("^IntGrAnx[0-9]{2}.?\\.post")),
#     min = 1,
#     max = 10
#   )$scores %>%
#   as.numeric
# 
# dtAllStm2$swl.pre_calc <-
#   psych::scoreItems(
#     keys = rep(1,5),
#     items = dtAllStm2 %>% select(matches("^SWL[0-9]{2}\\.pre")),
#     min = 1,
#     max = 7,
#     missing = TRUE,
#     impute = "none",
#   )$scores %>%
#   as.numeric
# 
# dtAllStm2$discrimination.post_calc <-
#   psych::scoreItems(
#     keys = rep(1,9),
#     items = dtAllStm2 %>% select(matches("^EvDayDiscr[0-9]{2}\\.post")),
#     min = 1,
#     max = 6,
#     missing = TRUE,
#     impute = "none",
#   )$scores %>%
#   as.numeric
# 
# dtAllStm3 <- dtAllStm2 %>%
#   select(
#     -matches("^IntGrAnx[0-9]{2}.?\\.pre"),
#     -matches("^IntGrAnx[0-9]{2}.?\\.post"),
#     -matches("^SWL[0-9]{2}\\.pre"),
#     -matches("^EvDayDiscr[0-9]{2}\\.post"),
#     -relatednessSelf,
#     -relatednessOther,
#     -relatednessNoInteraction
#   )
# 
# write.csv2(dtAllStm3,
#            file = "data/keyMotivesSTM.csv")
#################################################################


#################################################################
##                    Dataframe Preparation                    ##
#################################################################
dtMiMain <- rbind(
  dtS1Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S1") %>% 
    mutate(across(!TID & !study, as.numeric)),
  dtS2Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S2"),
  dtS3Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S3")
) %>%
  group_by(study, PID) %>%
  mutate(ID = cur_group_id()) %>%
  ungroup %>%
  mutate(
    date = as.Date(gsub(" .*", "", TID)),
    week = strftime(date, format = "%Y-W%V")
  ) %>%
  rowwise() %>%
  mutate(
    InteractionDum = sum(IntergroupContact, IngroupContact, na.rm = TRUE),
    InteractionDum = if_else(InteractionDum > 0, 1, InteractionDum)
  ) %>%
  ungroup() %>%
  select(ID,
         study,
         date,
         week,
         everything(),
         -PID,
         -starts_with("ResponseId"), 
         -starts_with("roommate"),
         -starts_with("Reason")) %>%
  as.data.frame %>%
  select_if(~ sum(!is.na(.)) > 1) %>% # only include variables that have any data (i.e., not all NA)
  complete(., ID, TIDnum) %>% # add na if id and TID combination is missing
  group_by(ID) %>%
  mutate(across(ends_with(c(".pre", ".post")),  ~ replace_na(.x, mean(.x, na.rm = TRUE)))) %>%
  mutate(study = replace_na(study, calc_mode(study))) %>%
  ungroup %>%
  group_by(study, TIDnum) %>%
  mutate(across(c("date", "week", "TID"),  ~ replace_na(.x, calc_mode(.x)))) %>%
  ungroup %>% 
  mutate(across(ends_with(c(".pre", ".post")), ~ ifelse(is.nan(.), NA, .))) %>%
  as.data.frame 
```


```{r MainImp}
#################################################################
##                     Imputation Bi-daily                     ##
#################################################################
# set.seed(123)
# MiMain <-
#   amelia(
#     dtMiMain,
#     m = 1, # 20
#     ts = "TIDnum",
#     cs = "ID",
#     idvars = c("TID", "date", "week", "study", "IntergroupContact", "IngroupContact"), #"PID",
#     #lags = names(dtMiMain)[!names(dtMiMain) %in% idVars],
#     #lags = names(dtMiMain)[names(dtMiMain) %in%varNamInMiMainRed],
#     #noms = names(dtMiMain)[grepl(paste(varNamMiMainNoms, collapse = "|^"), names(dtMiMain))],
#     noms = c("InteractionDum"),
#     parallel = "multicore",
#     ncpus = parallel::detectCores(),
#     p2s = 1
#   )
# save(MiMain, file = "data/imputed/MainImputed.RData")
load("data/imputed/MainImputed.RData")


# # Stability to imputations:
# set.seed(123)
# MiMainStability <-
#   amelia(
#     dtMiMain,
#     m = 20,
#     ts = "TIDnum",
#     cs = "ID",
#     idvars = c(
#       "TID",
#       "date",
#       "week",
#       "study",
#       "IntergroupContact",
#       "IngroupContact"
#     ),
#     #lags = names(dtMiMain)[!names(dtMiMain) %in% idVars],
#     #lags = names(dtMiMain)[names(dtMiMain) %in%varNamInMiMainRed],
#     #noms = names(dtMiMain)[grepl(paste(varNamMiMainNoms, collapse = "|^"), names(dtMiMain))],
#     noms = c("InteractionDum"),
#     parallel = "multicore",
#     ncpus = parallel::detectCores(),
#     p2s = 1
#   )
# save(MiMainStability, file = "data/imputed/MainImputedStability.RData")
load("data/imputed/MainImputedStability.RData")

##---------------------------------------------------------------
##                Restore Missingness by Design                --
##---------------------------------------------------------------

# SHOULD STILL BE DONE? 
# (remove imputed values that were missing by design. E.g., partner attitudes when there were no interactions)

```

### Follow-up: Studies

#### Study 1

```{r MiS1Prep}
#################################################################
##                    Variable Name Vectors                    ##
#################################################################
varNamS1MiPsbl <- c(
  paste(varNamS1MI, rep(".pre", length(varNamS1MI)), sep = ""),
  varNamS1MI,
  paste(varNamS1MI, rep(".post", length(varNamS1MI)), sep = "")
)

varNamdtS1Noms <- varNames %>%
  filter(
    aux == 1,
    format == "nominal",
    studyS1 == "S1"
  ) %>%
  select(varNam) %>%
  pull

#################################################################
##                    Dataframe Preparation                    ##
#################################################################
dtMiS1 <- dtS1Red %>% 
  select(TID, any_of(varNamS1MiPsbl)) %>% 
  mutate(across(!TID, as.numeric)) %>%
  mutate(
    date = as.Date(gsub(" .*", "", TID)),
    week = strftime(date, format = "%Y-W%V")
  ) %>%
  select(PID,
         date,
         week,
         everything(),
         -starts_with("ResponseId")) %>%
  as.data.frame %>% 
  select_if(~ sum(!is.na(.)) > 1) %>% # only include variables that have any data (i.e., not all NA)
  complete(., PID, TIDnum) %>% # add na if id and TID combination is missing
  group_by(PID) %>%
  mutate(across(ends_with(c(".pre", ".post")),  ~ replace_na(.x, mean(.x, na.rm = TRUE)))) %>%
  ungroup %>% 
  group_by(TIDnum) %>%
  mutate(across(c("date", "week", "TID"),  ~ replace_na(.x, calc_mode(.x)))) %>%
  ungroup %>% 
  mutate(across(ends_with(c(".pre", ".post")), ~ ifelse(is.nan(.), NA, .))) %>% 
  select(PID,
         TID,
         TIDnum,
         date,
         week,
         which(sapply(., sd, na.rm = TRUE) > 0)) %>% # only include variables that vary (i.e., sd != 0)
  as.data.frame 


allDuplicated <- function(vec){
  front <- duplicated(vec)
  back <- duplicated(vec, fromLast = TRUE)
  all_dup <- front + back > 0
  return(all_dup)
}

varNamDifS1 <- names(dtMiS1)[gsub(".pre|.post|_calc", "", names(dtMiS1)) %>% allDuplicated]

# Variables removed for study 1 due to high multicollinearity:
varNamS1Out <- dtMiS1 %>%
  select(
    any_of(varNamDifS1),
    mdmq.calmness.pre_calc,
    mdmq.alertness.pre_calc,
    mdmq.valence.pre_calc,
    relatednessFrust.pre_calc,
    competenceFrust.pre_calc,
    competenceSat.pre_calc,
    relatednessSat.pre_calc,
    DutchFriends.pre,
    secNatDum.pre,
    timeNL.pre,
    stress.post,
    discrimination_month.post,
    rosenberg.post,
    qualityStar,
    starts_with("SSAS")
  ) %>%
  names

dtMiS1 <- dtMiS1 %>%
  # mutate(
  #   assimilation.dif = assimilation.post - assimilation.pre,
  #   integration.dif = integration.post - integration.pre,
  #   marginalization.dif = marginalization.post - marginalization.pre,
  #   separation.dif = separation.post - separation.pre,
  #   VIA_Dutch.dif = VIA_Dutch.post - VIA_Dutch.post,
  #   VIA_heritage.dif = VIA_heritage.post - VIA_heritage.pre,
  #   IntGrAnx.dif = IntGrAnx.post_calc - IntGrAnx.pre_calc,
  #   swl.dif = swl.post_calc - swl.pre_calc
  # ) %>% 
  select(
    -any_of(varNamS1Out)
  ) %>%
  select(PID,
         TID,
         TIDnum,
         date,
         week,
         which(sapply(., sd, na.rm = TRUE) > 0)) %>% 
  mutate_all(~ifelse(is.nan(.), NA, .)) %>%
  as.data.frame

# lares::corr_cross(
#   dtMiS1 %>% select_if(is.numeric), # name of dataset
#   max_pvalue = 0.05, # display only significant correlations (at 5% level)
#   top = 20 # display top 20 couples of variables (by correlation coefficient)
# )
```

```{r MiS1}
#################################################################
##                     Imputation Bi-daily                     ##
#################################################################
# set.seed(123)
# MiS1 <-
#   amelia(
#     dtMiS1,
#     m = 1, # 20
#     ts = "TIDnum",
#     cs = "PID",
#     idvars = c("TID", "date", "week", "IntergroupContact", "IngroupContact"),
#     # lags = names(dtMiS1)[!names(dtMiS1) %in% idVars],
#     # lags = names(dtMiS1)[names(dtMiS1) %in% varNamS1MiPsbl], # alternative?
#     # leads = ...,
#     # noms = names(dtMiS1)[grepl(paste(varNamdtS1Noms, collapse = "|^"), names(dtMiS1))],
#     # parallel = "multicore",
#     # ncpus = parallel::detectCores(),
#     p2s = 1
#   )
# save(MiS1, file = "data/imputed/S1Imputed.RData")
load("data/imputed/S1Imputed.RData")

# # Imputation Stability:
# set.seed(123)
# MiS1Stability <-
#   amelia(
#     dtMiS1,
#     m = 20,
#     ts = "TIDnum",
#     cs = "PID",
#     idvars = c("TID", "date", "week", "IntergroupContact", "IngroupContact"),
#     # lags = names(dtMiS1)[!names(dtMiS1) %in% idVars],
#     # lags = names(dtMiS1)[names(dtMiS1) %in% varNamS1MiPsbl], # alternative?
#     # leads = ...,
#     # noms = names(dtMiS1)[grepl(paste(varNamdtS1Noms, collapse = "|^"), names(dtMiS1))],
#     # parallel = "multicore",
#     # ncpus = parallel::detectCores(),
#     p2s = 2
#   )
# save(MiS1Stability, file = "data/imputed/S1ImputedStability.RData")
load("data/imputed/S1ImputedStability.RData")
```

#### Study 2

To be done once initial tests approved.

```{r MiS2Prep}
#################################################################
##                    Variable Name Vectors                    ##
#################################################################
varNamS2MiPsbl <- c(
  paste(varNamS2MI, rep(".pre", length(varNamS2MI)), sep = ""),
  varNamS2MI,
  paste(varNamS2MI, rep(".post", length(varNamS2MI)), sep = "")
)

varNamdtS2Noms <- varNames %>%
  filter(
    aux == 1,
    format == "nominal",
    studyS2 == "S2"
  ) %>%
  select(varNam) %>%
  pull

#################################################################
##                    Dataframe Preparation                    ##
#################################################################
dtMiS2 <- dtS2Red %>% 
  select(TID, any_of(varNamS2MiPsbl)) %>% 
  mutate(across(!TID, as.numeric)) %>%
  mutate(
    date = as.Date(gsub(" .*", "", TID)),
    week = strftime(date, format = "%Y-W%V")
  ) %>%
  select(PID,
         date,
         week,
         everything(),
         -starts_with("ResponseId")) %>%
  as.data.frame %>% 
  select_if(~ sum(!is.na(.)) > 1) %>% # only include variables that have any data (i.e., not all NA)
  complete(., PID, TIDnum) %>% # add na if id and TID combination is missing
  group_by(PID) %>%
  mutate(across(ends_with(c(".pre", ".post")),  ~ replace_na(.x, mean(.x, na.rm = TRUE)))) %>%
  ungroup %>% 
  group_by(TIDnum) %>%
  mutate(across(c("date", "week", "TID"),  ~ replace_na(.x, calc_mode(.x)))) %>%
  ungroup %>% 
  mutate(across(ends_with(c(".pre", ".post")), ~ ifelse(is.nan(.), NA, .))) %>% 
  select(PID,
         TID,
         TIDnum,
         date,
         week,
         any_of(names(which(sapply(dtMiS2 %>% select(where(is.numeric)), sd, na.rm = TRUE) > 0)))) %>% # only vars that sd != 0
         #which(sapply(. %>% select(where(is.numeric)), sd, na.rm = TRUE) > 0)) %>% # only include variables that vary (i.e., sd != 0)
  as.data.frame 



varNamDifS2 <- names(dtMiS2)[gsub(".pre|.post|_calc", "", names(dtMiS2)) %>% allDuplicated]

# # Variables removed for study 2 due to high multicollinearity:
# varNamS1Out <- dtMiS1 %>%
#   select(
#     any_of(varNamDifS1),
#     mdmq.calmness.pre_calc,
#     mdmq.alertness.pre_calc,
#     mdmq.valence.pre_calc,
#     relatednessFrust.pre_calc,
#     competenceFrust.pre_calc,
#     competenceSat.pre_calc,
#     relatednessSat.pre_calc,
#     DutchFriends.pre,
#     secNatDum.pre,
#     timeNL.pre,
#     stress.post,
#     discrimination_month.post,
#     rosenberg.post,
#     qualityStar,
#     starts_with("SSAS")
#   ) %>%
#   names

# dtMiS2Fin <- dtMiS2 %>%
#   # mutate(
#   #   assimilation.dif = assimilation.post - assimilation.pre,
#   #   integration.dif = integration.post - integration.pre,
#   #   marginalization.dif = marginalization.post - marginalization.pre,
#   #   separation.dif = separation.post - separation.pre,
#   #   VIA_Dutch.dif = VIA_Dutch.post - VIA_Dutch.post,
#   #   VIA_heritage.dif = VIA_heritage.post - VIA_heritage.pre,
#   #   IntGrAnx.dif = IntGrAnx.post_calc - IntGrAnx.pre_calc,
#   #   swl.dif = swl.post_calc - swl.pre_calc
#   # ) %>% 
#   # select(
#   #   -any_of(varNamS2Out)
#   # ) %>%
#   select(PID,
#          TID,
#          TIDnum,
#          date,
#          week,
#          which(sapply(., sd, na.rm = TRUE) > 0)) %>% 
#   mutate_all(~ifelse(is.nan(.), NA, .)) %>%
#   as.data.frame

# lares::corr_cross(
#   dtMiS2 %>% select_if(is.numeric), # name of dataset
#   max_pvalue = 0.05, # display only significant correlations (at 5% level)
#   top = 20 # display top 20 couples of variables (by correlation coefficient)
# )
```

```{r MiS1}
#################################################################
##                     Imputation Bi-daily                     ##
#################################################################
set.seed(123)
MiS2 <-
  amelia(
    dtMiS2,
    m = 1, # 20
    ts = "TIDnum",
    cs = "PID",
    idvars = c("TID", "date", "week", "IntergroupContact", "IngroupContact"),
    # lags = names(dtMiS1)[!names(dtMiS1) %in% idVars],
    # lags = names(dtMiS1)[names(dtMiS1) %in% varNamS1MiPsbl], # alternative?
    # leads = ...,
    # noms = names(dtMiS1)[grepl(paste(varNamdtS1Noms, collapse = "|^"), names(dtMiS1))],
    # parallel = "multicore",
    # ncpus = parallel::detectCores(),
    p2s = 1
  )
save(MiS2, file = "data/imputed/S2Imputed.RData")
load("data/imputed/S2Imputed.RData")

# # Imputation Stability:
# set.seed(123)
# MiS1Stability <-
#   amelia(
#     dtMiS1,
#     m = 20,
#     ts = "TIDnum",
#     cs = "PID",
#     idvars = c("TID", "date", "week", "IntergroupContact", "IngroupContact"),
#     # lags = names(dtMiS1)[!names(dtMiS1) %in% idVars],
#     # lags = names(dtMiS1)[names(dtMiS1) %in% varNamS1MiPsbl], # alternative?
#     # leads = ...,
#     # noms = names(dtMiS1)[grepl(paste(varNamdtS1Noms, collapse = "|^"), names(dtMiS1))],
#     # parallel = "multicore",
#     # ncpus = parallel::detectCores(),
#     p2s = 2
#   )
# save(MiS1Stability, file = "data/imputed/S1ImputedStability.RData")
load("data/imputed/S1ImputedStability.RData")
```

#### Study 3

To be done once initial tests approved.

```{r MiS3}
# BASED ON MiS1
```


### Follow-up: Time Scales

#### Daily

Preparing the data for the time scale follow-up, which combines the common variables of all three studies on a daily level. Here, we:

1.  Select the variables for all relevant analysis (i.e., imputation, 3MPCA, and validation) across all three studies.
2.  Create the combined dataset in the correct format (long format with all missed time points as NAs).
3.  Impute the data and save the imputed datasets for later recall.

```{r MiDailyPrep}
#################################################################
##                    Dataframe Preparation                    ##
#################################################################
dtMiMainDaily <- rbind(
  dtS1Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S1") %>% 
    mutate(across(!TID & !study, as.numeric)),
  dtS2Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S2"),
  dtS3Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S3")
) %>%
  group_by(study, PID) %>%
  mutate(ID = cur_group_id()) %>%
  ungroup %>%
  mutate(
    date = as.Date(gsub(" .*", "", TID)),
    week = strftime(date, format = "%Y-W%V")
  ) %>%
  rowwise() %>%
  mutate(
    InteractionDum = sum(IntergroupContact, IngroupContact, na.rm = TRUE),
    InteractionDum = if_else(InteractionDum > 0, 1, InteractionDum)
  ) %>%
  ungroup() %>%
  group_by(ID, date, study) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>% # contact dum should be summarized as sum()
  ungroup %>%
  group_by(study) %>%
  mutate(TIDnum = as.numeric(factor(date))) %>%
  ungroup %>%
  select(ID,
         study,
         date,
         everything(),
         -PID,
         -starts_with("ResponseId")) %>%
  as.data.frame %>%
  select_if(~ sum(!is.na(.)) > 1) %>% # only include variables that have any data (i.e., not all NA)
  complete(., ID, TIDnum) %>% # add na if id and TID combination is missing
  group_by(ID) %>%
  mutate(across(ends_with(c(".pre", ".post")),  ~ replace_na(.x, mean(.x, na.rm = TRUE)))) %>%
  mutate(study = replace_na(study, calc_mode(study))) %>%
  ungroup %>%
  group_by(study, TIDnum) %>%
  mutate(across(c("date"),  ~ replace_na(.x, calc_mode(.x)))) %>%
  ungroup %>% 
  mutate_if(is.numeric, ~ ifelse(is.nan(.), NA, .)) %>%
  as.data.frame 
```


```{r DailyImp}
#################################################################
##               Imputation at Daily Aggregation               ##
#################################################################
# MiMainDaily <-
#   amelia(
#     dtMiMainDaily,
#     m = 1, # 20
#     ts = "TIDnum",
#     cs = "ID",
#     idvars = c("date", "study", "IntergroupContact", "IngroupContact"), #"PID",
#     #lags = names(dtMiMainDaily)[!names(dtMiMainDaily) %in% idVars],
#     #lags = names(dtMiMainDaily)[names(dtMiMainDaily) %in%varNamInMiMainRed],
#     #noms = names(dtMiMainDaily)[grepl(paste(varNamMiMainNoms, collapse = "|^"), names(dtMiMainDaily))],
#     parallel = "multicore",
#     ncpus = parallel::detectCores(),
#     p2s = 1
#   )
# save(MiMainDaily, file = "data/imputed/MainDailyImputed.RData")
load("data/imputed/MainDailyImputed.RData")

# # Stability to imputations:
# set.seed(123)
# MiMainDailyStability <-
#   amelia(
#     dtMiMainDaily,
#     m = 20,
#     ts = "TIDnum",
#     cs = "ID",
#     idvars = c("date", "study", "IntergroupContact", "IngroupContact"), #"PID",
#     #lags = names(dtMiMainDaily)[!names(dtMiMainDaily) %in% idVars],
#     #lags = names(dtMiMainDaily)[names(dtMiMainDaily) %in%varNamInMiMainRed],
#     #noms = names(dtMiMainDaily)[grepl(paste(varNamMiMainNoms, collapse = "|^"), names(dtMiMainDaily))],
#     parallel = "multicore",
#     ncpus = parallel::detectCores(),
#     p2s = 1
#   )
# save(MiMainDailyStability, file = "data/imputed/MainDailyImputedStability.RData")
load("data/imputed/MainDailyImputedStability.RData")
```


#### Weekly

Preparing the data for the time scale follow-up, which combines the common variables of all three studies on a weekly level. Here, we:

1.  Select the variables for all relevant analysis (i.e., imputation, 3MPCA, and validation) across all three studies.
2.  Create the combined dataset in the correct format (long format with all missed time points as NAs).
3.  Impute the data and save the imputed datasets for later recall.

```{r MiWeeklyPrep}
dtMiMainWeeekly <- rbind(
  dtS1Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S1") %>% 
    mutate(across(!TID & !study, as.numeric)),
  dtS2Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S2"),
  dtS3Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S3")
) %>%
  group_by(study, PID) %>%
  mutate(ID = cur_group_id()) %>%
  ungroup %>%
  mutate(
    date = as.Date(gsub(" .*", "", TID)),
    week = strftime(date, format = "%Y-W%V")
  ) %>%
  rowwise() %>%
  mutate(
    InteractionDum = sum(IntergroupContact, IngroupContact, na.rm = TRUE),
    InteractionDum = if_else(InteractionDum > 0, 1, InteractionDum)
  ) %>%
  ungroup() %>%
  group_by(ID, week, study) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup %>%
  group_by(study) %>%
  mutate(TIDnum = as.numeric(factor(week))) %>%
  ungroup %>%
  select(ID,
         study,
         week,
         everything(),
         -PID,
         -starts_with("ResponseId")) %>%
  as.data.frame %>%
  select_if(~ sum(!is.na(.)) > 1) %>% # only include variables that have any data (i.e., not all NA)
  complete(., ID, TIDnum) %>% # add na if id and TID combination is missing
  group_by(ID) %>%
  mutate(across(ends_with(c(".pre", ".post")),  ~ replace_na(.x, mean(.x, na.rm = TRUE)))) %>%
  mutate(study = replace_na(study, calc_mode(study))) %>%
  ungroup %>%
  group_by(study, TIDnum) %>%
  mutate(across(c("week"),  ~ replace_na(.x, calc_mode(.x)))) %>%
  ungroup %>% 
  mutate_if(is.numeric, ~ ifelse(is.nan(.), NA, .)) %>%
  as.data.frame 
```


```{r WeeklyImp}
##################################################################
##               Imputation at Weekly Aggregation               ##
##################################################################
# MiMainWeekly <-
#   amelia(
#     dtMiMainWeeekly,
#     m = 1, # 20
#     ts = "TIDnum",
#     cs = "ID",
#     idvars = c("week", "study", "IntergroupContact", "IngroupContact"), #"PID",
#     #lags = names(dtMiMainWeeekly)[!names(dtMiMainWeeekly) %in% idVars],
#     #lags = names(dtMiMainWeeekly)[names(dtMiMainWeeekly) %in%varNamInMiMainRed],
#     #noms = names(dtMiMainWeeekly)[grepl(paste(varNamMiMainNoms, collapse = "|^"), names(dtMiMainWeeekly))],
#     parallel = "multicore",
#     ncpus = parallel::detectCores(),
#     p2s = 1
#   )
# save(MiMainWeekly, file = "data/imputed/MainWeeklyImputed.RData")
load("data/imputed/MainWeeklyImputed.RData")

# # Stability to imputations:
# set.seed(123)
# MiMainWeeklyStability <-
#   amelia(
#     dtMiMainWeeekly,
#     m = 20,
#     ts = "TIDnum",
#     cs = "ID",
#     idvars = c("week", "study", "IntergroupContact", "IngroupContact"), #"PID",
#     #lags = names(dtMiMainWeeekly)[!names(dtMiMainWeeekly) %in% idVars],
#     #lags = names(dtMiMainWeeekly)[names(dtMiMainWeeekly) %in%varNamInMiMainRed],
#     #noms = names(dtMiMainWeeekly)[grepl(paste(varNamMiMainNoms, collapse = "|^"), names(dtMiMainWeeekly))],
#     parallel = "multicore",
#     ncpus = parallel::detectCores(),
#     p2s = 1
#   )
# save(MiMainWeeklyStability, file = "data/imputed/MainWeeklyImputedStability.RData")
load("data/imputed/MainWeeklyImputedStability.RData")
```

### Follow-up: Response Type

#### Interaction Responses

```{r MiInteractionPrep}
#################################################################
##                    Dataframe Preparation                    ##
#################################################################

# # any interaction
# dtMiMain %>%
#   filter(is.na(InteractionDum) | InteractionDum != 0) %>%
#   #complete(., ID, TIDnum) %>% # add na if id and TID combination is missing
#   nrow
# 
# # Intergroup Contact
# dtMiMain %>%
#   filter(is.na(IntergroupContact) | IntergroupContact != 0) %>%
#   #complete(., ID, TIDnum) %>% # add na if id and TID combination is missing
#   nrow
# 
# # Ingroup Contact
# dtMiMain %>%
#   filter(is.na(IngroupContact) | IngroupContact != 0) %>%
#   #complete(., ID, TIDnum) %>% # add na if id and TID combination is missing
#   nrow

```

```{r InteractionImp}
# TBD
```

#### Non-Interaction Responses

```{r MiNoInteractionPrep}
# TBD
```

```{r NoInteractionImp}
# TBD
```


## Center and Normalize

For the 3MPCA, we center (across participants but within time point) and normalize (within variable but across time points) all key variables. The between-person centering, ensures that all variations are around the mean trend (which is removed by the centering). The normalization within variable are important for ensuring equal variances across variables, which ensures equal weighting of the variables in the 3MPCA.

```{r mainCentered}
varNam3mpcaMainAnalysis <- varNamesDailyFull %>%
  filter(!varNam %in% c(dropVarPcaMain, "PID", "TID", "TIDnum", "startDate", "startTime")) %>%
  pull

dtMainMi <- MiMain$imputations$imp1 %>%
  rowwise() %>%
  mutate(
    InteractionDumOg = sum(IntergroupContact, IngroupContact, na.rm = TRUE),
    InteractionDumOg = if_else(InteractionDum > 0, 1, InteractionDum),
    InteractionDumOg = if_else(sum(is.na(c(IntergroupContact, IngroupContact))) == 2, NA_real_, InteractionDum),
    relatednessInteraction = mean(c(relatednessSelf, relatednessOther), na.rm = TRUE),
    relatedness = if_else(InteractionDum == 1, relatednessInteraction, relatednessNoInteraction)
    #relatednessNoInteraction = if_else(is.na(InteractionDumOg), NA_real_, relatednessNoInteraction)
  ) %>%
  ungroup()

dtMain3mpcaNorm <-
  PCAnorm(data = MiMain$imputations$imp1,
          pid = "ID",
          tid = "TIDnum",
          selection = varNam3mpcaMainAnalysis)
```

## Long to Wide

For the Three-way ANOVA and the Three-mode PCA, the datasets need to be in a $n * mp$ matrix format, where $n$ is are the participants, $m$ are the time points, and $p$ are the variables. We do this by taking the prepared datasets, melting them into a id-variable-value table and casting the table into a wide format of variables_timepoint columns.

### Main Analysis 

```{r longToWideMain}
dtPcaS123 <- rbind(
  dtS1Red %>% select(any_of(varNamS123PCA)) %>% mutate(study = "S1"),
  dtS2Red %>% select(any_of(varNamS123PCA)) %>% mutate(study = "S2"),
  dtS3Red %>% select(any_of(varNamS123PCA)) %>% mutate(study = "S3")
  ) %>%
  group_by(study, PID) %>%
  mutate(ID = cur_group_id()) %>%
  ungroup %>%
  mutate(
    date = as.Date(gsub(" .*", "", TID)),
    week = strftime(date, format = "%Y-W%V")
    ) %>%
  select(
    ID,
    PID,
    TID,
    date,
    week,
    TIDnum,
    study,
    everything()
  ) %>%
  arrange(ID, TIDnum) %>%
  filter(TIDnum <= min(max(dtS1Red$TIDnum), max(dtS2Red$TIDnum), max(dtS3Red$TIDnum)))
# 
# dtMainWideC <- PCAnorm(data = dtPcaS123,
#           pid = "ID",
#           tid = "TIDnum",
#           selection = names(dtPcaS123)[!names(dtPcaS123) %in% idVars]) %>%
#   select(
#     ID,
#     PID,
#     TIDnum,
#     study,
#     ends_with("_gmc")
#   ) %>%
#   data.frame %>% 
#   melt(
#     .,
#     id=c("ID", "PID", "TIDnum", "study")
#   ) %>%
#   mutate(
#     variable = paste(gsub("_gmc", "", variable), stringr::str_pad(TIDnum, width = 2, pad = 0), sep = "_")
#   ) %>%
#   arrange(variable, ID) %>%
#   #arrange(ID, variable, TIDnum) %>%
#   select(-TIDnum) %>%
#   pivot_wider(names_from = variable, values_from = value) %>%
#   replace(is.na(.), 0) %>%
#   select(-ID, -PID, -study)
#   #reshape(., idvar = "ID", timevar = "variable", direction = "wide") %>% View
#   #pivot_wider(names_from = variable, values_from = value)
# 
# # dtMainWideC %>%
# #   filter(
# #     ID == 54
# #   ) %>% 
# #   t
# # 
# # dtMainWideC$AttitudesDutch_59[dtMainWideC$ID == 54]
# 
# dtMainWide <- dtPcaS123 %>%
#   select(
#     -TID,
#     -date,
#     -week
#   ) %>%
#   data.frame %>% 
#   melt(
#     .,
#     id=c("ID", "PID", "TIDnum", "study")
#   ) %>%
#   mutate(
#     variable = paste(variable, stringr::str_pad(TIDnum, width = 2, pad = 0), sep = "_")
#   ) %>%
#   select(-TIDnum) %>%
#   arrange(variable, ID) %>%
#   pivot_wider(names_from = variable, values_from = value) %>%
#   replace(is.na(.), 0) %>%
#   select(-ID, -PID, -study)

dtPcaS123Imp <- MiMain$imputations[[1]] %>% 
  select(ID, date, week, study, any_of(varNamS123PCA)) %>%
  arrange(ID, TIDnum) %>%
  filter(TIDnum <= min(max(dtS1Red$TIDnum), max(dtS2Red$TIDnum), max(dtS3Red$TIDnum)))

dtMainImpWide <- MiMain$imputations[[1]] %>% 
  select(ID, date, week, study, any_of(varNamS123PCA)) %>%
  arrange(ID, TIDnum) %>%
  filter(TIDnum <= min(max(dtS1Red$TIDnum), max(dtS2Red$TIDnum), max(dtS3Red$TIDnum))) %>%
  select(
    -TID,
    -date,
    -week
  ) %>%
  melt(
    .,
    id=c("ID", "TIDnum", "study")
  ) %>%
  mutate(
    variable = paste("T", stringr::str_pad(TIDnum, width = 2, pad = 0), "_", variable, sep = "")
  ) %>%
  select(-TIDnum, -study) %>%
  arrange(ID, variable) %>%
  pivot_wider(names_from = variable, values_from = value) %>%
  #replace(is.na(.), 0) %>%
  select(-ID)
```

```{r longToWideMainStability}
# Long to Wide Main Bi-daily
MiMainStabilityWide <- list()
for (i in 1:length(MiMainStability$imputations)) {
  MiMainStabilityWide[[i]] <-
    MiMainStability$imputations[[i]]  %>%
    # Variable selection
    select(ID, date, week, study, any_of(varNamS123PCA)) %>%
    # Sorting
    arrange(ID, TIDnum) %>%
    # Filter minimum common num of measurements
    filter(TIDnum <= min(
      max(dtS1Red$TIDnum), max(dtS2Red$TIDnum), max(dtS3Red$TIDnum)
    )) %>%
    # Rm unnecessary variables for restructuring
    select(-TID,
           -date,
           -week) %>%
    # Long to wide
    melt(.,
         id = c("ID", "TIDnum", "study")) %>%
    mutate(variable = paste(
      "T",
      stringr::str_pad(TIDnum, width = 2, pad = 0),
      "_",
      variable,
      sep = ""
    )) %>%
    select(-TIDnum, -study) %>%
    arrange(ID, variable) %>%
    pivot_wider(names_from = variable, values_from = value) %>%
    #replace(is.na(.), 0) %>%
    select(-ID)
}
```

### Follow-up: Studies

#### Study 1

##### Common Variables

```{r longToWideS1Common}
dtS1ImpWide <- MiS1$imputations[[1]] %>% 
  select(PID, date, week, any_of(varNamS123PCA)) %>%
  arrange(PID, TIDnum) %>%
  #filter(TIDnum <= min(max(dtS1Red$TIDnum), max(dtS2Red$TIDnum), max(dtS3Red$TIDnum))) %>%
  select(
    -TID,
    -date,
    -week
  ) %>%
  melt(
    .,
    id=c("PID", "TIDnum")
  ) %>%
  mutate(
    variable = paste("T", stringr::str_pad(TIDnum, width = 2, pad = 0), "_", variable, sep = "")
  ) %>%
  select(-TIDnum) %>%
  arrange(PID, variable) %>%
  pivot_wider(names_from = variable, values_from = value) %>%
  #replace(is.na(.), 0) %>%
  select(-PID)
```

```{r longToWideS1StabilityCommon}
# Long to Wide S1 Bi-daily
MiS1StabilityWide <- list()
for (i in 1:length(MiS1Stability$imputations)) {
  MiS1StabilityWide[[i]] <-
    MiS1Stability$imputations[[i]]  %>%
    # Variable selection
    select(PID, date, week, any_of(varNamS123PCA)) %>%
    # Sorting
    arrange(PID, TIDnum) %>%
    # Rm unnecessary variables for restructuring
    select(-TID,
           -date,
           -week) %>%
    # Long to wide
    melt(.,
         id = c("PID", "TIDnum")) %>%
    mutate(variable = paste(
      "T",
      stringr::str_pad(TIDnum, width = 2, pad = 0),
      "_",
      variable,
      sep = ""
    )) %>%
    select(-TIDnum) %>%
    arrange(PID, variable) %>%
    pivot_wider(names_from = variable, values_from = value) %>%
    #replace(is.na(.), 0) %>%
    select(-PID)
}
```


##### Idiosyncratic Variables

```{r longToWideS1Idiosyncratic}
dtS1ImpWideIdio <- MiS1$imputations[[1]] %>% 
  select(PID, date, week, any_of(varNamS1PCA)) %>%
  arrange(PID, TIDnum) %>%
  #filter(TIDnum <= min(max(dtS1Red$TIDnum), max(dtS2Red$TIDnum), max(dtS3Red$TIDnum))) %>%
  select(
    -TID,
    -date,
    -week
  ) %>%
  melt(
    .,
    id=c("PID", "TIDnum")
  ) %>%
  mutate(
    variable = paste("T", stringr::str_pad(TIDnum, width = 2, pad = 0), "_", variable, sep = "")
  ) %>%
  select(-TIDnum) %>%
  arrange(PID, variable) %>%
  pivot_wider(names_from = variable, values_from = value) %>%
  #replace(is.na(.), 0) %>%
  select(-PID)
```

```{r longToWideS1StabilityIdiosyncratic}
# Long to Wide S1 Bi-daily
MiS1StabilityWideIdio <- list()
for (i in 1:length(MiS1Stability$imputations)) {
  MiS1StabilityWideIdio[[i]] <-
    MiS1Stability$imputations[[i]]  %>%
    # Variable selection
    select(PID, date, week, any_of(varNamS1PCA)) %>%
    # Sorting
    arrange(PID, TIDnum) %>%
    # Rm unnecessary variables for restructuring
    select(-TID,
           -date,
           -week) %>%
    # Long to wide
    melt(.,
         id = c("PID", "TIDnum")) %>%
    mutate(variable = paste(
      "T",
      stringr::str_pad(TIDnum, width = 2, pad = 0),
      "_",
      variable,
      sep = ""
    )) %>%
    select(-TIDnum) %>%
    arrange(PID, variable) %>%
    pivot_wider(names_from = variable, values_from = value) %>%
    #replace(is.na(.), 0) %>%
    select(-PID)
}
```


#### Study 2

#### Study 3

### Follow-up: Response Type

#### Interaction Responses

#### Non-Interaction Responses

### Follow-up: Time Scales

#### Daily

```{r longToWideDaily}
dtMainDailyImpWide <- MiMainDaily$imputations[[1]] %>%
  select(ID, date, study, any_of(varNamS123PCA)) %>%
  arrange(ID, TIDnum) %>%
  filter(
    TIDnum <= dtMiMainDaily %>% group_by(study) %>% summarise(TIDmax = max(TIDnum)) %>% select(TIDmax) %>% min
  ) %>%
  select(
    -date
  ) %>% 
  melt(
    .,
    id=c("ID", "TIDnum", "study")
  ) %>%
  mutate(
    variable = paste("T", stringr::str_pad(TIDnum, width = 2, pad = 0), "_", variable, sep = "")
  ) %>%
  select(-TIDnum, -study) %>%
  arrange(ID, variable) %>%
  pivot_wider(names_from = variable, values_from = value) %>%
  #replace(is.na(.), 0) %>%
  select(-ID)
```

```{r longToWideDailyStability}
# Long to Wide Main Daily
MiMainDailyStabilityWide <- list()
for (i in 1:length(MiMainDailyStability$imputations)) {
  MiMainDailyStabilityWide[[i]] <-
    MiMainDailyStability$imputations[[i]] %>%
    # Variable selection
    select(ID, date, study, any_of(varNamS123PCA)) %>%
    # Sorting
    arrange(ID, TIDnum) %>%
    # Filter minimum common num of measurements
    filter(
      TIDnum <= dtMiMainDaily %>% group_by(study) %>% summarise(TIDmax = max(TIDnum)) %>% select(TIDmax) %>% min
    ) %>%
    # Rm unnecessary variables for restructuring
    select(-date) %>%
    # Long to wide
    melt(.,
         id = c("ID", "TIDnum", "study")) %>%
    mutate(variable = paste(
      "T",
      stringr::str_pad(TIDnum, width = 2, pad = 0),
      "_",
      variable,
      sep = ""
    )) %>%
    select(-TIDnum, -study) %>%
    arrange(ID, variable) %>%
    pivot_wider(names_from = variable, values_from = value) %>%
    #replace(is.na(.), 0) %>%
    select(-ID)
}
```


#### Weekly

```{r longToWideWeekly}
dtMainWeeklyImpWide <- MiMainWeekly$imputations[[1]] %>%
  select(ID, week, study, any_of(varNamS123PCA)) %>%
  arrange(ID, TIDnum) %>%
  filter(
    TIDnum <= dtMiMainWeeekly %>% group_by(study) %>% summarise(TIDmax = max(TIDnum)) %>% select(TIDmax) %>% min
  ) %>%
  select(
    -week
  ) %>% 
  melt(
    .,
    id=c("ID", "TIDnum", "study")
  ) %>%
  mutate(
    variable = paste("T", stringr::str_pad(TIDnum, width = 2, pad = 0), "_", variable, sep = "")
  ) %>%
  select(-TIDnum, -study) %>%
  arrange(ID, variable) %>%
  pivot_wider(names_from = variable, values_from = value) %>%
  #replace(is.na(.), 0) %>%
  select(-ID)
```

```{r longToWideWeeklyStability}
# Long to Wide Main Weekly
MiMainWeeklyStabilityWide <- list()
for (i in 1:length(MiMainWeeklyStability$imputations)) {
  MiMainWeeklyStabilityWide[[i]] <-
    MiMainWeeklyStability$imputations[[i]] %>%
    # Variable selection
    select(ID, week, study, any_of(varNamS123PCA)) %>%
    # Sorting
    arrange(ID, TIDnum) %>%
    # Filter minimum common num of measurements
    filter(
      TIDnum <= dtMiMainWeeekly %>% group_by(study) %>% summarise(TIDmax = max(TIDnum)) %>% select(TIDmax) %>% min
    ) %>%
    # Rm unnecessary variables for restructuring
    select(-week) %>%
    # Long to wide
    melt(.,
         id = c("ID", "TIDnum", "study")) %>%
    mutate(variable = paste(
      "T",
      stringr::str_pad(TIDnum, width = 2, pad = 0),
      "_",
      variable,
      sep = ""
    )) %>%
    select(-TIDnum, -study) %>%
    arrange(ID, variable) %>%
    pivot_wider(names_from = variable, values_from = value) %>%
    #replace(is.na(.), 0) %>%
    select(-ID)
}
```


# **Three-Way ANOVA**

We assess the percentages of explained variance for the person-, variable-, and time aspects --- which offers an indication of whether a 3MPCA is useful for the dataset. Based on the grand mean centered variables we conduct a fixed-effects three-way ANOVA of the person-, variable-, and time modes as well as their various interactions. We are particularly interested in the highest order interaction term Person \* Variable \* Time + error, as a large amount of variance in this effect would speak towards the possible interdependence of the three modes.

```{r threeWayAnovaMainBiDaily, message=FALSE, warning=FALSE}
capture.output(
  threeWayAnoBiDailyImp <- threewayanova(
    Y = dtMainImpWide,
    n = nrow(dtMainImpWide), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(dtMainImpWide)))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(dtMainImpWide)))) # Nr. time points (C)
  ), 
file='NUL')

options(knitr.kable.NA = '')
data.frame(SS = sapply(threeWayAnoBiDailyImp, c)) %>%
  rownames_to_column(., var = "EffectNam") %>%
  mutate(RSqrd = SS / sum(SS) * 100) %>%
  add_row(
    EffectNam = "Total", 
    SS = sum(.$SS),
    RSqrd = NA
  ) %>%
  mutate(
    Effect = c(
      "Person",
      "Variable",
      "Time",
      "Person*Variable",
      "Person*Time",
      "Variable*Time",
      "Person*Variable*Time+error",
      "Total"
    )
  ) %>%
  select(Effect, everything(), -EffectNam) %>%
  kbl(.,
      escape = FALSE,
      booktabs = T,
      align = c("l", "r", "c"),
      digits=2,
      col.names = c(
        "Effect",
        "Sum of Squares",
        "$R^2$"
      ),
      caption = "Three-way ANOVA Main Analysis Bi-daily (Single Imputation)") %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

<br>

We find that for the bi-daily time scale the $Person*Time*Variable+error$ sums of squares are indeed the largest and speak towards the utility of a Three-Mode PCA. Interestingly, time itself has a relatively small percentage of explained variance and only becomes important in compination with person- or person- and variable-specific differences.

```{r threeWayAnovaMainDaily, message=FALSE, warning=FALSE}
capture.output(
  threeWayAnoDailyImp <- threewayanova(
    Y = dtMainDailyImpWide,
    n = nrow(dtMainDailyImpWide), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(dtMainDailyImpWide)))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(dtMainDailyImpWide)))) # Nr. time points (C)
  ), 
file='NUL')

options(knitr.kable.NA = '')
data.frame(SS = sapply(threeWayAnoDailyImp, c)) %>%
  rownames_to_column(., var = "EffectNam") %>%
  mutate(RSqrd = SS / sum(SS) * 100) %>%
  add_row(
    EffectNam = "Total", 
    SS = sum(.$SS),
    RSqrd = NA
  ) %>%
  mutate(
    Effect = c(
      "Person",
      "Variable",
      "Time",
      "Person*Variable",
      "Person*Time",
      "Variable*Time",
      "Person*Variable*Time+error",
      "Total"
    )
  ) %>%
  select(Effect, everything(), -EffectNam) %>%
  kbl(.,
      escape = FALSE,
      booktabs = T,
      align = c("l", "r", "c"),
      digits=2,
      col.names = c(
        "Effect",
        "Sum of Squares",
        "$R^2$"
      ),
      caption = "Three-way ANOVA Main Analysis Daily (Single Imputation)") %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

<br>

We additionally see that for the daily time scale aggregation, the $Person*Time*Variable+error$ sums of squares are reduced but remain the largest sums of squares and are still substantial.

```{r threeWayAnovaMainWweekly, message=FALSE, warning=FALSE}
capture.output(
  threeWayAnoWeeklyImp <- threewayanova(
    Y = dtMainWeeklyImpWide,
    n = nrow(dtMainWeeklyImpWide), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(dtMainWeeklyImpWide)))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(dtMainWeeklyImpWide)))) # Nr. time points (C)
  ), 
file='NUL')

options(knitr.kable.NA = '')
data.frame(SS = sapply(threeWayAnoWeeklyImp, c)) %>%
  rownames_to_column(., var = "EffectNam") %>%
  mutate(RSqrd = SS / sum(SS) * 100) %>%
  add_row(
    EffectNam = "Total", 
    SS = sum(.$SS),
    RSqrd = NA
  ) %>%
  mutate(
    Effect = c(
      "Person",
      "Variable",
      "Time",
      "Person*Variable",
      "Person*Time",
      "Variable*Time",
      "Person*Variable*Time+error",
      "Total"
    )
  ) %>%
  select(Effect, everything(), -EffectNam) %>%
  kbl(.,
      escape = FALSE,
      booktabs = T,
      align = c("l", "r", "c"),
      digits=2,
      col.names = c(
        "Effect",
        "Sum of Squares",
        "$R^2$"
      ),
      caption = "Three-way ANOVA Main Analysis Weekly (Single Imputation)") %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

<br>

However, with the aggregation to `r length(unique(sub("_.*", "", names(dtMainWeeklyImpWide))))` weekly measurements, the $Person*Time*Variable+error$ sums of squares become less prominent and the variable differences explain substantially more variance.

# **Plots per Item** {.tabset .tabset-fade}

The three-mode PCA ultimately aims to explain the most variance with the fewest components (by maximizing the variance of data mapped in a lower dimensional space --- often by maximally separating groups of cases with similar variable patterns). Importantly, this procedure depends on variances and covariances to be present within the data. As one of our three modes is time and variance over time is not always given (e.g., because a variable is stable over the measured time span, such as personality over a month), it becomes important to assess whether a dimension reduction is possible for the variables measured during the measured time period. We, thus, start by visually inspecting the average changes and variance developments of all potentially relevant variables. We hope so see changes within the means and standard deviations over the measurement periods.

Please note that the items might be measured on different ranges. To avoid misinterpretation of variances and changes over time, we cluster variables with similar ranges in different plots where necessary.

## Means

To illustrate the developments of the means,we plot the item means of all variables. We plot the means of the individual variables for each measurement occasion, summarizing the responses of all participants at any given time point. The displayed time grouping is based on the measurement index rather than the actual date to ensure comparability of the changes between each measurement as well as studies. We plot the means for bi-daily, daily, and weekly timescales.

### Studies 1-3 (Means) {.unlisted .unnumbered}

We begin by assessing the imputed dataset of the main analysis, where participants from all three studies are combined, using the relevant variables that are available for all three studies.

```{r plotAllMean}
varClS123 <- dtPcaS123Imp %>%
  PCAnorm(data = .,
          pid = "ID",
          tid = "TIDnum",
          selection = names(.)[!names(.) %in% idVars]) %>%
  select(
    ends_with("_gmc")
  ) %>%
  data.frame %>%
  summarise_all(
    sd, na.rm = TRUE
  ) %>%
  t %>% 
  as.data.frame %>%
  kmeans(., centers = 2)

PCAnorm(data = dtPcaS123Imp,
          pid = "ID",
          tid = "TIDnum",
          selection = names(dtPcaS123)[!names(dtPcaS123) %in% idVars]) %>% 
  select(
    ID,
    TIDnum,
    ends_with("_gmc")
  ) %>%
  data.frame %>%
  melt(
    .,
    id=c("ID","TIDnum")
  ) %>%
  mutate(
    variable = gsub("_gmc", "", variable)
  ) %>%
  ggplot(., aes(x = variable, y = value, group = TIDnum, color = TIDnum)) +
  #geom_jitter() +
  stat_summary(fun=mean, geom="line") +
  labs(
    title = "Studies 1-3 Means over time [bi-daily]",
    y = "Grand Mean Centered Average",
    x = "Variable",
    color = "Timepoint",
    group = "Timepoint"
  ) +
  #coord_flip() +
  theme_Publication() +
  theme(
    axis.text.x = element_text(angle = 45, hjust=1),
    legend.key.size = unit(1, 'cm'),
    legend.key.height = unit(0.4, 'cm'),
  )

dtPcaS123 %>%
  group_by(ID, date, study) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup %>%
  group_by(study) %>%
  mutate(TIDnum = as.numeric(factor(date))) %>%
  ungroup %>%
  select(ID, date, TIDnum, everything()) %>%
  mutate_all(~ifelse(is.nan(.), NA, .)) %>%
  PCAnorm(data = .,
          pid = "ID",
          tid = "TIDnum",
          selection = names(.)[!names(.) %in% idVars]) %>% 
  select(
    ID,
    TIDnum,
    ends_with("_gmc")
  ) %>%
  data.frame %>%
  melt(
    .,
    id=c("ID","TIDnum")
  ) %>%
  mutate(
    variable = gsub("_gmc", "", variable)
  ) %>%
  ggplot(., aes(x = variable, y = value, group = TIDnum, color = TIDnum)) +
  #geom_jitter() +
  stat_summary(fun=mean, geom="line") +
  labs(
    title = "Studies 1-3 Means over time [daily]",
    y = "Grand Mean Centered Average",
    x = "Variable",
    color = "Timepoint",
    group = "Timepoint"
  ) +
  #coord_flip() +
  theme_Publication() +
  theme(
    axis.text.x = element_text(angle = 45, hjust=1),
    legend.key.size = unit(1, 'cm'),
    legend.key.height = unit(0.4, 'cm'),
  )

dtPcaS123 %>%
  group_by(ID, week, study) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup %>%
  group_by(study) %>%
  mutate(TIDnum = as.numeric(factor(week))) %>%
  ungroup %>%
  select(ID, week, TIDnum, everything()) %>%
  mutate_all(~ifelse(is.nan(.), NA, .)) %>%
  PCAnorm(data = .,
          pid = "ID",
          tid = "TIDnum",
          selection = names(.)[!names(.) %in% idVars]) %>% 
  select(
    ID,
    TIDnum,
    ends_with("_gmc")
  ) %>%
  data.frame %>%
  melt(
    .,
    id=c("ID","TIDnum")
  ) %>%
  mutate(
    variable = gsub("_gmc", "", variable)
  ) %>%
  ggplot(., aes(x = variable, y = value, group = TIDnum, color = TIDnum)) +
  #geom_jitter() +
  stat_summary(fun=mean, geom="line") +
  labs(
    title = "Studies 1-3 Means over time [weekly]",
    y = "Grand Mean Centered Average",
    x = "Variable",
    color = "Timepoint",
    group = "Timepoint"
  ) +
  #coord_flip() +
  theme_Publication() +
  theme(
    axis.text.x = element_text(angle = 45, hjust=1),
    legend.key.size = unit(1, 'cm'),
    legend.key.height = unit(0.4, 'cm'),
  )
```

### Study 1 (Means) {.unlisted .unnumbered}

The first study, utilized not only the smallest sample but also a slightly different set of measured variables compared to the later two studies. Several variables are consistent across all three studies (e.g., types of social interactions, interaction needs, self-determiantion theory needs, outgroup attitudes, and experienced well-being). Study 1 additionally included several emotion and mood measurements but fewer cognitive and behavioral measures than studies two and three.

```{r plotS1Mean, fig.height=16}
plotStat(
  dt = dtS1Red,
  varNams = varNamS1PCA,
  id = "PID",
  timescale = "bi-daily",
  clNum = 2,
  idVars = idVars,
  stat = "mean",
  title = "Study 1 Means over time [bi-daily]"
)

plotStat(
  dt = dtS1Red,
  varNams = varNamS1PCA,
  id = "PID",
  timescale = "daily",
  clNum = 2,
  idVars = idVars,
  stat = "mean",
  title = "Study 1 Means over time [daily]"
)

plotStat(
  dt = dtS1Red,
  varNams = varNamS1PCA,
  id = "PID",
  timescale = "weekly",
  clNum = 2,
  idVars = idVars,
  stat = "mean",
  title = "Study 1 Means over time [weekly]"
)
```

### Study 2 (Means) {.unlisted .unnumbered}

During the second study we collected the largest sample and additionally collected a range of motivations as well as pro-social and anti-social behaviors.

We, again, select all potentially relevant variables and their labels, and then plot their developments on a bi-daily, daily, and weekly level.

We again begin by plotting the item means of all variables. The plotting method is identical to that of Study 1.

```{r plotS2Mean, fig.height=16}
plotStat(
  dt = dtS2Red,
  varNams = varNamS2PCA,
  id = "PID",
  timescale = "bi-daily",
  clNum = 2,
  idVars = idVars,
  stat = "mean",
  title = "Study 2 Means over time [bi-daily]"
)

plotStat(
  dt = dtS2Red,
  varNams = varNamS2PCA,
  id = "PID",
  timescale = "daily",
  clNum = 2,
  idVars = idVars,
  stat = "mean",
  title = "Study 2 Means over time [daily]"
)

plotStat(
  dt = dtS2Red,
  varNams = varNamS2PCA,
  id = "PID",
  timescale = "weekly",
  clNum = 2,
  idVars = idVars,
  stat = "mean",
  title = "Study 2 Means over time [weekly]"
)
```

### Study 3 (Means) {.unlisted .unnumbered}

The third study uses a similar set up as Study 2 but targets a more vulnerable sample (of young medical professionals). While most key variables are identical to Study 2, in this last study we additionally collected several cognitive evaluations (e.g., importance ratings) and emotional status measures (e.g., anger, nervousness, energy, and loneliness).

We, again, select all potentially relevant variables and their labels, and then plot their developments on a bi-daily, daily, and weekly level.

As with the previous studies, we begin by plotting the item means of all variables. The plotting method is identical to that of Studies 1 and 2.

```{r plotS3Mean, fig.height=16}
plotStat(
  dt = dtS3Red,
  varNams = varNamS3PCA,
  id = "PID",
  timescale = "bi-daily",
  clNum = 2,
  idVars = idVars,
  stat = "mean",
  title = "Study 3 Means over time [bi-daily]"
)

plotStat(
  dt = dtS3Red,
  varNams = varNamS3PCA,
  id = "PID",
  timescale = "daily",
  clNum = 2,
  idVars = idVars,
  stat = "mean",
  title = "Study 3 Means over time [daily]"
)

plotStat(
  dt = dtS3Red,
  varNams = varNamS3PCA,
  id = "PID",
  timescale = "weekly",
  clNum = 2,
  idVars = idVars,
  stat = "mean",
  title = "Study 3 Means over time [weekly]"
)
```

## SD

To illustrate the developments of variance around the means, we also plot the item standard deviations of all variables. We, thus, plot the average variation from the mean for each individual variable, again summarizing the responses of all participants at any given time point. We retain the same measurement indices as the x axis.

### Studies 1-3 (SDs) {.unlisted .unnumbered}

<!-- <font size="5">Studies 1-3 (SDs)</font>  -->

```{r plotAllSd}
PCAnorm(data = dtPcaS123,
          pid = "ID",
          tid = "TIDnum",
          selection = names(dtPcaS123)[!names(dtPcaS123) %in% idVars]) %>% 
  select(
    ID,
    TIDnum,
    ends_with("_gmc")
  ) %>%
  data.frame %>%
  melt(
    .,
    id=c("ID","TIDnum")
  ) %>%
  mutate(
    variable = gsub("_gmc", "", variable)
  ) %>%
  ggplot(., aes(x = variable, y = value, group = TIDnum, color = TIDnum)) +
  #geom_jitter() +
  stat_summary(fun=sd, geom="line") +
  labs(
    title = "Studies 1-3 SDs over time [bi-daily]",
    y = "Standard Deviation",
    x = "Variable",
    color = "Timepoint",
    group = "Timepoint"
  ) +
  #coord_flip() +
  theme_Publication() +
  theme(
    axis.text.x = element_text(angle = 45, hjust=1),
    legend.key.size = unit(1, 'cm'),
    legend.key.height = unit(0.4, 'cm'),
  )

dtPcaS123 %>%
  group_by(ID, date, study) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup %>%
  group_by(study) %>%
  mutate(TIDnum = as.numeric(factor(date))) %>%
  ungroup %>%
  select(ID, date, TIDnum, everything()) %>%
  mutate_all(~ifelse(is.nan(.), NA, .)) %>%
  PCAnorm(data = .,
          pid = "ID",
          tid = "TIDnum",
          selection = names(.)[!names(.) %in% idVars]) %>% 
  select(
    ID,
    TIDnum,
    ends_with("_gmc")
  ) %>%
  data.frame %>%
  melt(
    .,
    id=c("ID","TIDnum")
  ) %>%
  mutate(
    variable = gsub("_gmc", "", variable)
  ) %>%
  ggplot(., aes(x = variable, y = value, group = TIDnum, color = TIDnum)) +
  #geom_jitter() +
  stat_summary(fun=sd, geom="line") +
  labs(
    title = "Studies 1-3 SDs over time [daily]",
    y = "Standard Deviation",
    x = "Variable",
    color = "Timepoint",
    group = "Timepoint"
  ) +
  #coord_flip() +
  theme_Publication() +
  theme(
    axis.text.x = element_text(angle = 45, hjust=1),
    legend.key.size = unit(1, 'cm'),
    legend.key.height = unit(0.4, 'cm'),
  )

dtPcaS123 %>%
  group_by(ID, week, study) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup %>%
  group_by(study) %>%
  mutate(TIDnum = as.numeric(factor(week))) %>%
  ungroup %>%
  select(ID, week, TIDnum, everything()) %>%
  mutate_all(~ifelse(is.nan(.), NA, .)) %>%
  PCAnorm(data = .,
          pid = "ID",
          tid = "TIDnum",
          selection = names(.)[!names(.) %in% idVars]) %>% 
  select(
    ID,
    TIDnum,
    ends_with("_gmc")
  ) %>%
  data.frame %>%
  melt(
    .,
    id=c("ID","TIDnum")
  ) %>%
  mutate(
    variable = gsub("_gmc", "", variable)
  ) %>%
  ggplot(., aes(x = variable, y = value, group = TIDnum, color = TIDnum)) +
  #geom_jitter() +
  stat_summary(fun=sd, geom="line") +
  labs(
    title = "Studies 1-3 SDs over time [weekly]",
    y = "Standard Deviation",
    x = "Variable",
    color = "Timepoint",
    group = "Timepoint"
  ) +
  #coord_flip() +
  theme_Publication() +
  theme(
    axis.text.x = element_text(angle = 45, hjust=1),
    legend.key.size = unit(1, 'cm'),
    legend.key.height = unit(0.4, 'cm'),
  )
```

### Study 1 (SDs) {.unlisted .unnumbered}

```{r plotS1Sd, fig.height=16}
plotStat(
  dt = dtS1Red,
  varNams = varNamS1PCA,
  id = "PID",
  timescale = "bi-daily",
  clNum = 2,
  idVars = idVars,
  stat = "sd",
  title = "Study 1 SDs over time [bi-daily]"
)

plotStat(
  dt = dtS1Red,
  varNams = varNamS1PCA,
  id = "PID",
  timescale = "daily",
  clNum = 2,
  idVars = idVars,
  stat = "sd",
  title = "Study 1 SDs over time [daily]"
)

plotStat(
  dt = dtS1Red,
  varNams = varNamS1PCA,
  id = "PID",
  timescale = "weekly",
  clNum = 2,
  idVars = idVars,
  stat = "sd",
  title = "Study 1 SDs over time [weekly]"
)
```

### Study 2 (SDs) {.unlisted .unnumbered}

```{r plotS2Sd, fig.height=16}
plotStat(
  dt = dtS2Red,
  varNams = varNamS2PCA,
  id = "PID",
  timescale = "bi-daily",
  clNum = 2,
  idVars = idVars,
  stat = "sd",
  title = "Study 2 SDs over time [bi-daily]"
)

plotStat(
  dt = dtS2Red,
  varNams = varNamS2PCA,
  id = "PID",
  timescale = "daily",
  clNum = 2,
  idVars = idVars,
  stat = "sd",
  title = "Study 2 SDs over time [daily]"
)

plotStat(
  dt = dtS2Red,
  varNams = varNamS2PCA,
  id = "PID",
  timescale = "weekly",
  clNum = 2,
  idVars = idVars,
  stat = "sd",
  title = "Study 2 SDs over time [weekly]"
)
```

### Study 3 (SDs) {.unlisted .unnumbered}

```{r plotS3Sd, fig.height=16}
plotStat(
  dt = dtS3Red,
  varNams = varNamS3PCA,
  id = "PID",
  timescale = "bi-daily",
  clNum = 2,
  idVars = idVars,
  stat = "sd",
  title = "Study 3 SDs over time [bi-daily]"
)

plotStat(
  dt = dtS3Red,
  varNams = varNamS3PCA,
  id = "PID",
  timescale = "daily",
  clNum = 2,
  idVars = idVars,
  stat = "sd",
  title = "Study 3 SDs over time [daily]"
)

plotStat(
  dt = dtS3Red,
  varNams = varNamS3PCA,
  id = "PID",
  timescale = "weekly",
  clNum = 2,
  idVars = idVars,
  stat = "sd",
  title = "Study 3 SDs over time [weekly]"
)
```

</br>

------------------------------------------------------------------------

</br>

# **Three-Mode PCA**

## Stability Test

### Main Analysis

- all three studies combined
- variables shared across studies
- interaction and no interaction responses
- bi-daily time scale

#### Center and Normalize

```{r MainStabCZ}
##################################################################
##                  Center across Participants                  ##
##################################################################
MiMainStabilityWideC <- list()
for (i in 1:length(MiMainStabilityWide)) {
  MiMainStabilityWideC[[i]] <-
    ThreeWay::cent3(
      X = MiMainStabilityWide[[i]],
      # Nr. ppt.s (A)
      n = nrow(MiMainStabilityWide[[i]]),
      # Nr. variables (B)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiMainStabilityWide[[i]])))),
      # Nr. time points (C)
      p = length(unique(sub("_.*", "", names(MiMainStabilityWide[[i]])))),
      # Center across Participants
      mode = 1
    ) %>%
    as.data.frame
}

##################################################################
##                 Normalize across Time Points                 ##
##################################################################
MiMainStabilityWideCZ <- list()
for (i in 1:length(MiMainStabilityWideC)) {
  MiMainStabilityWideCZ[[i]] <-
    ThreeWay::norm3(
      X = MiMainStabilityWideC[[i]], 
      # Nr. ppt.s (A)
      n = nrow(MiMainStabilityWideC[[i]]), 
      # Nr. variables (B)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiMainStabilityWideC[[i]])))),
      # Nr. time points (C)
      p = length(unique(sub("_.*", "", names(MiMainStabilityWideC[[i]])))),
      # Normalize across Time Points
      mode = 2
    ) %>%
    as.data.frame
  names(MiMainStabilityWideCZ[[i]]) <- names(MiMainStabilityWideC[[i]])
}
```

#### Component Complexity

```{r MainStabCompNum}
# Run PCA SUP to determine best combination
MainStabPcaSup <- list()
capture.output(
  for(i in 1:length(MiMainStabilityWideCZ)) {
    MainStabPcaSup[[i]] <- 
    ThreeWay::T3runsApproxFit(
    X = MiMainStabilityWideCZ[[i]], 
    n = nrow(MiMainStabilityWideCZ[[i]]), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(MiMainStabilityWideCZ[[i]])))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(MiMainStabilityWideCZ[[i]])))), # Nr. time points (C)
    maxa = 20,
    maxb = 20,
    maxc = 20
  )
  }, 
file='NUL')

# Select Dimension Complexity
MainStabDimSelector <- list()
for (i in 1:length(MainStabPcaSup)) {
  MainStabDimSelector[[i]] <- capture.output(
    ThreeWay::DimSelector(
      out = MainStabPcaSup[[i]],
      n = nrow(MiMainStabilityWideCZ[[i]]), # Nr. ppt.s (A)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiMainStabilityWideCZ[[i]])))), # Nr. variables (B)
      p = length(unique(sub("_.*", "", names(MiMainStabilityWideCZ[[i]])))), # Nr. time points (C)
      model = 2
    )
  )
}

MainStabDim <- list()
for (i in 1:length(MainStabDimSelector)) {
  MainStabDim[[i]] <-
    tail(MainStabDimSelector[[i]], n = 1) %>%
    str_extract("\\(.*\\)") %>%
    stringr::str_extract_all('\\d+', simplify = TRUE) %>%
    as.data.frame %>%
    mutate_all(as.numeric) %>%
    dplyr::rename(c(A = V1,
                    B = V2,
                    C = V3))
}

MainStabDim %>%
  bind_rows %>%
  as.data.frame %>%
  rownames_to_column(., var = "imputation") %>%
  group_by(A, B, C) %>%
  summarise(Frequency = n()) %>%
  arrange(-Frequency) %>%
  mutate(Percentage = formattable::percent(Frequency/sum(.$Frequency))) %>%
  kbl(
    .,
    escape = FALSE,
    booktabs = TRUE,
    align = "c",
    digits = 2,
    col.names = c(
      "Person",
      "Variable",
      "Time",
      "Frequency",
      "Percentage"
    ),
    caption = "Frequency Table of Suggested Component Dimensions (from 20 imputed datasets)") %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

### Follow-up: Studies (common variables)

- **individual studies**
- variables shared across studies
- interaction and no interaction responses
- bi-daily time scale

#### Study 1

##### Center and Normalize

```{r S1CommonStabCZ}
#################################################################
##                           Study 1                           ##
#################################################################


##----------------------------------------------------------------
##                  Center across Participants                  --
##----------------------------------------------------------------
MiS1StabilityWideC <- list()
for (i in 1:length(MiS1StabilityWide)) {
  MiS1StabilityWideC[[i]] <-
    ThreeWay::cent3(
      X = MiS1StabilityWide[[i]],
      # Nr. ppt.s (A)
      n = nrow(MiS1StabilityWide[[i]]),
      # Nr. variables (B)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiS1StabilityWide[[i]])))),
      # Nr. time points (C)
      p = length(unique(sub("_.*", "", names(MiS1StabilityWide[[i]])))),
      # Center across Participants
      mode = 1
    ) %>%
    as.data.frame
}

##----------------------------------------------------------------
##                 Normalize across Time Points                 --
##----------------------------------------------------------------
MiS1StabilityWideCZ <- list()
for (i in 1:length(MiS1StabilityWideC)) {
  MiS1StabilityWideCZ[[i]] <-
    ThreeWay::norm3(
      X = MiS1StabilityWideC[[i]], 
      # Nr. ppt.s (A)
      n = nrow(MiS1StabilityWideC[[i]]), 
      # Nr. variables (B)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiS1StabilityWideC[[i]])))),
      # Nr. time points (C)
      p = length(unique(sub("_.*", "", names(MiS1StabilityWideC[[i]])))),
      # Normalize across Time Points
      mode = 2
    ) %>%
    as.data.frame
  names(MiS1StabilityWideCZ[[i]]) <- names(MiS1StabilityWideC[[i]])
}
```

##### Component Complexity

```{r S1CommonStabCompNum}
# Run PCA SUP to determine best combination
S1StabPcaSup <- list()
capture.output(
  for(i in 1:length(MiS1StabilityWideCZ)) {
    S1StabPcaSup[[i]] <- 
    ThreeWay::T3runsApproxFit(
    X = MiS1StabilityWideCZ[[i]], 
    n = nrow(MiS1StabilityWideCZ[[i]]), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(MiS1StabilityWideCZ[[i]])))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(MiS1StabilityWideCZ[[i]])))), # Nr. time points (C)
    maxa = 20,
    maxb = 20,
    maxc = 20
  )
  }, 
file='NUL')

# Select Dimension Complexity
S1StabDimSelector <- list()
for (i in 1:length(S1StabPcaSup)) {
  S1StabDimSelector[[i]] <- capture.output(
    ThreeWay::DimSelector(
      out = S1StabPcaSup[[i]],
      n = nrow(MiS1StabilityWideCZ[[i]]), # Nr. ppt.s (A)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiS1StabilityWideCZ[[i]])))), # Nr. variables (B)
      p = length(unique(sub("_.*", "", names(MiS1StabilityWideCZ[[i]])))), # Nr. time points (C)
      model = 2
    )
  )
}

S1StabDim <- list()
for (i in 1:length(S1StabDimSelector)) {
  S1StabDim[[i]] <-
    tail(S1StabDimSelector[[i]], n = 1) %>%
    str_extract("\\(.*\\)") %>%
    stringr::str_extract_all('\\d+', simplify = TRUE) %>%
    as.data.frame %>%
    mutate_all(as.numeric) %>%
    dplyr::rename(c(A = V1,
                    B = V2,
                    C = V3))
}

S1StabDim %>%
  bind_rows %>%
  as.data.frame %>%
  rownames_to_column(., var = "imputation") %>%
  group_by(A, B, C) %>%
  summarise(Frequency = n()) %>%
  arrange(-Frequency) %>%
  mutate(Percentage = formattable::percent(Frequency/sum(.$Frequency))) %>%
  kbl(
    .,
    escape = FALSE,
    booktabs = TRUE,
    align = "c",
    digits = 2,
    col.names = c(
      "Person",
      "Variable",
      "Time",
      "Frequency",
      "Percentage"
    ),
    caption = "Follow-up (S1 Common): Frequency Table of Suggested Component Dimensions (from 20 imputed datasets)") %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

#### Study 2

##### Center and Normalize

##### Component Complexity

#### Study 3

##### Center and Normalize

##### Component Complexity


### Follow-up: Studies (ideosyncratic variables)

- **individual studies**
- **variables idiosyncratic to studies**
- interaction and no interaction responses
- bi-daily time scale

#### Study 1

##### Center and Normalize

```{r S1IdiosyncraticStabCZ}
#################################################################
##                           Study 1                           ##
#################################################################

##----------------------------------------------------------------
##                  Center across Participants                  --
##----------------------------------------------------------------
MiS1IdioStabilityWideC <- list()
for (i in 1:length(MiS1StabilityWideIdio)) {
  MiS1IdioStabilityWideC[[i]] <-
    ThreeWay::cent3(
      X = MiS1StabilityWideIdio[[i]],
      # Nr. ppt.s (A)
      n = nrow(MiS1StabilityWideIdio[[i]]),
      # Nr. variables (B)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiS1StabilityWideIdio[[i]])))),
      # Nr. time points (C)
      p = length(unique(sub("_.*", "", names(MiS1StabilityWideIdio[[i]])))),
      # Center across Participants
      mode = 1
    ) %>%
    as.data.frame
}

##----------------------------------------------------------------
##                 Normalize across Time Points                 --
##----------------------------------------------------------------
MiS1IdioStabilityWideCZ <- list()
for (i in 1:length(MiS1IdioStabilityWideC)) {
  MiS1IdioStabilityWideCZ[[i]] <-
    ThreeWay::norm3(
      X = MiS1IdioStabilityWideC[[i]], 
      # Nr. ppt.s (A)
      n = nrow(MiS1IdioStabilityWideC[[i]]), 
      # Nr. variables (B)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiS1IdioStabilityWideC[[i]])))),
      # Nr. time points (C)
      p = length(unique(sub("_.*", "", names(MiS1IdioStabilityWideC[[i]])))),
      # Normalize across Time Points
      mode = 2
    ) %>%
    as.data.frame
  names(MiS1IdioStabilityWideCZ[[i]]) <- names(MiS1IdioStabilityWideC[[i]])
}
```

##### Component Complexity

```{r S1IdiosyncraticStabCompNum}
# Run PCA SUP to determine best combination
S1IdioStabPcaSup <- list()
capture.output(
  for(i in 1:length(MiS1IdioStabilityWideCZ)) {
    S1IdioStabPcaSup[[i]] <- 
    ThreeWay::T3runsApproxFit(
    X = MiS1IdioStabilityWideCZ[[i]], 
    n = nrow(MiS1IdioStabilityWideCZ[[i]]), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(MiS1IdioStabilityWideCZ[[i]])))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(MiS1IdioStabilityWideCZ[[i]])))), # Nr. time points (C)
    maxa = 20,
    maxb = 20,
    maxc = 20
  )
  }, 
file='NUL')

# Select Dimension Complexity
S1IdioStabDimSelector <- list()
for (i in 1:length(S1IdioStabPcaSup)) {
  S1IdioStabDimSelector[[i]] <- capture.output(
    ThreeWay::DimSelector(
      out = S1IdioStabPcaSup[[i]],
      n = nrow(MiS1IdioStabilityWideCZ[[i]]), # Nr. ppt.s (A)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiS1IdioStabilityWideCZ[[i]])))), # Nr. variables (B)
      p = length(unique(sub("_.*", "", names(MiS1IdioStabilityWideCZ[[i]])))), # Nr. time points (C)
      model = 2
    )
  )
}

S1IdioStabDim <- list()
for (i in 1:length(S1IdioStabDimSelector)) {
  S1IdioStabDim[[i]] <-
    tail(S1IdioStabDimSelector[[i]], n = 1) %>%
    str_extract("\\(.*\\)") %>%
    stringr::str_extract_all('\\d+', simplify = TRUE) %>%
    as.data.frame %>%
    mutate_all(as.numeric) %>%
    dplyr::rename(c(A = V1,
                    B = V2,
                    C = V3))
}

S1IdioStabDim %>%
  bind_rows %>%
  as.data.frame %>%
  rownames_to_column(., var = "imputation") %>%
  group_by(A, B, C) %>%
  summarise(Frequency = n()) %>%
  arrange(-Frequency) %>%
  mutate(Percentage = formattable::percent(Frequency/sum(.$Frequency))) %>%
  kbl(
    .,
    escape = FALSE,
    booktabs = TRUE,
    align = "c",
    digits = 2,
    col.names = c(
      "Person",
      "Variable",
      "Time",
      "Frequency",
      "Percentage"
    ),
    caption = "Follow-up (S1 Idiosyncratic): Frequency Table of Suggested Component Dimensions (from 20 imputed datasets)") %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

#### Study 2

##### Center and Normalize

##### Component Complexity

#### Study 3

##### Center and Normalize

##### Component Complexity


### Follow-up: Response Type (interaction vs. no interaction)

- all three studies combined
- variables shared across studies
- **interaction vs. no interaction responses**
- bi-daily time scale

#### Center and Normalize

#### Component Complexity


### Follow-up: Time Scales (daily, weekly)

- all three studies combined
- variables shared across studies
- interaction and no interaction responses
- **daily and weekly time scales**

#### Daily

##### Center and Normalize

```{r DailyStabCZ}
##################################################################
##                  Center across Participants                  ##
##################################################################
MiMainDailyStabilityWideC <- list()
for (i in 1:length(MiMainDailyStabilityWide)) {
  MiMainDailyStabilityWideC[[i]] <-
    ThreeWay::cent3(
      X = MiMainDailyStabilityWide[[i]],
      # Nr. ppt.s (A)
      n = nrow(MiMainDailyStabilityWide[[i]]),
      # Nr. variables (B)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiMainDailyStabilityWide[[i]])))),
      # Nr. time points (C)
      p = length(unique(sub("_.*", "", names(MiMainDailyStabilityWide[[i]])))),
      # Center across Participants
      mode = 1
    ) %>%
    as.data.frame
}

##################################################################
##                 Normalize across Time Points                 ##
##################################################################
MiMainDailyStabilityWideCZ <- list()
for (i in 1:length(MiMainDailyStabilityWideC)) {
  MiMainDailyStabilityWideCZ[[i]] <-
    ThreeWay::norm3(
      X = MiMainDailyStabilityWideC[[i]], 
      # Nr. ppt.s (A)
      n = nrow(MiMainDailyStabilityWideC[[i]]), 
      # Nr. variables (B)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiMainDailyStabilityWideC[[i]])))),
      # Nr. time points (C)
      p = length(unique(sub("_.*", "", names(MiMainDailyStabilityWideC[[i]])))),
      # Normalize across Time Points
      mode = 2
    ) %>%
    as.data.frame
  names(MiMainDailyStabilityWideCZ[[i]]) <- names(MiMainDailyStabilityWideC[[i]])
}
```

##### Component Complexity

```{r MainDailyStabCompNum}
# Run PCA SUP to determine best combination
MainDailyStabPcaSup <- list()
capture.output(
  for(i in 1:length(MiMainDailyStabilityWideCZ)) {
    MainDailyStabPcaSup[[i]] <- 
    ThreeWay::T3runsApproxFit(
    X = MiMainDailyStabilityWideCZ[[i]], 
    n = nrow(MiMainDailyStabilityWideCZ[[i]]), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(MiMainDailyStabilityWideCZ[[i]])))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(MiMainDailyStabilityWideCZ[[i]])))), # Nr. time points (C)
    maxa = 20,
    maxb = 20,
    maxc = 20
  )
  }, 
file='NUL')

# Select Dimension Complexity
MainDailyStabDimSelector <- list()
for (i in 1:length(MainDailyStabPcaSup)) {
  MainDailyStabDimSelector[[i]] <- capture.output(
    ThreeWay::DimSelector(
      out = MainDailyStabPcaSup[[i]],
      n = nrow(MiMainDailyStabilityWideCZ[[i]]), # Nr. ppt.s (A)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiMainDailyStabilityWideCZ[[i]])))), # Nr. variables (B)
      p = length(unique(sub("_.*", "", names(MiMainDailyStabilityWideCZ[[i]])))), # Nr. time points (C)
      model = 2
    )
  )
}

MainDailyStabDim <- list()
for (i in 1:length(MainDailyStabDimSelector)) {
  MainDailyStabDim[[i]] <-
    tail(MainDailyStabDimSelector[[i]], n = 1) %>%
    str_extract("\\(.*\\)") %>%
    stringr::str_extract_all('\\d+', simplify = TRUE) %>%
    as.data.frame %>%
    mutate_all(as.numeric) %>%
    dplyr::rename(c(A = V1,
                    B = V2,
                    C = V3))
}

MainDailyStabDim %>%
  bind_rows %>%
  as.data.frame %>%
  rownames_to_column(., var = "imputation") %>%
  group_by(A, B, C) %>%
  summarise(Frequency = n()) %>%
  arrange(-Frequency) %>%
  mutate(Percentage = formattable::percent(Frequency/sum(.$Frequency))) %>%
  kbl(
    .,
    escape = FALSE,
    booktabs = TRUE,
    align = "c",
    digits = 2,
    col.names = c(
      "Person",
      "Variable",
      "Time",
      "Frequency",
      "Percentage"
    ),
    caption = "Follow-up (Daily): Frequency Table of Suggested Component Dimensions (from 20 imputed datasets)") %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```


#### Weekly

##### Center and Normalize

```{r MainWeeklyStabCZ}
##################################################################
##                  Center across Participants                  ##
##################################################################
MiMainWeeklyStabilityWideC <- list()
for (i in 1:length(MiMainWeeklyStabilityWide)) {
  MiMainWeeklyStabilityWideC[[i]] <-
    ThreeWay::cent3(
      X = MiMainWeeklyStabilityWide[[i]],
      # Nr. ppt.s (A)
      n = nrow(MiMainWeeklyStabilityWide[[i]]),
      # Nr. variables (B)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiMainWeeklyStabilityWide[[i]])))),
      # Nr. time points (C)
      p = length(unique(sub("_.*", "", names(MiMainWeeklyStabilityWide[[i]])))),
      # Center across Participants
      mode = 1
    ) %>%
    as.data.frame
}

##################################################################
##                 Normalize across Time Points                 ##
##################################################################
MiMainWeeklyStabilityWideCZ <- list()
for (i in 1:length(MiMainWeeklyStabilityWideC)) {
  MiMainWeeklyStabilityWideCZ[[i]] <-
    ThreeWay::norm3(
      X = MiMainWeeklyStabilityWideC[[i]], 
      # Nr. ppt.s (A)
      n = nrow(MiMainWeeklyStabilityWideC[[i]]), 
      # Nr. variables (B)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiMainWeeklyStabilityWideC[[i]])))),
      # Nr. time points (C)
      p = length(unique(sub("_.*", "", names(MiMainWeeklyStabilityWideC[[i]])))),
      # Normalize across Time Points
      mode = 2
    ) %>%
    as.data.frame
  names(MiMainWeeklyStabilityWideCZ[[i]]) <- names(MiMainWeeklyStabilityWideC[[i]])
}
```

##### Component Complexity

```{r MainWeeklyStabCompNum}
# Run PCA SUP to determine best combination
MainWeeklyStabPcaSup <- list()
capture.output(
  for(i in 1:length(MiMainWeeklyStabilityWideCZ)) {
    MainWeeklyStabPcaSup[[i]] <- 
    ThreeWay::T3runsApproxFit(
    X = MiMainWeeklyStabilityWideCZ[[i]], 
    n = nrow(MiMainWeeklyStabilityWideCZ[[i]]), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(MiMainWeeklyStabilityWideCZ[[i]])))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(MiMainWeeklyStabilityWideCZ[[i]])))), # Nr. time points (C)
    maxa = 20,
    maxb = 20,
    maxc = 20
  )
  }, 
file='NUL')

# Select Dimension Complexity
MainWeeklyStabDimSelector <- list()
for (i in 1:length(MainWeeklyStabPcaSup)) {
  MainWeeklyStabDimSelector[[i]] <- capture.output(
    ThreeWay::DimSelector(
      out = MainWeeklyStabPcaSup[[i]],
      n = nrow(MiMainWeeklyStabilityWideCZ[[i]]), # Nr. ppt.s (A)
      m = length(unique(sub("T[0-9]{2}_", "", names(MiMainWeeklyStabilityWideCZ[[i]])))), # Nr. variables (B)
      p = length(unique(sub("_.*", "", names(MiMainWeeklyStabilityWideCZ[[i]])))), # Nr. time points (C)
      model = 2
    )
  )
}

MainWeeklyStabDim <- list()
for (i in 1:length(MainWeeklyStabDimSelector)) {
  MainWeeklyStabDim[[i]] <-
    tail(MainWeeklyStabDimSelector[[i]], n = 1) %>%
    str_extract("\\(.*\\)") %>%
    stringr::str_extract_all('\\d+', simplify = TRUE) %>%
    as.data.frame %>%
    mutate_all(as.numeric) %>%
    dplyr::rename(c(A = V1,
                    B = V2,
                    C = V3))
}

MainWeeklyStabDim %>%
  bind_rows %>%
  as.data.frame %>%
  rownames_to_column(., var = "imputation") %>%
  group_by(A, B, C) %>%
  summarise(Frequency = n()) %>%
  arrange(-Frequency) %>%
  mutate(Percentage = formattable::percent(Frequency/sum(.$Frequency))) %>%
  kbl(
    .,
    escape = FALSE,
    booktabs = TRUE,
    align = "c",
    digits = 2,
    col.names = c(
      "Person",
      "Variable",
      "Time",
      "Frequency",
      "Percentage"
    ),
    caption = "Follow-up (Weekly): Frequency Table of Suggested Component Dimensions (from 20 imputed datasets)") %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

## Main Analysis

### Center and Normalize

We begin by centering across participants (within time point and variable) and normalize across time points and participants (within variable).

<div class="alert alert-warning alert-dismissible fade in" role="alert">
  <a class="close" data-dismiss="alert" aria-label="close">&times;</a>
  <i class="fas fa-exclamation-triangle"></i> <b>Question: </b><br/>
  Are the centering and especially the normalization done correctly?
</div>

```{r 3mpcaMainPrep}
##################################################################
##                  Center across Participants                  ##
##################################################################
dtMainImpWideC <- ThreeWay::cent3(
  X = dtMainImpWide, 
  n = nrow(dtMainImpWide), # Nr. ppt.s (A)
  m = length(unique(sub("T[0-9]{2}_", "", names(dtMainImpWide)))), # Nr. variables (B)
  p = length(unique(sub("_.*", "", names(dtMainImpWide)))), # Nr. time points (C)
  mode = 1
) %>%
  as.data.frame
#apply(dtMainImpWideC,2,sum)


##################################################################
##                 Normalize across Time Points                 ##
##################################################################
dtMainImpWideCZ <- ThreeWay::norm3(
  X = dtMainImpWideC, 
  n = nrow(dtMainImpWideC), # Nr. ppt.s (A)
  m = length(unique(sub("T[0-9]{2}_", "", names(dtMainImpWideC)))), # Nr. variables (B)
  p = length(unique(sub("_.*", "", names(dtMainImpWideC)))), # Nr. time points (C)
  mode = 2
) %>%
  as.data.frame
names(dtMainImpWideCZ) <- names(dtMainImpWideC)
#apply(dtMainImpWideC,2,sum)
# dtMainImpWideCZ %>%
#   select(ends_with("_AttitudesDutch")) %>%
#   unlist %>%
#   sd
# psych::describe(dtMainImpWideCZ)
```

### Select Component Complexity

We then select the optimal number of components for each mode (i.e., mode complexity). To do so, we use Convex Hull procedure heuristic based on total number of components and scree test value taken from a PCA-SUP analysis (where, PCASUP analyses are PCA's of supermatrices with slices of the 3way array next to each other, thus 3 supermatrices are analyed by PCA, and for each we get a component matrix, and eigenvalues. The eigenvalues may give an indication as to the required number of components for each mode.).

```{r 3mpcaMainCompNum}
# Run PCA SUP to determine best combination
capture.output(
  MainPcaSup <- ThreeWay::T3runsApproxFit(
    X = dtMainImpWideCZ, 
    n = nrow(dtMainImpWideCZ), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(dtMainImpWideCZ)))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(dtMainImpWideCZ)))), # Nr. time points (C)
    maxa = 20,
    maxb = 20,
    maxc = 20
  ), 
file='NUL')

# Select Dimension Complexity
MainDimSelector <- capture.output(
  ThreeWay::DimSelector(
    out = MainPcaSup,
    n = nrow(dtMainImpWideCZ), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(dtMainImpWideCZ)))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(dtMainImpWideCZ)))), # Nr. time points (C)
    model = 2
  )
)
MainDim <- tail(MainDimSelector, n = 1) %>%
  str_extract("\\(.*\\)") %>%
  stringr::str_extract_all('\\d+',simplify=TRUE) %>%
  as.data.frame %>% 
  mutate_all(as.numeric) %>%
  dplyr::rename(c(
    A = V1,
    B = V2,
    C = V3
  ))
#MainDim
```

Based on the Convex Hull heuristic, we select `r MainDim$A` components for the participant mode, `r MainDim$B` principle components for the variable mode, and `r MainDim$C` components for the time mode. It should be noted that these heuristics are sensitive to variations in the multiple imputation procedure. 

### Model Fitting

We then run the actual three-mode PCA with the inputs from the dimension selection procedure.

```{r 3mpcaMainRun}

# nrow(dtMainImpWide)
# length(names(dtPcaS123Imp)[!names(dtPcaS123Imp) %in% idVars])
# length(unique(dtPcaS123Imp$TIDnum))
# 
# t3MainFull <- ThreeWay::T3(
#   dtMainImpWide,
#   paste("PP", 1:nrow(dtMainImpWide), sep = "_"),
#   names(dtPcaS123Imp)[!names(dtPcaS123Imp) %in% idVars],
#   paste("T", 1:length(unique(dtPcaS123Imp$TIDnum)), sep = "")
# )

# 3MPCA with general trend
capture.output(
  t3MainRaw <- ThreeWay::T3func(
    X = dtMainImpWide,
    n = nrow(dtMainImpWide), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(dtMainImpWide)))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(dtMainImpWide)))), # Nr. time points (C)
    r1 = MainDim$A,
    r2 = MainDim$B,
    r3 = MainDim$C,
    start = 0,
    conv = 1e-6
  ), 
file='NUL')

# 3MPCA without general trend
capture.output(
  t3Main <- ThreeWay::T3func(
    X = dtMainImpWideCZ,
    n = nrow(dtMainImpWideCZ), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(dtMainImpWideCZ)))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(dtMainImpWideCZ)))), # Nr. time points (C)
    r1 = MainDim$A,
    r2 = MainDim$B,
    r3 = MainDim$C,
    start = 0,
    conv = 1e-6
  ), 
file='NUL')

# individual fit partitioning
FitT3Main <- T3fitpartitioning(
  Xprep = dtMainImpWideCZ,
  n = nrow(dtMainImpWideCZ), # Nr. ppt.s (A)
  m = length(unique(sub("T[0-9]{2}_", "", names(dtMainImpWideCZ)))), # Nr. variables (B)
  p = length(unique(sub("_.*", "", names(dtMainImpWideCZ)))), # Nr. time points (C)
  AS = t3Main$A, 
  BT = t3Main$B, 
  CU = t3Main$C, 
  K = t3Main$H, 
  renormmode = 0,
  laba = paste("PP", 1:nrow(dtMainImpWideCZ), sep = "_"), 
  labb = names(dtPcaS123Imp)[!names(dtPcaS123Imp) %in% idVars], 
  labc = paste("T", 1:length(unique(dtPcaS123Imp$TIDnum)), sep = "")
)

# Split half analysis
capture.output(
  SplitT3Main <- splithalfT3(
    X = dtMainImpWide,
    n = nrow(dtMainImpWide), # Nr. ppt.s (A)
    m = length(unique(sub("T[0-9]{2}_", "", names(dtMainImpWide)))), # Nr. variables (B)
    p = length(unique(sub("_.*", "", names(dtMainImpWide)))), # Nr. time points (C)
    r1 = MainDim$A,
    r2 = MainDim$B,
    r3 = MainDim$C, 
    centopt = 1,
    normopt = 2,
    conv = 1e-6,
    renormmode = 0,
    addanal = 5,
    wa_rel = 0,
    wb_rel = 0,
    wc_rel = 0,
    laba = paste("PP", 1:nrow(dtMainImpWideCZ), sep = "_"), 
    labb = names(dtPcaS123Imp)[!names(dtPcaS123Imp) %in% idVars], 
    labc = paste("T", 1:length(unique(dtPcaS123Imp$TIDnum)), sep = "")
  ), 
file='NUL')
# 0
# 0
```

In terms of fit percentage, we find that the three-mode PCA with the general trend included captures `r format(round(t3MainRaw$fp,2), nsmall = 2)`\% of the original variance, and the three-mode PCA without the general trend captures `r format(round(t3Main$fp,2), nsmall = 2)`\% of the variance.

<div class="alert alert-warning alert-dismissible fade in" role="alert">
  <a class="close" data-dismiss="alert" aria-label="close">&times;</a>
  <i class="fas fa-exclamation-triangle"></i> <b>Question: </b><br/>
  What is the interpretation of these Percentages? What exactly do the differences mean?
</div>

### Rotation

For interpretability of the components we perform an orthomax rotation of the component matrices and the core array.

```{r 3mpcaMainRotation}
capture.output(
  t3MainRot <- varimcoco(
    A = t3Main$A, 
    B = t3Main$B, 
    C = t3Main$C, 
    H = t3Main$H,
    wa_rel = 0,
    wb_rel = 0,
    wc_rel = 0
  ),
  file = 'NULL'
)
```

```{r 3mpcaMainAMat}
t3MainRot$AS %>%
  as.data.frame %>%
  rename_all(funs(str_replace_all(., "V", "Component_"))) %>%
  mutate(Participant = paste("PP", row_number(), sep = "_")) %>%
  select(
    Participant, 
    everything()
  ) %>%
  kbl(
  .,
  format = "html",
  escape = FALSE,
  align = c("l", rep("c",ncol(.)-1)),
  booktabs = TRUE,
  digits = 3,
  caption = "Rotated Component Matrix: Person Mode" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
```

```{r 3mpcaMainBMat}
t3MainRot$BT %>%
  as.data.frame %>%
  rename_all(funs(str_replace_all(., "V", "Component_"))) %>%
  mutate(Variable = names(dtPcaS123Imp)[!names(dtPcaS123Imp) %in% idVars]) %>%
  select(
    Variable, 
    everything()
  ) %>%
  kbl(
  .,
  format = "html",
  escape = FALSE,
  align = c("l", rep("c",ncol(.)-1)),
  booktabs = TRUE,
  digits = 3,
  caption = "Rotated Component Matrix: Variable Mode" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

```{r 3mpcaMainCMat}
t3MainRot$CU %>%
  as.data.frame %>%
  rename_all(funs(str_replace_all(., "V", "Component_"))) %>%
  mutate(Time = paste("T", row_number()-1, sep = "")) %>%
  select(
    Time, 
    everything()
  ) %>%
  kbl(
  .,
  format = "html",
  escape = FALSE,
  align = c("l", rep("c",ncol(.)-1)),
  booktabs = TRUE,
  digits = 3,
  caption = "Rotated Component Matrix: Time Mode" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
```

### Visualization: Time Components

We visualize the `r ncol(t3MainRot$C)` time components in their component scores over time.

<div class="alert alert-warning alert-dismissible fade in" role="alert">
  <a class="close" data-dismiss="alert" aria-label="close">&times;</a>
  <i class="fas fa-exclamation-triangle"></i> <b>Note: </b><br/>
  Not sure whether this interpretation is correct.
</div>


```{r 3mpcaMainCompVizTime}
t3MainRot$C %>%
  as.data.frame %>%
  rename_all(funs(str_replace_all(., "V", "Component_"))) %>%
  mutate(tid = row_number()) %>%
  melt(., id.vars = "tid") %>%
  ggplot(
    .,
    aes(
      x = tid,
      y = value,
      linetype = variable,
      shape = variable,
      color = variable
    )
  ) +
  geom_point() +
  geom_line() +
  geom_line(
    stat = "smooth",
    method = "loess",
    se = FALSE,
    span = .25,
    alpha = 0.75
  ) +
  labs(x = "Time occasion",
       y = "Component score",
       linetype = "Component",
       shape = "Component",
       color = "Component") +
  theme_Publication()

```

### Visualization: Person Components

We visualize how well the first two person components separate the participants. We focus on the first two components because they capture the most variance, while retaining easy two dimensional visualization options. We additionally color the participants using k means clustering to further showcase the separation.

```{r 3mpcaMainCompVizPerson}
# # Unsure how this should be interpreted 
# jointplotgen(
#   K = t3MainRot$K,
#   A = t3MainRot$A,
#   B = t3MainRot$B,
#   C = t3MainRot$C,
#   fixmode = 1,
#   fixunit = 3,
#   laba = paste("PP", 1:nrow(dtMainImpWideCZ), sep = "_"),
#   labb = names(dtPcaS123Imp)[!names(dtPcaS123Imp) %in% idVars],
#   labc = paste("T", 1:length(unique(dtPcaS123Imp$TIDnum)), sep = "")
# )

library(factoextra)
fviz_nbclust(t3MainRot$A[,1:2], kmeans, method = "wss")
clusterdata <- t3MainRot$A[,1:2]
km.res <- kmeans(clusterdata, 3, nstart = 25)
fviz_cluster(object = km.res, # kmeans object
             data = clusterdata, # data used for clustering
             ellipse.type = "norm",
             geom = "point",
             palette = "jco",
             main = "",
             xlab = "Person Component 1",
             ylab = "Person Component 2",
             ggtheme = theme_Publication())
```


### Visualization: Variable Components

To visualize how the principle components separate the different variables, we draw the component scores for all variables.

```{r 3mpcaMainCompVizVariable}
t3MainRot$B %>%
  as.data.frame %>%
  rename_all(funs(str_replace_all(., "V", "Component_"))) %>%
  mutate(var = names(dtPcaS123Imp)[!names(dtPcaS123Imp) %in% idVars]) %>%
  melt(., id.vars = "var") %>%
  ggplot(
    .,
    aes(
      x = var,
      y = value,
      group = variable,
      linetype = variable,
      shape = variable,
      color = variable
    )
  ) +
  geom_point() +
  geom_line() +
  coord_flip() +
  labs(x = "Variable",
       y = "Component score",
       linetype = "Component",
       shape = "Component",
       color = "Component") +
  theme_Publication()
```

<div class="alert alert-warning alert-dismissible fade in" role="alert">
  <a class="close" data-dismiss="alert" aria-label="close">&times;</a>
  <i class="fas fa-exclamation-triangle"></i> <b>Note: </b><br/>
  How do we best interpret and visualize the joint decomposition? 
</div>

# **Software Information**

The full session information with all relevant system information and all loaded and installed packages is available in the collapsible section below.

<details>

<summary>

System Info

</summary>

\renewcommand{\arraystretch}{0.8}

<!-- decrease line spacing for the table -->

```{r Reproducibility-sessionInfo-R-environment, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width='100%', results='asis'}
df_session_platform <- devtools::session_info()$platform %>%
  unlist(.) %>%
  as.data.frame(.) %>%
  rownames_to_column(.)

colnames(df_session_platform) <- c("Setting", "Value")

kbl(
  df_session_platform,
  booktabs = T,
  align = "l",
  caption = "R environment session info for reproducibility of results" # complete caption for main document
) %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

\renewcommand{\arraystretch}{1}

<!-- reset row height/line spacing -->

</details>

<br>

<details>

<summary>

Package Info

</summary>

\renewcommand{\arraystretch}{0.6}

<!-- decrease line spacing for the table -->

```{r Reproducibility-sessionInfo-R-packages, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width='100%', results='asis'}
df_session_packages <- devtools::session_info()$packages %>%
  as.data.frame(.) %>%
  filter(attached == TRUE) %>%
  dplyr::select(loadedversion, date, source) %>%
  rownames_to_column()

colnames(df_session_packages) <- c("Package", "Loaded version", "Date", "Source")

kbl(
  df_session_packages,
  booktabs = T,
  align = "l",
  caption = "Package info for reproducibility of results" # complete caption for main document
) %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

\renewcommand{\arraystretch}{1}

<!-- reset row height/line spacing -->

</details>

<br>

<details>

<summary>

Full Session Info (including loaded but unattached packages --- for troubleshooting only)

</summary>

`r pander(sessionInfo(), compact = FALSE)`

</details>

</br>

------------------------------------------------------------------------

</br>

# **References**

::: {.tocify-extend-page data-unique="tocify-extend-page" style="height: 0;"}
:::
