---
title: "Supplemental Information A: Full Annotated Analysis and Reproducible Code"
subtitle: "Supplemental Material for 'Migration Trajectories [working title]'"
author:
- Jannis Kreienkamp^1,3^
- Kai Epstude^1,3^
- Laura F. Bringmann^1,3^
- Maximilian Agostini^1,3^
- Peter de Jonge^1,3^
- Rei Monden^2,3^
- ^1^University of Groningen, Department of Psychology
- ^2^Hiroshima University, Graduate School of Advanced Science and Engineering
- ^3^Author order TBD [currently in first name alphabetical order]
- "Author Information:"
- "Correspondence concerning this article should be addressed to Jannis Kreienkamp, Department of Psychology, University of Groningen, Grote Kruisstraat 2/1, 9712 TS Groningen (The Netherlands).  E-mail: j.kreienkamp@rug.nl"
- 'The main manuscript is available at <a href="https://www.doi.org/ToBePublished" target="_blank">doi.org/ToBePublished</a>'
- 'The data repository for this manuscript is available at <a href="https://osf.io/TBA" target="_blank">osf.io/TBA</a>'
- 'The GitHub repository for this manuscript is available at <a href="https://github.com/maskedForPeerReview" target="_blank">github.com/maskedForPeerReview</a>'
date: "Last updated: `r format(Sys.time(), '%d %B, %Y')`"
output: 
  bookdown::html_document2:
    fig_caption: yes
    md_extensions: +footnotes
    code_folding: hide
    mathjax: default
    theme: yeti
    toc: yes
    toc_float: yes
    number_sections: false
    css: style.css
    includes:
      in_header: "_includes/head-custom-rmd.html" 
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: apa.csl
header-includes:
   - \usepackage{amsmath, nccmath}
---

```{=html}
<style type="text/css">
.main-container {
  max-width: 1300px;
  margin-left: auto;
  margin-right: auto;
}
.table {
  margin-left:auto; 
  margin-right:auto;
}
</style>
```


```{r setup, include=FALSE}
# R Studio Clean-Up
cat("\014") # clear console
# rm(list=ls()) # clear workspace - use restart R instead [cmd/alt + shift + F10]
gc() # garbage collector

# Install and Load Packages
# !IMPORTANT!
# BEFORE FIRST RENDER:
# To install all relevant packages please run "renv::restore()" (or renv::init() and then initiate from lockfile) in the console before the first use to ensure that all packages are using the correct version.
# to store the packages in a contained library within the project folder: renv::settings$use.cache(FALSE) and add 'RENV_CONFIG_SANDBOX_ENABLED = FALSE' to an '.Renviron' file
lib <- c(
  "rmarkdown",
  "knitr",
  "remedy",
  "bookdown",
  #"tidyverse",
  "MASS",
  "brms",
  "psych",
  "ggplot2",
  "ggthemes",
  "haven",
  "RColorBrewer",
  "plotly",
  "grid",
  "gridExtra",
  "ggpattern",
  "lme4",
  "nlme",
  "jtools",
  "gtsummary",
  "sessioninfo",
  "tibble",
  "pander",
  "devtools",
  "mada",
  "data.table",
  "plyr",
  "dplyr",
  "tidyr",
  "Hmisc",
  "kableExtra",
  "papaja",
  "stringr",
  "stringi",
  "reshape2",
  "anytime",
  "lubridate",
  "purrr",
  "metafor",
  "dygraphs",
  "readxl", 
  "reshape",
  "factoextra",
  "Amelia", 
  "ThreeWay",
  "DescTools",
  # GAM
  "gratia",
  #"MASS",
  "mgcv",
  # For moving window models
  "Kendall",
  # For the Hamed & Rao correction of tau
  "modifiedmk", 
  "mgcViz",
  "shiny"
)
invisible(lapply(lib, library, character.only = TRUE))
rm(lib)

anytime::addFormats("%d-%m-%Y %H:%M:%S")  ## add format to anytime package (not default)
anytime::addFormats("%Y-%m-%d %H:%M:%S")
anytime::addFormats("%d-%m-%Y")  
#options(rgl.useNULL = TRUE)

# Load Custom Packages
source("./scripts/functions/fun.panel.R")
source("./scripts/functions/themes.R")
source("./scripts/functions/binaryCor.R")
source("./scripts/functions/MlCorMat.R")
source("./scripts/functions/MlTbl.R")
source("./scripts/functions/metaLmer.R")
source("./scripts/functions/meanViz.R")
source("./scripts/functions/gam.R")
source("./scripts/functions/tsFeatureExtractor.R")

# Markdown Options
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # set working directory
knitr::opts_knit$get("root.dir") # check working directory
options(
  scipen = 999,
  digits = 4,
  width = 400
) # removes scientific quotation
# knitr::opts_chunk$set(echo = TRUE, cache = F, cache.path = rprojroot::find_rstudio_root_file('cache/')) # cache settings
knitr::knit_hooks$set(
  error = function(x, options) {
    paste('\n\n<div class="alert alert-danger">',
      gsub("##", "\n", gsub("^##\ Error", "**Error**", x)),
      "</div>",
      sep = "\n"
    )
  },
  warning = function(x, options) {
    paste('\n\n<div class="alert alert-warning">',
      gsub("##", "\n", gsub("^##\ Warning:", "**Warning**", x)),
      "</div>",
      sep = "\n"
    )
  },
  message = function(x, options) {
    paste('\n\n<div class="alert alert-info">',
      gsub("##", "\n", x),
      "</div>",
      sep = "\n"
    )
  }
)
htmltools::tagList(rmarkdown::html_dependency_font_awesome())

# Global Chunk Options
knitr::opts_chunk$set(
  fig.width = 12,
  fig.height = 8,
  fig.path = "Figures/",
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

</br>

------------------------------------------------------------------------

</br>

<i class="fas fa-exclamation-circle"></i> Note. Boxplots display the interquartile range (IQR, center box), and the whiskers extend 1.5\*IQR from the lower and upper hinge. The white point indicates the mean and the white center line indicates the median.

</br>

------------------------------------------------------------------------

</br>

# **Data Preparation**

In an initial preparatory step, we import the data into the R project environment and prepare the variables for further processing and later analyses.

## Data Import

The data were collected using two different survey tools. For the study with sojourners (Study 1: worker) we used the survey platform Qualtrics XM, whereas the studies with international students (Study 2: student), and the international medical professionals (Study 3: medical) were conducted using the survey framework FormR. This means that the datasets had inconsistent file formats and naming conventions. For the Qualtrics study we pre-processed some variables to ease the import process (for the syntax files see the SPS files in '*data/S1_Workers/processed/cleaned*' and for the raw data files see '*data/S1_Workers/raw*'). For the two other studies, we import the raw csv files from their respective folders.

```{r formrImport}
# Load variable name lookup table
varNames <- readxl::read_excel("preregistration/varNames.xlsx")

# rename function
reNam <- function(data = NA, names = varNames, study = "S1", survey = "pre") {
  # for testing
  # data = dtS1$raw.pre
  # names = varNames
  # study = "S1"
  # survey = "pre"
  
  var <- paste0("survey", study)
  varNamOld <- paste0("varNam", study)
  
  nameTbl <- names %>%
    filter((!!sym(var)) == survey) %>%
    select(varNam, varNamOld = (!!sym(varNamOld)))
  
  data.table::setnames(data, as.character(nameTbl$varNamOld), as.character(nameTbl$varNam), skip_absent=TRUE)
  
  return(data)
}

# workers
# initial data cleaning was done in SPSS (syntax files are available in "")
dtS1 <- list(
  raw.pre = read_spss("data/S1_Workers/processed/cleaned/MT - Pre-Measure - 06-15-2018.sav") %>% 
    reNam(data = ., names = varNames, study = "S1", survey = "pre"),
  raw.post = read_spss("data/S1_Workers/processed/cleaned/MT - Post-Measure - 06-15-2018.sav") %>% 
    reNam(data = ., names = varNames, study = "S1", survey = "post"),
  raw.morning = read_spss("data/S1_Workers/processed/cleaned/MT - Morning - 06-15-2018.sav") %>% 
    reNam(data = ., names = varNames, study = "S1", survey = "daily"),
  raw.afternoon = read_spss("data/S1_Workers/processed/cleaned/MT - Afternoon - 06-15-2018.sav") %>% 
    reNam(data = ., names = varNames, study = "S1", survey = "daily")
)

# students
dtS2 <- list(
  raw.pre = read.csv(file = "data/S2_Students/raw/AOTS_Pre.csv", header = T, sep = ",") %>% 
    reNam(data = ., names = varNames, study = "S2", survey = "pre"),
  raw.post = read.csv(file = "data/S2_Students/raw/AOTS_Post.csv", header = T, sep = ",") %>% 
    reNam(data = ., names = varNames, study = "S2", survey = "post"),
  raw.daily = read.csv(file = "data/S2_Students/raw/AOTS_Daily.csv", header = T, sep = ",") %>% 
    reNam(data = ., names = varNames, study = "S2", survey = "daily")
)

# young medical professionals
dtS3 <- list(
  raw.eligibility = read.csv("data/S3_Medical/raw/AOTM_Eligibility.csv") %>% 
    reNam(data = ., names = varNames, study = "S3", survey = "pre_entry"), # works but should be 'survey = "pre_elig"'
  raw.pre = read.csv("data/S3_Medical/raw/AOTM_Pre.csv") %>% 
    reNam(data = ., names = varNames, study = "S3", survey = "pre_entry"),
  raw.post = read.csv("data/S3_Medical/raw/AOTM_Post.csv") %>% 
    reNam(data = ., names = varNames, study = "S3", survey = "post"),
  raw.daily = read.csv("data/S3_Medical/raw/AOTM_Daily.csv") %>% 
    reNam(data = ., names = varNames, study = "S3", survey = "daily")
)

# # FROM INITIAL VAR NAME EXPORT [LEGACY]
# # export var names
# S1Nam <- rbind(
#   data.frame(
#     study = "S1",
#     survey = "pre",
#     varNam = names(dtS1$raw.pre)
#   ),
#   data.frame(
#     study = "S1",
#     survey = "daily",
#     varNam = names(dtS1$raw.afternoon)
#   ),
#   data.frame(
#     study = "S1",
#     survey = "post",
#     varNam = names(dtS1$raw.post)
#   )
# )
# S2Nam <- rbind(
#   data.frame(
#     study = "S2",
#     survey = "pre",
#     varNam = names(dtS2$raw.pre)
#   ),
#   data.frame(
#     study = "S2",
#     survey = "daily",
#     varNam = names(dtS2$raw.daily)
#   ),
#   data.frame(
#     study = "S2",
#     survey = "post",
#     varNam = names(dtS2$raw.post)
#   )
# )
# S3Nam <- rbind(
#   data.frame(
#     study = "S3",
#     survey = "pre_elig",
#     varNam = names(dtS3$raw.eligibility)
#   ),
#   data.frame(
#     study = "S3",
#     survey = "pre_entry",
#     varNam = names(dtS3$raw.pre)
#   ),
#   data.frame(
#     study = "S3",
#     survey = "daily",
#     varNam = names(dtS3$raw.daily)
#   ),
#   data.frame(
#     study = "S3",
#     survey = "post",
#     varNam = names(dtS3$raw.post)
#   )
# )
# write.csv(
#   x = rbind(
#     S1Nam,
#     S2Nam,
#     S3Nam
#   ),
#   file = "preregistration/varNames.csv"
# )
```

## Data Cleaning & Data Exclusions

### Study 1

For the sojourner sample data was collected in four separate surveys: (1) the pre-measurement, (2) the daily morning survey, (3) the daily afternoon survey, as well as (4) a post-measurement. We combine the four individual surveys into one cohesive dataframe and drop superfluous variables that are not relevant to the analyses relevant here. We then format the time and date variables and add person- and measurement indices (for easy and meaningful addressing of the data). We also exclude our own test data.\
<i class="fas fa-info-circle"></i> *Note:* All data preparation steps are saved in the '*dtS1*' list.

```{r cleanWorker}
# Create reduced data sets for morning and afternoon
dat.mo <- dtS1$raw.morning %>%
  select(-starts_with("t_"))
#setdiff(names(dtS1$raw.morning), names(dat.mo))
dat.mo$daytime <- "morning"

dat.af <- dtS1$raw.afternoon %>%
  select(-starts_with("t_"))
dat.af$daytime <- "afternoon"

# merge morning and afternoon measurements with indicator [+ clean up]
daily.dat <- plyr::rbind.fill(dat.mo, dat.af)
daily.dat <- daily.dat[daily.dat$last_outside_referrer != 55951, ]
dtS1$daily <- daily.dat
rm(dat.mo, dat.af, daily.dat)

# reduced data set for pre measurement
dat.pre.red <- dtS1$raw.pre %>%
  select(-starts_with("t_"))
names(dat.pre.red) <- paste(names(dat.pre.red), "pre", sep = ".")

# merge with daily data [+ clean up]
df.pre <- merge(
  x = dtS1$daily,
  y = dat.pre.red,
  by.x = "last_outside_referrer",
  by.y = "platformId.pre",
  all = T
)

# adjust duplicate names to fit to indicate daily or pre measurement
names(df.pre) <- gsub("[[:punct:]]x", ".daily", names(df.pre))
names(df.pre) <- gsub("[[:punct:]]y", ".pre", names(df.pre))

# reduced data set for post-measurement
dat.post.red <- dtS1$raw.post %>%
  select(-starts_with("t_"))
names(dat.post.red) <- paste(names(dat.post.red), "post", sep = ".")

# merge post measurement with pre- and daily data
df <- merge(
  x = df.pre,
  y = dat.post.red,
  by.x = "last_outside_referrer",
  by.y = "last_outside_referrer.post",
  all = T
)

# adjust duplicate names to indicate pre or post
names(df) <- gsub("[[:punct:]]x", ".pre", names(df))
names(df) <- gsub("[[:punct:]]y", ".post", names(df))

# add to list
dtS1$combined <- df
rm(df, df.pre, dat.post.red, dat.pre.red)

# create data frame with cleaned data
df <- dtS1$combined %>%
  filter(
    Finished.pre == 1,
    Finished == 1,
    !is.na(last_outside_referrer)
  )

# add running number as measurement ID within participants
df$measureID <- data.table::rowidv(df, cols = c("last_outside_referrer"))

dtS1$clean <- df

# clean up
rm(df)

# Export reduced Data
# write.csv(dtS1$clean, "data/processed/MT_clean-merged_07-05-2018.csv", row.names = F)
# save(dtS1$clean, file = "data/processed/MT_clean-merged_07-05-2018.RData")
```

### Study 2

For the student sample data was, similarly, collected in three separate surveys: (1) the pre-measurement, (2) the daily survey sent out at lunch and dinner time, and (3) a post-measurement. We combine the three individual surveys into one large dataframe and drop superfluous variables that are not relevant to the analyses relevant here. We exclude our own test data as well as one participant who entered the study twice (but gave different responses during the pre-measurement). We also reformat missing values and format core ID variables.\
<i class="fas fa-info-circle"></i> *Note:* All data preparation steps are saved in the '*dtS2*' list.

```{r cleanStudents}
# our own test IDs
ownIDs <- c(
  "beautifulLionfishXXXR5rcgVBzGu8hPvOqrK8UBJBw4owvi9nfRFSFu3lMzYhE",
  "niceDogoXXXmB8JI5SFu78SF3DVof84mGUPPNUr14p2HYFTtp31a6D1OwAzM6F-K",
  "amusedQuailXXXmhuc_fpTp8vPkMwDH1BzjaH1d1kHSO1bsPEfsnaEYk4WeVBfPi",
  "juwGAbtXX0_1kmZtSVqKh3PGaHOICqUyU4iBkrT3nDsI_uifuD1gzKcZerxaM5FL"
)

# Prepare dfs for Cleaning
df.pre <- dtS2$raw.pre %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>%
  filter(!is.na(ended)) %>% # remove all who did not finish
  filter(!email %in% .$email[duplicated(.$email)]) %>% # remove all who did the pre questionnaire multiple times (b/c inconsistent ratings scales)
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)
names(df.pre) <- paste(names(df.pre), "pre", sep = ".")

df.post <- dtS2$raw.post %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>%
  filter(!is.na(ResponseId)) %>% # remove own test runs
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  filter(ResponseId %in% df.pre$ResponseId) %>% # remove anyone who wasn't in the pre
  filter(!is.na(ended)) %>% # remove all who never finished
  filter(!ResponseId %in% .$ResponseId[duplicated(.$ResponseId)]) %>% # remove all duplicate ResponseIds
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)
names(df.post) <- paste(names(df.post), "post", sep = ".")

df.daily <- dtS2$raw.daily %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>%
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  filter(ResponseId %in% df.pre$ResponseId) %>% # remove anyone who wasn't in the pre
  filter(!is.na(ended)) %>% # remove all who never finished
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)

# merge daily with pre
dfPreDaily <- merge(
  x = df.daily,
  y = df.pre,
  by.x = "ResponseId",
  by.y = "ResponseId.pre", 
  suffixes = c(".daily", ".pre"),
  all = FALSE
)

# merge daily with post
dfCombined <- merge(
  x = dfPreDaily,
  y = df.post,
  by.x = "ResponseId",
  by.y = "ResponseId.post", 
  suffixes = c(".pre", ".post"),
  all = FALSE
)

# add to list
dtS2$clean <- dfCombined

# clean up workspace
rm(df.pre, df.daily, df.post, dfPreDaily, dfCombined, ownIDs)
```

### Study 3

For the medical professionals sample data was, again, collected in three separate surveys: (1) the pre-measurement, (2) the daily survey sent out at lunch and dinner time, and (3) a post-measurement. We combine the three individual surveys into one large dataframe. We exclude our own test data. We also reformat missing values and format core ID variables.\
<i class="fas fa-info-circle"></i> *Note:* All data preparation steps are saved in the '*dtS3*' list.

```{r cleanMedical}
# our own test IDs
ownIDs <- c(
  "test_LeonieXXXSklxecPLW0-FBPM4796o3pUwUhAY5jb9KGw8jQsKxWmGpa1Jiy", 
  "test_MaxXXXtOp_5dTNefIq0yKXtXt2IN6eEKxeHoPY9mlyvdsqPpLp1B0NGg4UL",
  "test_JannisXXXBsNqk62fOpX6chbd2tMWPptUdjjnhAqnQ3uBqckZ7gLIEoPlfZ",
  "quaintLeopardCatXXXAJ9cfSj-_SZLwNwMDxv_xv_iyr1Bg5YFLTlYdrjW0UXZY",
  "blue-eyedIndianElephantXXXLf5zPMpQCDGS3umFzIj-YVky7ivTItvvozW49m"
)

# Prepare dfs for Cleaning
df.pre <- dtS3$raw.pre %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>%
  filter(!is.na(ended)) %>% # remove all who did not finish
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)
names(df.pre) <- paste(names(df.pre), "pre", sep = ".")

df.post <- dtS3$raw.post %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>% 
  filter(!is.na(ResponseId)) %>% # remove own test runs
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  filter(ResponseId %in% df.pre$ResponseId) %>% # remove anyone who wasn't in the pre
  #filter(!is.na(ended)) %>% # remove all who never finished [disabled because only relevant if data is missing]
  filter(!ResponseId %in% .$ResponseId[duplicated(.$ResponseId)]) %>% # remove all duplicate ResponseIds
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)
names(df.post) <- paste(names(df.post), "post", sep = ".")

df.daily <- dtS3$raw.daily %>%
  mutate_all(na_if, "") %>%
  mutate_all(na_if, "NA") %>%
  filter(!ResponseId %in% ownIDs) %>% # remove our own test
  filter(ResponseId %in% df.pre$ResponseId) %>% # remove anyone who wasn't in the pre
  #filter(!is.na(ended)) %>% # remove all who never finished [disabled because only relevant if data is missing]
  mutate(ResponseId = as.character(ResponseId)) # turn factor into character strings (probably just precaution)

# merge daily with pre
dfPreDaily <- merge(
  x = df.daily,
  y = df.pre,
  by.x = "ResponseId",
  by.y = "ResponseId.pre", 
  suffixes = c(".daily", ".pre"),
  all = F
)

# merge daily with post
dfCombined <- merge(
  x = dfPreDaily,
  y = df.post,
  by.x = "ResponseId",
  by.y = "ResponseId.post", 
  suffixes = c(".pre", ".post"),
  all = F
)

# add to list
dtS3$clean <- dfCombined

# clean up workspace
rm(df.pre, df.daily, df.post, dfPreDaily, dfCombined, ownIDs)
```

## Calculate needed transformations

### Study 1

For the worker sample, the data transformation stage had three main aims:

1.  We first corrected time indicators within the surveys. In some cases participants completed their daily diary surveys for the afternoon after midnight. In these cases the measurement still is in reference to the previous day and is indicated in the corrected variable.\
2.  We then created indices of scales. Some indices were multi-item scales while some indices combine equivalent measurement for different situational circumstances (e.g., competence perceptions after interactions and at measurement occasions without interactions).\
3.  Finally, we calculated several basic participant summaries (averages across all measurement occasions).

```{r newVarsWorkers}
df <- dtS1$clean

# Correct date_period ()
df <- df %>%
  mutate(
    startDate = as.Date(created),
    startTime = format(as.POSIXct(created), format = "%H:%M:%S")
  )
  
# order time
# df$TID <- factor(df$date_period, levels = unique(dtS3$raw.daily$date_period))
# df$TIDnum <- as.numeric(df$TID) # get numeric TID
df <- df %>%
  mutate(
    PID = as.numeric(factor(last_outside_referrer))
  )

# Time and Date Variables
df <- df %>%
  mutate(
    TID = measureID - 1,
    # time ID with t0 = 0 for meaningfull intercept interpretations
    date = substr(created, 1, 10),
    # awkward way of extracting date (best converted to )
    time = substr(created, 12, 19),
    # awkward way of extracting time
    daynum = as.numeric(factor(dateQualtrics)),
    # all days as numeric for ordering
    daycor = ifelse(
      daytime == "morning" &
        period_to_seconds(hms(time)) < period_to_seconds(hms("12:00:00")) |
        daytime == "afternoon" &
          period_to_seconds(hms(time)) < period_to_seconds(hms("19:00:00")),
      daynum - 1,
      daynum
    ),
    daycorTest = ifelse(
      daytime == "morning" &
        period_to_seconds(hms(time)) < period_to_seconds(hms("12:00:00")) |
        daytime == "afternoon" &
          period_to_seconds(hms(time)) < period_to_seconds(hms("19:00:00")),
      1,
      0
    ),
    # correctly identify which date the questionnaire is about
    daycor.lead = sprintf("%02d", daycor),
    daytime.lt = ifelse(daytime == "morning", "a", "b"),
    # morning / afternoon to a / b
    day_time = paste(daycor.lead, daytime.lt, sep = "_"),
    # combine day id with morning / afternoon
    ResponseId = as.numeric(factor(day_time)),
    # day and time identifier as numeric id
    SubTime = chron::times(time.0),
    time.daily = as.character(time),
    PPDate = as.Date(df$dateQualtrics),
    number = replace_na(ContactNum, 0),
    NonDutchNum = replace_na(NonDutchNum, 0)
  )

# remove seconds from afternoon time
df$SubTime[df$daytime == "afternoon"] <- paste0(substring(as.character(df$time.0[df$daytime == "afternoon"]), 4, 8), ":00")
df$time.daily[df$daytime == "afternoon" &
  !is.na(df$time.daily != "<NA>")] <- paste0(substring(as.character(df$time.daily[df$daytime == "afternoon" &
  !is.na(df$time.daily != "<NA>")]), 4, 8), ":00")

# Correct morning / afternoon date where survey was collected the day after to indicate the correct date that was targeted
df$PPDate[df$SubTime < "11:50:00" &
  df$daytime == "morning"] <- df$PPDate[df$SubTime < "11:50:00" &
  df$daytime == "morning"] - 1
df$PPDate[df$SubTime < "18:50:00" &
  df$daytime == "afternoon"] <- df$PPDate[df$SubTime < "18:50:00" &
  df$daytime == "afternoon"] - 1

# Make time IDs consistent with later studies
df$TID <- paste(df$PPDate, str_to_title(df$daytime))
df$TIDnum <- as.numeric(factor(df$TID %>% gsub(" Morning", "-A", .) %>% gsub(" Afternoon", "-B", .)))

df <- df %>%
  rowwise() %>%
  mutate(
    InteractionDum = sum(IntergroupContact, IngroupContact, na.rm = FALSE),
    InteractionDum = if_else(InteractionDum > 0, 1, InteractionDum),
    alertness = sum(alertness1, alertness2, na.rm = TRUE),
    calmness = sum(calmness1, calmness2, na.rm = TRUE),
    valence = sum(valence1, valence2, na.rm = TRUE)
  ) %>%
  ungroup()

# Need scales
df$keyMotiveFulfilled <- rowSums(df[, c("KeyNeedFulfillment", "DaytimeNeedFulfillment")], na.rm = T)
df$autonomy.daily.all <- rowSums(df[, c("autonomy_Int", "autonomy_NoInt")], na.rm = T)
df$competence.daily.all <- rowSums(df[, c("competence_Int", "competence_NoInt")], na.rm = T)
# cor(df$relatednessOther, df$relatedness_self_1,use="complete.obs")
df$relatedness.daily.all <- rowMeans(df[, c(
  "relatednessOther",
  "relatednessSelf",
  "relatednessNoInteraction"
)], na.rm = T)

pairs.panels.new(
  df[c("relatednessSelf", "relatednessOther")],
  labels = c(
    "I shared information about myself.",
    "X shared information about themselves."
  )
)
df$relatedness <- rowMeans(df[, c("relatednessOther", "relatednessSelf")], na.rm = T)

df$autonomy <- df$autonomy.daily.all
df$competence <- df$competence.daily.all
df$relatedness <- df$relatedness.daily.all

varNamIndicesS1 <- c(
  "autonomy",
  "competence",
  "relatedness",
  "education_level.pre",
  "associationMerged.pre",
  "assimilation.pre",
  "separation.pre",
  "integration.pre",
  "marginalization.pre",
  "VIA_heritage.pre",
  "VIA_Dutch.pre",
  "SSAS_surrounding.pre",
  "SSAS_privat.pre",
  "SSAS_public.pre",
  "assimilation.post",
  "separation.post",
  "integration.post",
  "marginalization.post",
  "VIA_heritage.post",
  "VIA_Dutch.post",
  "rosenberg.post",
  "social_support.post",
  "stress.post",
  "discrimination.post",
  "discrimination_month.post",
  "NLE_1month.post",
  "NLE_6month.post",
  "NLE_12month.post"
  )

df$roommate.pre_calc <- df %>%
  select(
    starts_with("roommate")
  ) %>% 
  mutate_all(as_factor) %>%
  unite(., "Comb", sep = ", ", remove = TRUE, na.rm = TRUE) %>%
  pull

df$Reason.pre_calc <- df %>%
  select(
    starts_with("Reason"),
    -Reason_nodesire, 
    -ReasonOther.pre 
  ) %>% 
  mutate_all(as_factor) %>% 
  unite(., "Comb", sep = ", ", remove = TRUE, na.rm = TRUE) %>%
  pull

df$occupation.pre_calc <- df %>%
  select(
    starts_with("occupation"),
    -occupation_8_TEXT.pre
  ) %>%
  mutate_all(as_factor) %>% 
  unite(., "Comb", sep = ", ", remove = TRUE, na.rm = TRUE) %>% 
  pull 
  
df$CurrentEducation.pre_calc <-
  df %>%
  select(starts_with("CurrentEducation"),
         -CurrentEducation_6_TEXT.pre) %>%
  mutate_all(as_factor) %>%
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$groupType.pre_calc <-
  df %>%
  select(starts_with("gr_type"),
          -ends_with("TEXT")) %>%
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$groupContext.pre_calc <-
  df %>%
  select(starts_with("gr_context"),
          -ends_with("TEXT")) %>%
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$dyadType.pre_calc <-
  df %>%
  select(starts_with("dyad_type"),
          -ends_with("TEXT")) %>%
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$context.pre_calc <-
  df %>%
  select(starts_with("Context"),
          -ends_with("TEXT")) %>% 
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$desireType.pre_calc <-
  df %>%
  select(starts_with("desire_type"),
          -ends_with("TEXT")) %>% 
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$desireType.pre_calc <-
  df %>%
  select(starts_with("desire_context"),
          -ends_with("TEXT")) %>% 
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$nonDutchType.pre_calc <-
  df %>%
  select(starts_with("NonDutchType"),
          -ends_with("TEXT")) %>% 
  mutate_all(as_factor) %>% 
  unite(.,
        "Comb",
        sep = ", ",
        remove = TRUE,
        na.rm = TRUE) %>%
  pull

df$autonomySat.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT0[1-4].pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric
df$autonomyFrust.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT0[5-8].pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric

df$relatednessSat.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT(09|1[0-2]).pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric
df$relatednessFrust.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT1[3-6].pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric

df$competenceSat.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT(1[7-9]|20).pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric
df$competenceFrust.pre_calc <-
  psych::scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(matches("^SDT2[1-4].pre")),
    min = 1,
    max = 7
  )$scores %>% 
  as.numeric

df$IntGrAnx.pre_calc <-
  psych::scoreItems(
    keys = c(rep(1,6),rep(-1,6)),
    items = df %>% select(matches("^IntGrAnx[0-9]{2}.?\\.pre")),
    min = 1,
    max = 10
  )$scores %>% 
  as.numeric

df$swl.pre_calc <-
  psych::scoreItems(
    keys = rep(1,5),
    items = df %>% select(matches("^SWL[0-9]{2}\\.pre")),
    min = 1,
    max = 7,
    missing = TRUE,
    impute = "none",
  )$scores %>% 
  as.numeric

df$mdmq.alertness.pre_calc <- 
  psych::scoreItems(
    keys = c(1, -1, -1, 1),
    items = df %>% select(MDMQ02.pre, MDMQ05.pre, MDMQ07.pre, MDMQ10.pre),
    min = 1,
    max = 6,
    missing = TRUE,
    impute = "none",
  )$scores %>% 
  as.numeric
df$mdmq.calmness.pre_calc <- 
  psych::scoreItems(
    keys = c(-1, 1, -1, 1),
    items = df %>% select(MDMQ03.pre, MDMQ06.pre, MDMQ09.pre, MDMQ12.pre),
    min = 1,
    max = 6,
    missing = TRUE,
    impute = "none",
  )$scores %>% 
  as.numeric
df$mdmq.valence.pre_calc <- 
  psych::scoreItems(
    keys = c(1, -1, 1, -1),
    items = df %>% select(MDMQ01.pre, MDMQ04.pre, MDMQ08.pre, MDMQ11.pre),
    min = 1,
    max = 6,
    missing = TRUE,
    impute = "none",
  )$scores %>% 
  as.numeric

df$IntGrAnx.post_calc <-
  psych::scoreItems(
    keys = c(rep(1,6),rep(-1,6)),
    items = df %>% select(matches("^IntGrAnx[0-9]{2}.?\\.post")),
    min = 1,
    max = 10
  )$scores %>% 
  as.numeric

df$swl.post_calc <-
  psych::scoreItems(
    keys = rep(1,5),
    items = df %>% select(matches("^SWL[0-9]{2}\\.post")),
    min = 1,
    max = 7,
    missing = TRUE,
    impute = "none",
  )$scores %>% 
  as.numeric

varNamIndicesS1 <-
  append(varNamIndicesS1,
         df %>% select(ends_with("_calc")) %>% names)


#
df <- df %>%
  mutate(
    exWB = scales::rescale(exWB, from = range(exWB, na.rm = TRUE), to = c(-100, 100)),
    relatedness = relatedness + 50,
    autonomy = autonomy + 50,
    competence = competence + 50,
    KeyNeedFulfillment = KeyNeedFulfillment + 50,
    KeyNeedDueToPartner = KeyNeedDueToPartner + 50,
    InteractionContextAccidental = InteractionContextAccidental + 50,
    InteractionContextvoluntary = InteractionContextvoluntary + 50,
    InteractionContextCooperative = InteractionContextCooperative + 50,
    InteractionContextRepresentativeNL = InteractionContextRepresentativeNL + 50,
    qualityOverall = qualityOverall + 50,
    qualityMeaning = qualityMeaning + 50,
    DaytimeNeedFulfillment = DaytimeNeedFulfillment + 50
  )
dtS1$full <- df
rm(df)
```

### Study 2

For the student sample, the data transformation stage had five main aims:

1.  We first create person, survey type, and measurement ID variables.\
2.  We then created indices of scales. Some indices were multi-item scales while some indices combine equivalent measurement for different situational circumstances (e.g., competence perceptions after interactions and at measurement occasions without interactions).\
3.  We add information about the interaction partner to the beep during which a person was selected as an interaction partner.\
4.  We cluster mean-center key variables within participants.\
5.  Finally, we calculated several basic participant summaries (averages across all measurement occasions).

```{r newVarsStudents}
df <- dtS2$clean

# Add ID variables
df$PID <- as.numeric(factor(df$ResponseId)) # participant ID

# Correct date_period ()
df <- df %>%
  mutate(
    startDate = as.Date(created),
    startTime = format(as.POSIXct(created), format = "%H:%M:%S"),
    date_period_old = date_period,
    date_period = ifelse(
      startTime >= c("12:00") & startTime <= c("18:59"),
      paste(format(as.POSIXct(created), format = "%Y-%m-%d"), "Morning"),
      ifelse(
        startTime >= c("19:00") & startTime <= c("23:59"),
        paste(format(as.POSIXct(created), format = "%Y-%m-%d"), "Afternoon"),
        ifelse(
          startTime >= c("00:00") & startTime <= c("11:59"),
          paste(as.Date(created) - 1, "Afternoon"),
          NA
        )
      )
    )
  )
  
# order time
# df$TID <- factor(df$date_period, levels = unique(dtS3$raw.daily$date_period))
# df$TIDnum <- as.numeric(df$TID) # get numeric TID
df <- df %>%
  mutate(
    dateOrder = stri_replace_all_regex(
      date_period,
      pattern = c(' Morning', ' Afternoon'),
      replacement = c('_A', '_B'),
      vectorize = FALSE
    ),
    TID = factor(date_period),
    TIDnum = as.numeric(factor(dateOrder))
  )

# check whether time ordering worked
df <- df %>%
  arrange(PID, TID) 

# Interaction as Factor
df$interaction.f <-
  factor(df$Interaction,
    levels = c("no interaction", "Dutch", "Non-Dutch")
  )
df$intNL <- ifelse(df$Interaction == "Dutch", 1, 0)
df$intNonNL <- ifelse(df$Interaction == "Non-Dutch", 1, 0)

df$IntergroupContact <- (df$IntergroupContact-2)*-1
df$IngroupContact <- (df$IngroupContact-2)*-1

# -------------------------------------------------------------------------------------------------------------
#                                       Combine Variables
# -------------------------------------------------------------------------------------------------------------
# Relatedness
pairs.panels.new(
  df[c("relatednessSelf", "relatednessOther")],
  labels = c(
    "I shared information about myself.",
    "X shared information about themselves."
  )
)
df$relatednessInteraction <- rowMeans(df[c("relatednessSelf", "relatednessOther")], na.rm = TRUE)
df$relatednessInteraction[df$relatednessInteraction == "NaN"] <- NA
# Relatedness Overall (JANNIS NOT SURE THESE ARE CORRECT, CHANGE ROWS?; JK: Changed "NaN" in df$RelatednessInteraction to NA() should work now)
df$relatedness <-
  rowMeans(df[, c("relatednessInteraction", "relatednessNoInteraction")],
           na.rm = TRUE) %>%
  ifelse(is.nan(.), NA, .)

# Core Need
df$DaytimeNeedFulfillment[df$InteractionDum == 0 & !is.na(df$KeyNeedFulfillment)] <-
  df$KeyNeedFulfillment[df$InteractionDum == 0 & !is.na(df$KeyNeedFulfillment)]

df$InteractionNeedFullfillment <- NA 
df$InteractionNeedFullfillment[df$InteractionDum == 1 & !is.na(df$KeyNeedFulfillment)] <- 
  df$KeyNeedFulfillment[df$InteractionDum == 1 & !is.na(df$KeyNeedFulfillment)]


# Randomly selected variables
df <- df %>%
  rowwise() %>%
  mutate(
    ProSo = mean(c(ProSo1, ProSo2, ProSo3, ProSo4), na.rm = TRUE),
    AntiSo = mean(c(AntiSo1, AntiSo2, AntiSo3, AntiSo4, AntiSo5, AntiSo6, AntiSo7), na.rm = TRUE)
  )

# -------------------------------------------------------------------------------------------------------------
#                                 Add Variables related to interaction partner
# -------------------------------------------------------------------------------------------------------------
# create function for later lapply
createIntPartDf <- function(inp) {
  # prepare the dataframe so that we can forloop over it later
  tmp <- data.frame(
    CC = as.character(inp$CC),
    NewCC = as.character(inp$NewCC),
    NewName = as.character(inp$NewName),
    NewCloseness = inp$NewCloseness,
    NewGender = inp$NewGender,
    NewEthnicity = as.character(inp$NewEthnicity),
    NewRelationship = as.character(inp$NewRelationship)
  )

  tmp$CC2 <- recode(tmp$CC, "SOMEONE ELSE" = "NA")
  tmp$CC2 <-
    ifelse(
      tmp$CC == 1 |
        tmp$CC == "SOMEONE ELSE",
      as.character(tmp$NewName),
      as.character(tmp$CC2)
    )
  # maybe add [[:space:]]\b to remove space before word boundary or ^[[:space:]] to remove space in the beginning of a string
  tmp$CC2 <- gsub("^[[:space:]]", "", tmp$CC2)
  tmp$NewName <- gsub("^[[:space:]]", "", tmp$NewName)

  # open the variables that will be filled up in the foor-loop
  tmp$closeness <- rep(NA, nrow(tmp))
  tmp$gender <- rep(NA, nrow(tmp))
  tmp$ethnicity <- rep(NA, nrow(tmp))
  tmp$relationship <- rep(NA, nrow(tmp))

  # Run the for-loop. It finds the variables related to the name of the interaction partner. If there is a repeating interaction
  # partner (i.e. CC2) it takes the value (i.e. NewCloseness) from the first interaction (i.e. NewName)
  for (i in 1:nrow(tmp)) {
    if (is.na(tmp$CC2[i])) {
      next
    } else {
      tmp$closeness[i] <-
        na.omit(tmp$NewCloseness[as.character(tmp$CC2[i]) == as.character(tmp$NewName)])[1] # find closeness where CC2 matches NewName (na.omit + [1] to get the number)
      tmp$gender[i] <-
        na.omit(tmp$NewGender[as.character(tmp$CC2[i]) == as.character(tmp$NewName)])[1] # (na.omit + [1] to get the number and not the rest of the na.omit list)
      tmp$ethnicity[i] <-
        na.omit(as.character(tmp$NewEthnicity[as.character(tmp$CC2[i]) == as.character(tmp$NewName)]))[1] # PROBLEM IS THAT THERE ARE TOO MANY NA's: Difficult to deal with
      tmp$relationship[i] <-
        na.omit(as.character(tmp$NewRelationship[as.character(tmp$CC2[i]) == as.character(tmp$NewName)]))[1]
    }
  }

  out <- tmp
  out
}

# split df per participants and run function
PP <- split(df, df$PID)
PP <- lapply(PP, createIntPartDf)
rm(createIntPartDf)

# add variables back to df
remergePP <- do.call(rbind.data.frame, PP)
colnames(remergePP) <-
  paste(colnames(remergePP), "_Calc", sep = "")
df <- cbind(df, remergePP)
rm(remergePP, PP)

dtS2$full <- df
rm(df)
```

### Study 3

For the medical professional sample, the data transformation stage had five main aims:

1.  We first create person, survey type, and measurement ID variables.\
2.  We then created indices of scales. Some indices were multi-item scales while some indices combine equivalent measurement for different situational circumstances (e.g., competence perceptions after interactions and at measurement occasions without interactions).\
3.  We cluster mean-center key variables within participants.\
4.  Finally, we calculated several basic participant summaries (averages across all measurement occasions).

```{r newVarsMedical}
df <- dtS3$clean

# Add ID variables
df$PID <- as.numeric(factor(df$ResponseId)) # participant ID

# Correct date_period ()
df <- df %>%
  mutate(
    startDate = as.Date(created),
    startTime = format(as.POSIXct(created), format = "%H:%M:%S"),
    date_period_old = date_period,
    date_period = ifelse(
      startTime >= c("12:00") & startTime <= c("18:59"),
      paste(format(as.POSIXct(created), format = "%Y-%m-%d"), "Morning"),
      ifelse(
        startTime >= c("19:00") & startTime <= c("23:59"),
        paste(format(as.POSIXct(created), format = "%Y-%m-%d"), "Afternoon"),
        ifelse(
          startTime >= c("00:00") & startTime <= c("11:59"),
          paste(as.Date(created) - 1, "Afternoon"),
          NA
        )
      )
    )
  )
  
# order time
# df$TID <- factor(df$date_period, levels = unique(dtS3$raw.daily$date_period))
# df$TIDnum <- as.numeric(df$TID) # get numeric TID
df <- df %>%
  mutate(
    dateOrder = stri_replace_all_regex(
      date_period,
      pattern = c(' Morning', ' Afternoon'),
      replacement = c('_A', '_B'),
      vectorize = FALSE
    ),
    TID = factor(date_period),
    TIDnum = as.numeric(factor(dateOrder))
  )

# check whether time ordering worked
df <- df %>%
  arrange(PID, TID)

# Interaction as Factor
df$interaction.f <-
  factor(df$Interaction,
    levels = c("no interaction", "Dutch", "Non-Dutch")
  )
df$intNL <- ifelse(df$Interaction == "Dutch", 1, 0)
df$intNonNL <- ifelse(df$Interaction == "Non-Dutch", 1, 0)

df <- df %>%
  mutate(
    NonDutchContact = replace_na(NonDutchNum, 0), # make second non-Dutch countable
    NonDutchContact = ifelse(NonDutchContact > 1, 1, 0) # recode (yes = 1 -> 1, no = 2 -> 0)
  ) %>%
  mutate(
    OutgroupInteraction = factor(
      InteractionDumDutch,
      levels = c(0, 1),
      labels = c("No", "Yes")
    ),
    NonOutgroupInteraction = factor(
      rowSums(select(., c(InteractionDumNonDutch, NonDutchContact)), na.rm = TRUE), # combine the two non-Dutch Q.,
      levels = c(0, 1),
      labels = c("No", "Yes")
    )
  )

df$IntergroupContact <- (df$IntergroupContact-2)*-1
df$IngroupContact <- (df$IngroupContact-2)*-1

# -------------------------------------------------------------------------------------------------------------
#                                       Combine Variables
# -------------------------------------------------------------------------------------------------------------
# Relatedness
pairs.panels.new(
  df[c("relatednessSelf", "relatednessOther")],
  labels = c(
    "I shared information about myself.",
    "X shared information about themselves."
  )
)
df$relatednessInteraction <-
  rowMeans(df[c("relatednessSelf", "relatednessOther")], na.rm = TRUE)
df$relatednessInteraction[df$relatednessInteraction == "NaN"] <- NA
# Relatedness Overall (JANNIS NOT SURE THESE ARE CORRECT, CHANGE ROWS?; J: Changed "NaN" in df$RelatednessInteraction to NA() should work now)
df$relatedness <-
  rowMeans(df[, c("relatednessInteraction", "relatednessNoInteraction")],
           na.rm = TRUE) %>%
  ifelse(is.nan(.), NA, .)


df$DaytimeNeedFulfillment[df$InteractionDum == 0 & !is.na(df$KeyNeedFulfillment)] <-
  df$KeyNeedFulfillment[df$InteractionDum == 0 & !is.na(df$KeyNeedFulfillment)]

df$InteractionNeedFullfillment <- NA 
df$InteractionNeedFullfillment[df$InteractionDum == 1 & !is.na(df$KeyNeedFulfillment)] <- 
  df$KeyNeedFulfillment[df$InteractionDum == 1 & !is.na(df$KeyNeedFulfillment)]

df$InteractionNeedImportance <- NA 
df$InteractionNeedImportance[df$InteractionDum == 1 & !is.na(df$KeyNeedImp)] <- 
  df$KeyNeedImp[df$InteractionDum == 1 & !is.na(df$KeyNeedImp)]


# Randomly selected variables
df <- df %>%
  rowwise() %>%
  mutate(
    ProSo = mean(c(ProSo1, ProSo2, ProSo3, ProSo4), na.rm = TRUE),
    AntiSo = mean(c(AntiSo1, AntiSo2, AntiSo3, AntiSo4, AntiSo5, AntiSo6, AntiSo7), na.rm = TRUE),
    agency = mean(c(agency1, agency2, agency3), na.rm = TRUE),
    autoFrust = mean(c(autoFrust1, autoFrust2, autoFrust3, autoFrust4), na.rm = TRUE),
    autoSat = mean(c(autoSat1, autoSat2, autoSat3, autoSat4), na.rm = TRUE),
    relatFrust = mean(c(relatFrust1, relatFrust2, relatFrust3, relatFrust4), na.rm = TRUE),
    relatSat = mean(c(relatSat1, relatSat2, relatSat3, relatSat4), na.rm = TRUE),
    compFrust = mean(c(compFrust1, compFrust2, compFrust3, compFrust4), na.rm = TRUE),
    compSat = mean(c(compSat1, compSat2, compSat3, compSat4), na.rm = TRUE),
    lonely = mean(c(lonely4, lonely4, lonely4, lonely4), na.rm = TRUE),
    emotRegPos = mean(c(emotRegPos01, emotRegPos02), na.rm = TRUE),
    emotRegNeg = mean(c(emotRegNeg01, emotRegNeg02), na.rm = TRUE)
  )


# Allport's Conditions
df %>%
  #filter(OutgroupInteraction == "Yes") %>%
  select(
    InteractionContextEqualStatus,
    KeyNeedShared,
    InteractionContextCooperative,
    InteractionContextvoluntary
  ) %>%
  pairs.panels.new

AlportDescr <- df %>%
  #filter(OutgroupInteraction == "Yes") %>%
  select(
    InteractionContextEqualStatus,
    KeyNeedShared,
    InteractionContextCooperative,
    InteractionContextvoluntary
  ) %>%
  psych::describe(., skew=F,ranges=T) %>%
  as.data.frame() %>%
  select(-vars) %>%
  kable(., caption = "Descriptives of Allport's Condition items") %>% 
  kable_styling("hover", full_width = F, latex_options = "hold_position")



iaWorkerAllport <- 
  df %>%
  #filter(OutgroupInteraction == "Yes") %>%
  select(
    InteractionContextEqualStatus,
    KeyNeedShared,
    InteractionContextCooperative,
    InteractionContextvoluntary
  )

itemScaleAllport01 <- sjPlot::tab_itemscale(iaWorkerAllport)

pca <- parameters::principal_components(iaWorkerAllport)
factor.groups <- parameters::closest_component(pca)

sjPlot::tab_itemscale(iaWorkerAllport, factor.groups, show.kurtosis = TRUE)


AllportAlpha <- ltm::cronbach.alpha(na.omit(iaWorkerAllport), CI = TRUE)


data <- 
  df %>%
  select(
    PID,
    TIDnum,
    InteractionContextEqualStatus,
    KeyNeedShared,
    InteractionContextCooperative,
    InteractionContextvoluntary
  ) %>%
  drop_na %>%
  reshape2::melt(
    ., 
    id.vars = c("PID", "TIDnum")
  )

AllportNestedAlpha <- horst::nestedAlpha(item.level.1 = "value",
                   level.2      = "TIDnum",
                   level.3      = "PID",
                   data         = data)
rm(data)

iaWorkerAllportScale <- 
  iaWorkerAllport %>%
  Scale::Scale() %>%
  Scale::ItemAnalysis()

df$AllportsCondition <-
  scoreItems(
    keys = c(1, 1, 1, 1),
    items = df %>% select(
      InteractionContextEqualStatus,
      KeyNeedShared,
      InteractionContextCooperative,
      InteractionContextvoluntary
    ),
    min = 0,
    max = 100
  )$scores

as.data.frame(psych::describe(df$AllportsCondition, skew=T)) %>%
  mutate(vars = "Allport's Conditions Index") %>%
  kable(., caption = "Allport's Conditions: Scale Descriptives", row.names = FALSE) %>% 
  kable_styling("hover", full_width = F, latex_options = "hold_position")

ggplot(df, aes(x = AllportsCondition)) +
  geom_histogram() +
  theme_Publication()

# -------------------------------------------------------------------------------------------------------------
#                                 Add Variables related to interaction partner
# -------------------------------------------------------------------------------------------------------------
# create function for later lapply
createIntPartDf <- function(inp) {
  # prepare the dataframe so that we can forloop over it later
  tmp <- data.frame(
    CC = as.character(inp$CC),
    NewCC = as.character(inp$NewCC),
    NewName = as.character(inp$NewName),
    NewCloseness = inp$NewCloseness,
    NewGender = inp$NewGender,
    NewEthnicity = as.character(inp$NewEthnicity),
    NewRelationship = as.character(inp$NewRelationship)
  )

  tmp$CC2 <- recode(tmp$CC, "SOMEONE ELSE" = "NA")
  tmp$CC2 <-
    ifelse(
      tmp$CC == 1 |
        tmp$CC == "SOMEONE ELSE",
      as.character(tmp$NewName),
      as.character(tmp$CC2)
    )
  # maybe add [[:space:]]\b to remove space before word boundary or ^[[:space:]] to remove space in the beginning of a string
  tmp$CC2 <- gsub("^[[:space:]]", "", tmp$CC2)
  tmp$NewName <- gsub("^[[:space:]]", "", tmp$NewName)

  # open the variables that will be filled up in the foor-loop
  tmp$closeness <- rep(NA, nrow(tmp))
  tmp$gender <- rep(NA, nrow(tmp))
  tmp$ethnicity <- rep(NA, nrow(tmp))
  tmp$relationship <- rep(NA, nrow(tmp))

  # Run the for-loop. It finds the variables related to the name of the interaction partner. If there is a repeating interaction
  # partner (i.e. CC2) it takes the value (i.e. NewCloseness) from the first interaction (i.e. NewName)
  for (i in 1:nrow(tmp)) {
    if (is.na(tmp$CC2[i])) {
      next
    } else {
      tmp$closeness[i] <-
        na.omit(tmp$NewCloseness[as.character(tmp$CC2[i]) == as.character(tmp$NewName)])[1] # find closeness where CC2 matches NewName (na.omit + [1] to get the number)
      tmp$gender[i] <-
        na.omit(tmp$NewGender[as.character(tmp$CC2[i]) == as.character(tmp$NewName)])[1] # (na.omit + [1] to get the number and not the rest of the na.omit list)
      tmp$ethnicity[i] <-
        na.omit(as.character(tmp$NewEthnicity[as.character(tmp$CC2[i]) == as.character(tmp$NewName)]))[1] # PROBLEM IS THAT THERE ARE TOO MANY NA's: Difficult to deal with
      tmp$relationship[i] <-
        na.omit(as.character(tmp$NewRelationship[as.character(tmp$CC2[i]) == as.character(tmp$NewName)]))[1]
    }
  }

  out <- tmp
  out
}

# split df per participants and run function
PP <- split(df, df$PID)
PP <- lapply(PP, createIntPartDf)
rm(createIntPartDf)

# add variables back to df
remergePP <- do.call(rbind.data.frame, PP)
colnames(remergePP) <-
  paste(colnames(remergePP), "_Calc", sep = "")
df <- cbind(df, remergePP)
rm(remergePP, PP)

dtS3$full <- df
```

## Data Availability and Sample Selection {.tabset .tabset-fade}

As one of our main analyses is a three-mode principal component analysis (3MPCA) we begin by assessing the amount of missing data in each of the ESM studies. To assess the missingness in detail, we prepare a data availability table of whether data is available or missing for each possible measurement point (data + morning/afternoon) for all individual participants. We then export the data availability tables as comma separated value files (.csv) to be assessed in detail using a spreadsheet program (such as MS Excel).

For our sample selection we address both the selection of time points and participants. Given that multiple imputation procedures even work well with large proportions of missing data [assuming missingness at random; @Madley-Dowd2019], we decided on a general criterion of less than 45% missingness to balance sample size retention and bias in the multiple imputation model. Thus, we then select the time points for which we have less than 45% missingness and select participants who have less than 45% missingness across the selected time range.

To approximate an ideal sample selection that maximized the number of participants and timepoints that fit the 55% data availability criterion. We select sequentially remove the rows or columns with the lowest data availability rate untill all rows and columns have more than 55% data available (i.e., less than 45% missingness).

::: {.alert .alert-info .alert-dismissible .fade .in role="alert"}
<a class="close" data-dismiss="alert" aria-label="close">×</a> <i class="fas fa-info-circle"></i> <b>Note: </b><br/> This needs to be adjusted current missingness criterion set to more conservative 33%.
:::

### Study 1

```{r workerDataAvailability}
dtS1Availability <- dtS1$full %>%
  select(
    PID,
    TIDnum
  ) %>%
  arrange(PID, TIDnum) %>%
  mutate(data = 1)
dtS1Availability <- reshape::cast(dtS1Availability, PID ~ TIDnum) %>%
  select(-PID) %>%
  mutate_all(function(x) ifelse(x>1,1,x))
# sum(colMeans(dtS1Availability)*100 >= 66)
# sum(rowMeans(dtS1Availability)*100 >= 66)
# sum(colMeans(dtS1Availability[-5,])*100 >= 66)
# sum(rowMeans(dtS1Availability[,-8])*100 >= 66)

rownames(dtS1Availability) <- paste("PP", 1:nrow(dtS1Availability), sep = "_")
colnames(dtS1Availability) <- paste("t", 1:ncol(dtS1Availability), sep = "_")
dtS1AvailabilityKbl <- dtS1Availability

dtS1AvailabilityKbl[1:ncol(dtS1AvailabilityKbl)] <- lapply(dtS1AvailabilityKbl[1:ncol(dtS1AvailabilityKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS1AvailabilityKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 1: Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")

write.csv(dtS1Availability, "data/S1_Workers/processed/workerAvailability.csv")
```

```{r testOptimizer, include=FALSE}
# set.seed(42)
# nr <- 5
# nc <- 10
# df <- as.data.table(matrix(rbinom(nr*nc,1,.66), nrow=nr, ncol=nc))
# df
# 
# best.list.row.df = list()
# for (i in 1:nrow(df)) {
#   # get best subset for rows based on how many columns have more than 66% data
#   rowlist = combn(nrow(df), i)
#   numobs = apply(rowlist, 2, function(x) sum(colMeans(df[x,])*100 >= 66))
#   cat("For subsets of", i, "rows, the highest number of observations is", max(numobs), "out of the", ncol(df), "maximum. Product =", i*max(numobs),"\n")
#   best = which(numobs == max(numobs))[1]
#   best.list.row.df = c(best.list.row.df, list(rowlist[, best]))
# }
# 
# best.list.col.df = list()
# for (i in 1:ncol(df)) {
#   # get best subset for columns based on how many rows have more than 66% data
#   collist = combn(ncol(df), i)
#   numobs = apply(collist, 2, function(x) sum(rowMeans(df[, ..x])*100 >= 66))
#   cat("For subsets of", i, "columns, the highest number of participants is", max(numobs), "out of the", nrow(df), "maximum. Product =", i*max(numobs),"\n")
#   best = which(numobs == max(numobs))[1]
#   best.list.col.df = c(best.list.col.df, list(collist[, best]))
# }
# rm(df, best.list.row.df, best.list.col.df)
```

```{r testOptimizer03, include=FALSE}
# library(dplyr)
# library(ROI)
# library(ROI.plugin.glpk)
# library(ompr)
# library(ompr.roi)
# 
# set.seed(42)
# tf <- matrix(sample(c(TRUE, FALSE), 1488, replace=TRUE), 31)
# M3 <- t(replicate(31, 1969:2016, simplify=TRUE))
# M3[tf] <- NA
# 
# m <- +!is.na(M3) # gets logical matrix; 0 if NA else 1    
# #m <- as.data.frame(dtS1AvailabilityTest)
# nr <- nrow(m)
# nc <- ncol(m)
# n_years <- 15 
# 
# 
# model <- MIPModel() %>% 
#   # keep[i,j] is 1 if matrix cell [i,j] is to be kept else 0
#   add_variable(keep[i,j], i = 1:nr, j = 1:nc, typ = "binary") %>% 
#   # rm_row[i] is 1 if row i is selected for removal else 0
#   add_variable(rm_row[i], i = 1:nr, type = "binary") %>% 
#   # rm_col[j] is 1 if column j is selected for removal else 0
#   add_variable(rm_col[j], j = 1:nc, type = "binary") %>% 
#   # maximize good cells kept
#   set_objective(sum_expr(keep[i,j], i = 1:nr, j = 1:nc), "max") %>% 
#   # cell can be kept only when row is not selected for removal
#   add_constraint(sum_expr(keep[i,j], j = 1:nc) <= 1 - rm_row[i], i = 1:nr) %>%
#   # cell can be kept only when column is not selected for removal
#   add_constraint(sum_expr(keep[i,j], i = 1:nr) <= 1 - rm_col[j], j = 1:nc) %>%
#   # keep at most n_years columns i.e. remove at least (nc - n_years) columns
#   # only non-NA values can be kept
#   add_constraint(m[i,j] + rm_row[i] + rm_col[j] >= 1, i = 1:nr, j = 1:nc) %>% 
#   #add_constraint(sum_expr(m[,j], j = 1:nc)/nc*100 >= 66) %>% 
#   #add_constraint(sum_expr(m[i,], i = 1:nr)/nr*100 >= 66) %>% 
#   # keep cols with more 
#   #add_constraint(colMeans(m[i,j], i = 1:nr, j = 1:nc) >= 0.66) %>%
#   # I used >= instead of == to avoid infeasiblity
#   #add_constraint(sum_expr(rm_col[j], j = 1:nc) >= nc - n_years) %>% 
#   # solve using free glpk solver
#   solve_model(with_ROI(solver = "glpk", verbose = TRUE))
# 
# solver_status(model)
# rm_rows <- model %>% 
#   get_solution(rm_row[i]) %>% 
#   filter(value > 0) %>% 
#   pull(i)
# 
# rm_cols <- model %>% 
#   get_solution(rm_col[j]) %>% 
#   filter(value > 0) %>% 
#   pull(j) 
# 
# result <- m[-rm_rows, -rm_cols, drop = F]
# result
```

```{r testOptimizer05}
cleanM <- function(M, c = 55) { #c = 66
  check <- list()
  rowNamesOut <- c()
  colNamesOut <- c()
  for (i in 1:length(c(row.names(M), names(M)))) {
    rowMeans <- rowMeans(M) * 100
    colMeans <- colMeans(M) * 100
    rcMeans <- c(rowMeans, colMeans)
    rm <- which.min(rcMeans) %>% names
    rc <- ifelse(startsWith(rm, "PP_"), "row", "col")
    
    check[[i]] <- rcMeans
    
    if (!all(rcMeans >= c) && rc == "row") {
      M <- M[!(row.names(M) %in% rm), ]
      rowNamesOut <- append(rowNamesOut, rm)
      cat(i, ": Row ", rm, " had a completion rate of ", format(round(min(rcMeans), 2), nsmall = 2), "% and was removed.\n", sep = "")
    } else if (!all(rcMeans >= c) && rc == "col") {
      M <- M[, !(names(M) %in% rm)]
      colNamesOut <- append(colNamesOut, rm)
      cat(i, ": Column ", rm, " had a completion rate of ", format(round(min(rcMeans), 2), nsmall = 2), "% and was removed.\n", sep = "")
    } else {
      cat(i, ": All row- and column means are over ", c, "%. The final matrix has ", nrow(M), " rows and ", ncol(M), " columns.", sep = "")
      return(
        list(
          reducedMatrix = M,
          rowNamesIn = row.names(M),
          colNamesIn = names(M),
          rowNamesOut = rowNamesOut,
          colNamesOut = colNamesOut,
          rowMeans = rowMeans,
          colMeans = colMeans
        )
      )
    }
  }
}

dtS1RedInfo <- cleanM(M = as.data.frame(dtS1Availability), c = 55)

PIDout <- gsub("PP_", "", dtS1RedInfo$rowNamesOut) %>% as.numeric
TIDout <- gsub("t_", "", dtS1RedInfo$colNamesOut) %>% as.numeric
TIDInRed <- gsub("t_", "", dtS1RedInfo$colNamesIn) %>% as.numeric
TIDIn <- seq(min(TIDInRed), max(TIDInRed), 1)

dtS1Red <- dtS1$full %>%
  filter(
    !PID %in% PIDout,
    TIDnum %in% TIDIn
  ) %>% 
  mutate(TIDnum = TIDnum - min(TIDnum))
rm(PIDout, TIDout, TIDInRed, TIDIn)

# change glitch where survey was sent too early
dtS1Red$TIDnum[dtS1Red$PID == 7 & as.character(dtS1Red$created) == "2018-05-18 11:46:40"] <- 10
dtS1Red$TID[dtS1Red$PID == 7 & as.character(dtS1Red$created) == "2018-05-18 11:46:40"] <- "2018-05-18 Morning"

dtS1AvailabilityRedKbl <- dtS1RedInfo$reducedMatrix
dtS1AvailabilityRedKbl[1:ncol(dtS1AvailabilityRedKbl)] <- lapply(dtS1AvailabilityRedKbl[1:ncol(dtS1AvailabilityRedKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS1AvailabilityRedKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 1: Final Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
```

### Study 2

```{r studentDataAvailability}
dtS2Availability <- dtS2$full %>%
  select(
    PID,
    TIDnum
  ) %>%
  #na.omit %>%
  arrange(PID, TIDnum) %>%
  mutate(data = 1)
dtS2Availability <- reshape::cast(dtS2Availability, PID ~ TIDnum) %>%
  select(-PID)

rownames(dtS2Availability) <- paste("PP", 1:nrow(dtS2Availability), sep = "_")
colnames(dtS2Availability) <- paste("t", 1:ncol(dtS2Availability), sep = "_")

write.csv(dtS2Availability, "data/S2_Students/processed/studentAvailability.csv")

dtS2AvailabilityKbl <- dtS2Availability
dtS2AvailabilityKbl[1:ncol(dtS2AvailabilityKbl)] <- lapply(dtS2AvailabilityKbl[1:ncol(dtS2AvailabilityKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS2AvailabilityKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 2: Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
```

```{r s2SampleReduction}
dtS2RedInfo <- cleanM(M = as.data.frame(dtS2Availability), c = 55)

PIDout <- gsub("PP_", "", dtS2RedInfo$rowNamesOut) %>% as.numeric
TIDout <- gsub("t_", "", dtS2RedInfo$colNamesOut) %>% as.numeric
TIDInRed <- gsub("t_", "", dtS2RedInfo$colNamesIn) %>% as.numeric
TIDIn <- seq(min(TIDInRed), max(TIDInRed), 1)

dtS2Red <- dtS2$full %>%
  filter(
    !PID %in% PIDout,
    TIDnum %in% TIDIn
  ) %>% 
  mutate(TIDnum = TIDnum - min(TIDnum))
rm(PIDout, TIDout, TIDInRed, TIDIn)

# remove glitch entries 
dtS2Red <- dtS2Red[!(dtS2Red$PID == 46 & dtS2Red$TIDnum == 57 & dtS2Red$ended == "2018-12-20 21:58:31"),]
dtS2Red <- dtS2Red[!(dtS2Red$PID == 46 & dtS2Red$TIDnum == 59 & dtS2Red$ended == "2018-12-20 21:58:31"),]

dtS2AvailabilityRedKbl <- dtS2RedInfo$reducedMatrix
dtS2AvailabilityRedKbl[1:ncol(dtS2AvailabilityRedKbl)] <- lapply(dtS2AvailabilityRedKbl[1:ncol(dtS2AvailabilityRedKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS2AvailabilityRedKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 2: Final Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
```

### Study 3

```{r medicalDataAvailability}
dtS3Availability <- dtS3$full %>%
  select(
    PID,
    TIDnum
  ) %>%
  arrange(PID, TIDnum) %>%
  mutate(data = 1)
dtS3Availability <- reshape::cast(dtS3Availability, PID ~ TIDnum, fill = 0) %>%
  select(-PID)

rownames(dtS3Availability) <- paste("PP", 1:nrow(dtS3Availability), sep = "_")
colnames(dtS3Availability) <- paste("t", 1:ncol(dtS3Availability), sep = "_")

write.csv(dtS3Availability, "data/S3_Medical/processed/medicalAvailability.csv")

dtS3AvailabilityKbl <- dtS3Availability
dtS3AvailabilityKbl[1:ncol(dtS3AvailabilityKbl)] <- lapply(dtS3AvailabilityKbl[1:ncol(dtS3AvailabilityKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS3AvailabilityKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 2: Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
rm(dtS3AvailabilityKbl)
```

```{r s3SampleReduction}
dtS3RedInfo <- cleanM(M = as.data.frame(dtS3Availability), c = 55)

PIDout <- gsub("PP_", "", dtS3RedInfo$rowNamesOut) %>% as.numeric
TIDout <- gsub("t_", "", dtS3RedInfo$colNamesOut) %>% as.numeric
TIDInRed <- gsub("t_", "", dtS3RedInfo$colNamesIn) %>% as.numeric
TIDIn <- seq(min(TIDInRed), max(TIDInRed), 1)

dtS3Red <- dtS3$full %>%
  filter(
    !PID %in% PIDout,
    TIDnum %in% TIDIn
  ) %>% 
  mutate(TIDnum = TIDnum - min(TIDnum))
rm(PIDout, TIDout, TIDInRed, TIDIn)

dtS3AvailabilityRedKbl <- dtS3RedInfo$reducedMatrix
dtS3AvailabilityRedKbl[1:ncol(dtS3AvailabilityRedKbl)] <- lapply(dtS3AvailabilityRedKbl[1:ncol(dtS3AvailabilityRedKbl)], function(x) {
    cell_spec(x,
              bold = FALSE,
              color = "white",
              background = ifelse(x == 1, "green", "red")
              )
})
kbl(
  dtS3AvailabilityRedKbl,
  format = "html",
  escape = FALSE,
  align = "c",
  booktabs = TRUE,
  caption = "Study 3: Final Data Availability" # complete caption for main document
) %>%
  kable_classic(
    full_width = FALSE,
    lightable_options = "hover",
    html_font = "Cambria"
  ) %>%
  scroll_box(width = "100%", height = "500px")
```

## Time Scales and Timepoint Aggregation

```{r checkVarAvailability}
varNamesPre <- varNames %>%
  filter(
    surveyS1 == "pre",
    surveyS2 == "pre",
    str_detect(surveyS3, "^pre")
  ) %>%
  select(varNam)
varNamesPreFull <-
  data.frame(varNam = Reduce(intersect, list(
    names(dtS1$full), names(dtS3$full), names(dtS3$full)
  ))) %>%
  filter(str_detect(varNam, ".pre$"))

varNamesDaily <- varNames %>%
  filter(
    surveyS1 == "daily",
    surveyS2 == "daily",
    surveyS3 == "daily"
  ) %>%
  select(varNam)
varNamesDailyFull <-
  data.frame(varNam = Reduce(intersect, list(
    names(dtS1Red), names(dtS2Red), names(dtS3Red)
  ))) %>%
  filter(!str_detect(varNam, ".pre$|.post$"))

varNamesPost <- varNames %>%
  filter(
    surveyS1 == "post",
    surveyS2 == "post",
    surveyS3 == "post"
  ) %>%
  select(varNam)
varNamesPostFull <-
  data.frame(varNam = Reduce(intersect, list(
    names(dtS1Red), names(dtS3Red), names(dtS3Red)
  ))) %>%
  filter(str_detect(varNam, ".post$"))
```

```{r analysisDFs}
# Main Analysis: 3MPCA (common variables across studies)
dropVarPcaMain <- c(
  # Meta Data
  "last_outside_referrer",
  "created",
  "ended",
  "ip_address",
  "date",
  "ResponseId",
  "server_language",
  "Meta_Browser",
  # Interaction Dummies (non-continuous)
  "IntergroupContact",
  "IngroupContact",
  "InteractionDum",
  # Interaction Counts (non-continuous)
  "ContactNum",
  "NonDutchNum",
  "groupDutch",
  # Interaction Descriptives (non-continuous)
  "duration",
  "dyadGroup",
  "groupSize",
  # Keyneed Freetexts (non-continuous)
  "KeyNeed",
  "DaytimeNeed",
  # Relatedness individual items
  "relatednessSelf",
  "relatednessOther",
  "relatednessNoInteraction"
)

varNam3mpcaMain <- varNamesDailyFull %>%
  filter(!varNam %in% dropVarPcaMain) %>%
  pull

dtMain3mpca <- rbind(
  dtS1Red %>% select(any_of(varNam3mpcaMain)) %>% mutate(study = "S1"),
  dtS3Red %>% select(any_of(varNam3mpcaMain)) %>% mutate(study = "S2"),
  dtS3Red %>% select(any_of(varNam3mpcaMain)) %>% mutate(study = "S3")
  ) %>%
  group_by(study, PID) %>%
  mutate(ID = cur_group_id()) %>%
  ungroup %>%
  mutate(
    date = as.Date(gsub(" .*", "", TID)),
    week = strftime(date, format = "%Y-W%V")
    ) %>%
  select(
    ID,
    PID,
    TID,
    date,
    week,
    TIDnum,
    study,
    everything()
  ) %>%
  arrange(ID, TIDnum)

dtMain3mpcaDaily <- dtMain3mpca %>%
  group_by(ID, date, study) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup %>%
  group_by(study) %>%
  mutate(TIDnum = as.numeric(factor(date))) %>%
  ungroup %>%
  select(ID, date, TIDnum, everything()) %>%
  mutate_all(~ifelse(is.nan(.), NA, .))

dtMain3mpcaWeekly <- dtMain3mpca %>%
  group_by(ID, week, study) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup %>%
  group_by(study) %>%
  mutate(TIDnum = as.numeric(factor(week))) %>%
  ungroup %>%
  select(ID, week, TIDnum, everything()) %>%
  mutate_all(~ifelse(is.nan(.), NA, .))
```

We aggregate the key variables over time to archive a reasonably interpretable number of time points and remove a first proportion of missing data. Given that little data is available on the meaningful time scales of the selected psychological variables, we chose to determine the appropriate time scales using variance decomposition [e.g., see @Ram2014]. This is to say that we create multi-level unconditional means models (without predictors) that include possible nested time scales as levels. We chose to select time scales that align with common human cycles. We thus compare the variances of bi-daily, daily, and weekly aggregations. Additional aggregations of two weeks or the full four weeks might be possible but would most likely reduce the variance too much for any meaningul further reduction during the 3MPCA. We then chose the time scales that have the most variance.

```{r keyNeedTest}

# WILL USE IMPUTED DATA AND FOR-LOOP ACROSS ALL RELEVANT DVs
# PLOT EFFECT SIZES

dtMain3mpca <- dtMain3mpca %>%
  group_by(study) %>%
  mutate(
    TIDdate = as.numeric(factor(date)),
    TIDweek = as.numeric(factor(week))
  ) %>%
  ungroup

# Not sure whether TIDs as predictors or empty model?
keyNeedMlTest01 <-
  lme4::lmer(KeyNeedFulfillment ~ 1 + TIDnum + TIDdate + TIDweek + (1 + TIDnum + TIDdate + TIDweek | ID),
    data = dtMain3mpca
  ) 

keyNeedMlTest02 <-
  lme(
    KeyNeedFulfillment ~ 1 + TIDnum + TIDdate + TIDweek,
    random = ~ 1 + TIDnum + TIDdate + TIDweek | ID,
    data = dtMain3mpca %>% filter(!is.na(KeyNeedFulfillment)),
    control = list(opt = "nlmimb")
  ) # use optim if it does not converge

# summary(keyNeedMlTest02)
# summ(keyNeedMlTest01, digits = 3, center = FALSE)
# anova(keyNeedMlTest02)

```

# **Feature-based Time-Series Clustering**

Feature-based clustering has the advantage of being extremely flexible when it comes to dealing with real-world human data. Human data is commonly noisy, incomplete, and non-stationary. Many descriptive dimension-reduction, modeling, and raw time-series clustering procedures will require complete or stationary data from the researcher, and are thus currently not well-suited to deal with messy intensive-longitudinal data collected by many social science researchers. 

We used the following five-step procedure to arrive at a stable, reproducible, and meaningful differentiation of participants ESM trajectories:

1. Variable selection
2. Feature extraction
3. (Feature selection)
4. Clustering
5. Visualization, interpretation, and validation

## Variable Selection

We begin by selecting the variables, which we would like to include in the clustering procedure. For the clustering of time-series characteristics we focus on variables that are specific to the ESM measurements (i.e., variables collected during the various daily dairy pings). This means, that variables from the pre- and post-test are excluded from the clustering procedure. While the variables can in theory be added as additional features, they are not the focus of this manuscript. 

In our case, we begin by collecting the variables that are in the ESM measurements of all three studies, in order to build the most powerful dataset for the main analysis. We then additionally, divide the variables by their response type and by study to check for cluster differences in the validation phase:

1. *Main Analysis*: Variables across all three data sets
2. *Validation*: Variables by response type (interaction-specific or not)
3. *Validation*: Variables by study

```{r Preparation of Variable Names}
varNamS123Aux <- varNames %>%
  filter(
    aux != 0,
    studyS1 == "S1",
    studyS2 == "S2",
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull
varNamS1Aux <- varNames %>%
  filter(
    aux != 0,
    studyS1 == "S1"
  ) %>%
  select(varNam) %>%
  pull
varNamS2Aux <- varNames %>%
  filter(
    aux != 0,
    studyS2 == "S2"
  ) %>%
  select(varNam) %>%
  pull
varNamS3Aux <- varNames %>%
  filter(
    aux != 0,
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull

varNamS123PCA <- varNames %>%
  filter(
    pca != 0,
    studyS1 == "S1",
    studyS2 == "S2",
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull
varNamS1PCA <- varNames %>%
  filter(
    pca != 0,
    studyS1 == "S1"
  ) %>%
  select(varNam) %>%
  pull
varNamS2PCA <- varNames %>%
  filter(
    pca != 0,
    studyS2 == "S2"
  ) %>%
  select(varNam) %>% 
  pull
varNamS3PCA <- varNames %>%
  filter(
    pca != 0,
    studyS3 == "S3"
  ) %>%
  select(varNam) %>%
  pull

varNamOut <- c(
  "ResponseId",
  "relatednessNoInteraction",
  "relatednessSelf",
  "relatednessOther",
  "autonomy_Int",
  "autonomy_NoInt",
  "competence_Int",
  "competence_NoInt",
  varNamS1PCA[grepl('^MDMQ', varNamS1PCA)],
  varNamS2PCA[grepl('^ProSo|^AntiSo', varNamS2PCA)],
  varNamS3PCA[grepl(
    '^ProSo|^AntiSo|^agency|^autoFrust|^autoSat|^relatFrust|^relatSat|^compFrust|^compSat|^lonely[0-9]|^emotRegPos|^emotRegNeg',
    varNamS3PCA
  )]
)
varNamCore <- c(
  "PID",
  "TID",
  "TIDnum"
)
varNamNewAll <- c(
  "relatedness",
  "autonomy",
  "competence"
)
varNamNewS1 <- c(
  "relatedness",
  "autonomy",
  "competence",
  "alertness", 
  "calmness", 
  "valence"
)
varNamNewS2 <- c(
  "relatedness",
  "autonomy",
  "competence",
  "ProSo", 
  "AntiSo"
)
varNamNewS3 <- c(
  "relatedness",
  "autonomy",
  "competence",
  "ProSo", 
  "AntiSo",
  "agency",
  "autoFrust",
  "autoSat",
  "relatFrust",
  "relatSat",
  "compFrust",
  "compSat",
  "lonely",
  "emotRegPos",
  "emotRegNeg"
)

varNamNewCat <- c(
  "closeness_Calc",
  "gender_Calc",
  "ethnicity_Calc",
  "relationship_Calc"
)

varNamIntDep <- c(
  varNamS123PCA[grepl('^InteractionContext|AttitudesPartner|KeyNeedDueToPartner|^quality', varNamS123PCA)]
)

varNamIndiceItemsS1 <- varNames %>%
  filter(
    aux == -1,
    studyS1 == "S1"
  ) %>%
  select(varNam) %>%
  pull %>%
  append(., gsub(".pre|.post|_calc", "", varNamIndicesS1)) %>%
  unique

varNamS123MI <- c(varNamCore, varNamNewAll, varNamS123Aux)
varNamS1MI <- c(varNamCore, varNamIndicesS1, varNamS1Aux[!varNamS1Aux %in% varNamIndiceItemsS1]) #
varNamS2MI <- c(varNamCore, varNamNewS2, varNamS2Aux)
varNamS3MI <- c(varNamCore, varNamNewS3, varNamS3Aux)

varNamS123PCA <- c(varNamCore, varNamS123PCA[!varNamS123PCA %in% varNamOut])
varNamS1PCA <- c(varNamCore, varNamNewS1, varNamS1PCA[!varNamS1PCA %in% varNamOut])
varNamS2PCA <- c(varNamCore, varNamNewS2, varNamS2PCA[!varNamS2PCA %in% varNamOut])
varNamS3PCA <- c(varNamCore, varNamNewS3, varNamS3PCA[!varNamS3PCA %in% varNamOut])

varNamS123MIPsbl <- c(
  paste(varNamS123MI, rep(".pre", length(varNamS123MI)), sep = ""),
  varNamS123MI,
  paste(varNamS123MI, rep(".post", length(varNamS123MI)), sep = "")
)

varNamS123MiRed <- Reduce(
  intersect,
  list(
    dtS1Red %>% select(PID, TID, TIDnum, any_of(varNamS123MIPsbl)) %>% names,
    dtS2Red %>% select(PID, TID, TIDnum, any_of(varNamS123MIPsbl)) %>% names,
    dtS3Red %>% select(PID, TID, TIDnum, any_of(varNamS123MIPsbl)) %>% names
  )
)

idVars <- c("ID", "PID", "TID", "TIDnum", "date", "week", "study")
```

```{r variable names for all analyses}
# ID Variables (just to re-iterate)
idVars <- idVars

# Common variables across studies 
varNamS123 <- varNamS123PCA[!varNamS123PCA %in% idVars]

# Variable Groups
# varNames %>%
#   filter(pca != 0) %>%
#   mutate(contactSpecific = recode(.$contactSpecific, 
#                          `-1`="No Interaction Only",
#                          `0`="unspecific",
#                          `1`="Interaction Only")) %>%
#   crosstable::crosstable(., varGroup, by=contactSpecific) %>%
#   select(-c(.id, label)) %>%
#   kbl(.,
#       escape = FALSE,
#       booktabs = T,
#       align = c("l", "r", "c"),
#       digits=2,
#       caption = "Full Analysis: Variable Groups by Interaction Types") %>%
#   kable_classic(
#     full_width = F,
#     lightable_options = "hover",
#     html_font = "Cambria"
#   )
# 
# varNames %>%
#   filter(
#     pca != 0
#   ) %>%
#   mutate(contactSpecific = recode(.$contactSpecific, 
#                          `-1`="No Interaction Only",
#                          `0`="unspecific",
#                          `1`="Interaction Only")) %>%
#   select(varNam, varGroup, contactSpecific) %>%
#   group_by(varGroup, varNam, contactSpecific) %>%
#   summarise(n = n()) %>%
#   pivot_wider(names_from = c("varGroup", "varNam"), values_from = n, names_sep=" - ") %>%
#   ungroup %>%
#   t %>%
#   as.data.frame %>%
#   janitor::row_to_names(1) %>%
#   replace(is.na(.), 0) %>%
#   tibble::rownames_to_column(., var = "variable") %>%
#   separate(variable, c("concept", "variable"), " - ") %>%
#   arrange(concept, unspecific, `Interaction Only`) %>%
#   mutate_all(~recode(.x, `0`="", ` 1`="✓")) %>%
#   kbl(.,
#       escape = FALSE,
#       booktabs = T,
#       align = c("l", "l", rep("c", ncol(.)-2)),
#       digits=2,
#       caption = "All Possible Variables by Interaction Types") %>%
#   kable_classic(
#     full_width = F,
#     lightable_options = "hover",
#     html_font = "Cambria"
#   ) %>%
#   scroll_box(width = "100%", height = "500px")

# Non-Interaction Specific Variables
varNamS123NoInt <- varNames %>%
  filter(
    varNam %in% varNamS123PCA,
    contactSpecific == 0
  ) %>%
  select(varNam) %>%
  pull

# Interaction Specific Variables
varNamS123Int <- varNames %>%
  filter(
    varNam %in% varNamS123PCA,
    contactSpecific == 1
  ) %>%
  select(varNam) %>%
  pull

# S1 variables
varNamS1Clust <- varNamS1PCA[!varNamS1PCA %in% idVars]

# S1 Non-Interaction Specific Variables
varNamS1NoInt <- varNames %>%
  filter(
    varNam %in% varNamS1Clust,
    contactSpecific == 0
  ) %>%
  select(varNam) %>%
  pull

# S1 Interaction Specific Variables
varNamS1Int <- varNames %>%
  filter(
    varNam %in% varNamS1Clust,
    contactSpecific == 1
  ) %>%
  select(varNam) %>%
  pull


# S2 variables
varNamS2Clust <- varNamS2PCA[!varNamS2PCA %in% idVars]

# S2 Non-Interaction Specific Variables
varNamS2NoInt <- varNames %>%
  filter(
    varNam %in% varNamS2Clust,
    contactSpecific == 0
  ) %>%
  select(varNam) %>%
  pull

# S2 Interaction Specific Variables
varNamS2Int <- varNames %>%
  filter(
    varNam %in% varNamS2Clust,
    contactSpecific == 1
  ) %>%
  select(varNam) %>%
  pull

# S3 variables
varNamS3Clust <- varNamS3PCA[!varNamS3PCA %in% idVars]

# S3 Non-Interaction Specific Variables
varNamS3NoInt <- varNames %>%
  filter(
    varNam %in% varNamS3Clust,
    contactSpecific == 0
  ) %>%
  select(varNam) %>%
  pull

# S3 Interaction Specific Variables
varNamS3Int <- varNames %>%
  filter(
    varNam %in% varNamS3Clust,
    contactSpecific == 1
  ) %>%
  select(varNam) %>%
  pull
```

We also see that a number of variables are specific to interactions. These variables will by their very nature have substantially larger missingness (because not every measurement occasion will follow new social interactions) and these variables are also un-imputable in traditional time-series modeling and hypothesis testing procedures. 

```{r variable names by response type}
varNames %>%
  filter(
    pca != 0,
    varNam %in% c(varNamS123PCA, varNamS1PCA, varNamS2PCA, varNamS3PCA)
  ) %>%
  mutate(contactSpecific = recode(.$contactSpecific, 
                         `-1`="No Interaction Only",
                         `0`="unspecific",
                         `1`="Interaction Only")) %>%
  select(varNam, varGroup, contactSpecific) %>%
  group_by(varGroup, varNam, contactSpecific) %>%
  summarise(n = n()) %>%
  pivot_wider(names_from = c("varGroup", "varNam"), values_from = n, names_sep=" - ") %>%
  ungroup %>%
  t %>%
  as.data.frame %>%
  janitor::row_to_names(1) %>%
  replace(is.na(.), 0) %>%
  tibble::rownames_to_column(., var = "variable") %>%
  separate(variable, c("concept", "variable"), " - ") %>%
  arrange(concept, unspecific, `Interaction Only`) %>%
  mutate_all(~recode(.x, `0`="", ` 1`="✓")) %>%
  kbl(.,
      escape = FALSE,
      booktabs = T,
      align = c("l", "l", rep("c", ncol(.)-2)),
      digits=2,
      caption = "Variables by Interaction Types") %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

Please note that not all variables are included in the main analysis, as some variables are only found within specific studies.

## Feature Extraction

Time-series can be described with a plethora of features. A good example is the list of 100 time-series features that one can extract with [tsfresh python package](https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html# "list of features to extract"). In practice there are usually two steps to the feature engineering process. (1) *Feature extraction*, that is summarizing the time series in meaningful descriptors, and (2) *feature selection*, the process of ensuring that only meaningful features are included in the clustering step. For the interpretability of our clustering procedure we have made a pre-selection of features, we wish to include to describe our time-series: 

1. central tendency
    + mean ✓
    + median ✓
2. variation around the central tendency (i.e., density)
    + SD ✓
    + MAD ✓
3. (in)stability
    + root mean square of successive differences (rmssd) ✓
    + mean absolute change (mac) ✓
4. inertia
    + lag-1 autocorrelation ✓
    + periodicity (lag-2 & lag-14 autocorrelation) ✓
5. linear trend
    + OLS regression parameter ✓
6. non-linear trend
    + GAM edf ✓
    
(!discuss NA procedures!)

```{r load saved and prepared features}
load("data/features/featOut.RData")
```

### Across Studies

We begin by extracting the features for the main analysis that combines the sample of the three studies. We do this across all selected variables as well as for the content (non-)specific variables separately. During the feature extraction step there can at times be features that cannot be calculated for a small number of participants. An example would a participant who never filled in two consecutive measurements that describe an interaction. In that case, we cannot extract an auto correlation of lag-1 for some of the interaction specific variables.

```{r Feature Extraction Main Analysis}
# varNames
#items <- varNamS123PCA[!varNamS123PCA %in% idVars]

# Joined Dataset of all three studies
featData <- rbind(
  dtS1Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S1") %>%
    mutate(across(!TID & !study, as.numeric)),
  dtS2Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S2"),
  dtS3Red %>% select(any_of(varNamS123MiRed)) %>% mutate(study = "S3")
) %>%
  group_by(study, PID) %>%
  mutate(ID = cur_group_id()) %>%
  ungroup %>%
  mutate(date = as.Date(gsub(" .*", "", TID)),
         week = strftime(date, format = "%Y-W%V")) %>%
  select(ID,
         PID,
         study,
         date,
         week,
         TIDnum,
         any_of(varNamS123PCA)) %>%
   mutate(
     across(all_of(varNamS123PCA), ~ lag(.x, n = 1), .names = "{col}_lag1"),
     across(all_of(varNamS123PCA), ~ lag(.x, n = 2), .names = "{col}_lag2"),
     across(all_of(varNamS123PCA), ~ lag(.x, n = 14), .names = "{col}_lag14")
   ) %>%
  arrange(ID, TIDnum) %>%
  select_if(~ sum(!is.na(.)) > 1) %>% # only include variables that have any data (i.e., not all NA)
  as.data.frame


# Don't re-run unless you have a few hours to spare!
# featFull <-
#   featureExtractor(
#     data = featData,
#     pid = "ID",
#     tid = "TIDnum",
#     items = varNamS123
#   )
# 
# featFullContact <-
#   featureExtractor(
#     data = featData,
#     pid = "ID",
#     tid = "TIDnum",
#     items = varNamS123Int
#   )
# 
# featFullNoContact <-
#   featureExtractor(
#     data = featData,
#     pid = "ID",
#     tid = "TIDnum",
#     items = varNamS123NoInt
#   )
# 
# 
# featFull$features <- featFull$features %>%
#     filter(
#       ID != 19
#     )


pMissFull <- pMissFeat(
  all = featFull$features,
  contact = featFullContact$features,
  nocontact = featFullNoContact$features,
  title = "Feature-wise Missingess Across all Studies"
)

pMissFull$pMissPlot

# featFullImp <- featureImputer(featFull)
# featFullNoContactImp <- featureImputer(featFullNoContact)
# featFullContactImp <- featureImputer(featFullContact)

```

For the full dataset across the three studies, we find that about `r format(round(pMissFull$pmiss %>% filter(set == "All") %>% select(pMissTotal) %>% pull, 2), nsmall=2)`\% of the extracted features are missing across the `r nrow(featFull$features)` participants and `r ncol(featFull$features)` features. These missing features are imputed with a single predictive mean matching imputation using the MICE library. 

### Study 1

We then repeat the feature extraction procedure for all selected variables that are specific to Study 1.

```{r Feature Extraction Study 1 specific variables}
# varNames
#items <- varNamS1Clust

# Prepare Dataframe
featDataS1 <- 
  dtS1Red %>%
  mutate(date = as.Date(gsub(" .*", "", TID)),
         week = strftime(date, format = "%Y-W%V")) %>%
  select(PID,
         date,
         week,
         TIDnum,
         any_of(varNamS1Clust)) %>%
   mutate(
     across(all_of(varNamS1Clust), ~ lag(.x, n = 1), .names = "{col}_lag1"),
     across(all_of(varNamS1Clust), ~ lag(.x, n = 2), .names = "{col}_lag2"),
     across(all_of(varNamS1Clust), ~ lag(.x, n = 14), .names = "{col}_lag14")
   ) %>%
  arrange(PID, TIDnum) %>%
  select_if(~ sum(!is.na(.)) > 1) %>% # only include variables that have any data (i.e., not all NA)
  as.data.frame

# Don't re-run unless you have a few hours to spare!
# featS1 <-
#   featureExtractor(
#     data = featDataS1,
#     pid = "PID",
#     tid = "TIDnum",
#     items = varNamS1Clust
#   )
# 
# featS1Contact <-
#   featureExtractor(
#     data = featDataS1,
#     pid = "PID",
#     tid = "TIDnum",
#     items = varNamS1Int
#   )
# 
# featS1NoContact <-
#   featureExtractor(
#     data = featDataS1,
#     pid = "PID",
#     tid = "TIDnum",
#     items = varNamS1NoInt
#   )

pMissS1 <- pMissFeat(
  all = featS1$features,
  contact = featS1Contact$features,
  nocontact = featS1NoContact$features,
  title = "Feature-wise Missingess Study 1"
)

pMissS1$pMissPlot

# featS1Imp <- featureImputer(featS1)
# featS1NoContactImp <- featureImputer(featS1NoContact)
# featS1ContactImp <- featureImputer(featS1Contact)

```

### Study 2

In the same manner, the features are extracted for the variables that are specific to Study 2.

```{r Feature Extraction Study 2 specific variables}
# varNames
#items <- varNamS2Clust

# Prepare Dataframe
featDataS2 <- 
  dtS2Red %>%
  mutate(date = as.Date(gsub(" .*", "", TID)),
         week = strftime(date, format = "%Y-W%V")) %>%
  select(PID,
         date,
         week,
         TIDnum,
         any_of(varNamS2Clust)) %>%
   mutate(
     across(all_of(varNamS2Clust), ~ lag(.x, n = 1), .names = "{col}_lag1"),
     across(all_of(varNamS2Clust), ~ lag(.x, n = 2), .names = "{col}_lag2"),
     across(all_of(varNamS2Clust), ~ lag(.x, n = 14), .names = "{col}_lag14")
   ) %>%
  arrange(PID, TIDnum) %>%
  select_if(~ sum(!is.na(.)) > 1) %>% # only include variables that have any data (i.e., not all NA)
  as.data.frame

# Don't re-run unless you have a few hours to spare!
# featS2 <-
#   featureExtractor(
#     data = featDataS2,
#     pid = "PID",
#     tid = "TIDnum",
#     items = varNamS2Clust
#   )
# 
# featS2Contact <-
#   featureExtractor(
#     data = featDataS2,
#     pid = "PID",
#     tid = "TIDnum",
#     items = varNamS2Int
#   )
# 
# featS2NoContact <-
#   featureExtractor(
#     data = featDataS2,
#     pid = "PID",
#     tid = "TIDnum",
#     items = varNamS2NoInt
#   )

pMissS2 <- pMissFeat(
  all = featS2$features,
  contact = featS2Contact$features,
  nocontact = featS2NoContact$features,
  title = "Feature-wise Missingess Study 2"
)

pMissS2$pMissPlot

# featS2Imp <- featureImputer(featS2)
# featS2NoContactImp <- featureImputer(featS2NoContact)
# featS2ContactImp <- featureImputer(featS2Contact)
```

### Study 3

And finally, we extract the features for the Study 3 specific variables.

```{r Feature Extraction Study 3 specific variables}
# varNames
#items <- varNamS3Clust

# Prepare Dataframe
featDataS3 <- 
  dtS3Red %>%
  mutate(date = as.Date(gsub(" .*", "", TID)),
         week = strftime(date, format = "%Y-W%V")) %>%
  select(PID,
         date,
         week,
         TIDnum,
         any_of(varNamS3Clust)) %>%
   mutate(
     across(all_of(varNamS3Clust), ~ lag(.x, n = 1), .names = "{col}_lag1"),
     across(all_of(varNamS3Clust), ~ lag(.x, n = 2), .names = "{col}_lag2"),
     across(all_of(varNamS3Clust), ~ lag(.x, n = 14), .names = "{col}_lag14")
   ) %>%
  arrange(PID, TIDnum) %>%
  select_if(~ sum(!is.na(.)) > 1) %>% # only include variables that have any data (i.e., not all NA)
  as.data.frame

# Don't re-run unless you have a few hours to spare!
# featS3 <-
#   featureExtractor(
#     data = featDataS3,
#     pid = "PID",
#     tid = "TIDnum",
#     items = varNamS3Clust
#   )
# 
# featS3Contact <-
#   featureExtractor(
#     data = featDataS3,
#     pid = "PID",
#     tid = "TIDnum",
#     items = varNamS3Int
#   )
# 
# featS3NoContact <-
#   featureExtractor(
#     data = featDataS3,
#     pid = "PID",
#     tid = "TIDnum",
#     items = varNamS3NoInt
#   )

pMissS3 <- pMissFeat(
  all = featS3$features,
  contact = featS3Contact$features,
  nocontact = featS3NoContact$features,
  title = "Feature-wise Missingess Study 3"
)
pMissS3$pMissPlot

# featS3Imp <- featureImputer(featS3)
# featS3NoContactImp <- featureImputer(featS3NoContact)
# featS3ContactImp <- featureImputer(featS3Contact)
# 
# save(list = ls(pattern = "feat"), file = "data/features/featOut.RData")
```


## Feature Selection

<div class="alert alert-warning alert-dismissible fade in" role="alert">
  <a class="close" data-dismiss="alert" aria-label="close">&times;</a>
  <i class="fas fa-exclamation-triangle"></i> <b>Note: </b><br/>
  Here we might have to make a selection of the features we extract because some of the clustering algorithms might not be able to deal with the many different features. See below
</div>

## Clustering

Cluster analyses have been a staple within a variety of sciences for the better part of a century. It's applications are wide and because most procedures fall under the "unsupervised learning" umbrella the methodological developments are fast-paced and new methods or refinements are published constantly. Among the many options available four types are found most commonly within the modern literature on clustering analyses:

1. centroid-based
    + e.g., k-means
    + no assumptions (+)
    + fast even with large matrices (+)
    + non-probabilistic (+/-)
    + sensitive to local minima (-)
2. distribution-based
    + e.g., gaussian mixture model (GMM)
    + flexible shape (+)
    + probabilistic (+/-)
    + (gaussian) distributions assumed (-)
3. density-based
    + e.g., dbscan / OPTICS
    + no shape assumption (+)
    + does not assign outliers/noise (+)
4. Hierarchical-based (not relevant)
    + e.g., agglomerative hierarchical clustering
    + best with hierarchical data (-)
    + best with small *n* (-)

Each clustering procedures comes with their own sets of advantages and disadvantages. We will focus mainly on the k-means procedure because it has the least assumptions and is fast even with large data sets. Additionally, the methods are well-established and the results are easy to interpret. As such, the procedure should be applicable to most users within the social sciences and for ESM feature data specifically. We offer Gaussian mixture modeling and a common density-based approach as supplementary analyses. 

### Main Anlaysis: Across Studies

The main clustering procedure, uses the features calculated for the relevant variables across all three studies.

#### Centroid-based

For our main method, we use the centroid-based k-means clustering. The procedure needs the researcher to make a selection on the optimal number of clusters in order to run the main algorithm. We use the Elbow and the Silhouette method, which both suggest two clusters to be the optimal solution.

```{r kmeans determin k}
# prepare data for main analysis
raw_data <- featData
z_data <- featFullImp$featuresImpZ
scaled_data <- featFullImp$featuresImpZMat

# load libraries
library(factoextra)
library(NbClust)

# Elbow method
fviz_nbclust(scaled_data, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = 2)+
  labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(scaled_data, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
# Gap statistic [takes for ever]
# nboot = 50 to keep the function speedy. 
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
# set.seed(123)
# fviz_nbclust(scaled_data, kmeans, nstart = 100,  method = "gap_stat", nboot = 500)+
#   labs(subtitle = "Gap statistic method")
```

We use this parameter to run the actual k-means cluster analysis, during which the algorithm iteratively seeks to optimally separate the participants into two clusters based on their feature values. During the iterative process the algorithm might at times stop at a non-optimal solution (i.e., local minima). To avoid this drawback and to find the (globally) optimal solution we let the algorithm run with 100 separate staring positions.

```{r k-means modeling based on selected k}
kmeansOut <- kmeans(scaled_data, centers = 2, nstart = 100)
```

Because the clustering happens in a multidimensional space of the `r ncol(featFull$features)` features, we begin by visualizing the data in a two-dimensional space, where we encircle the points of the clusters (i.e., convex hulls) for the first two dimensions of a principle component analysis. 

```{r Plot Convex Hulls}
fviz_cluster(kmeansOut, geom = "point", data = scaled_data, ggtheme = theme_Publication())
```

We then inspect the cluster-averages of all feature for each variable. We (1) see that for some variables the features are generally stronger in separating the clusters (e.g., how cooperative an interaction was) than for other variables (e.g., Attitudes towards the Dutch). Additionally, we see that (2) some within the variables some features are better at distinguishing clusters (e.g., mean and median of well-being) than others (e.g., sd and rmssd of well-being).

```{r features for each variable}
data.frame(ID = as.numeric(names(kmeansOut$cluster)), cluster = kmeansOut$cluster) %>%
  merge(., z_data) %>%
  group_by(cluster) %>%
  summarize_all(mean) %>%
  ungroup %>%
  select(-ID) %>% 
  reshape2::melt(id.vars = "cluster") %>% 
  mutate(lab = variable) %>%
  mutate(
    lab = variable,
    variable = gsub("\\_.*", "", lab),
    feature = gsub("^.*?\\_", "", lab)
  ) %>% 
  ggplot(., aes(x = feature, y = value, group = cluster, color = as.factor(cluster))) +
  geom_point() +
  geom_line() +
  coord_flip() +
  labs(
    x = "Features",
    y = "Standardized Mean",
    color = "Cluster"
  ) +
  facet_wrap(~ variable) +
  theme_Publication()
```

Next, we inspect the cluster-averages of all variables for each feature. This is the same data as in the plot above. However, here we see more clearly that some features are better at distinguishing the clusters across variables (e.g., mean and median) than others (e.g., auto correlations).

```{r variables for each feature}
data.frame(ID = as.numeric(names(kmeansOut$cluster)), cluster = kmeansOut$cluster) %>%
  merge(., z_data) %>%
  group_by(cluster) %>%
  summarize_all(mean) %>%
  ungroup %>%
  select(-ID) %>% 
  reshape2::melt(id.vars = "cluster") %>% 
  mutate(
    lab = variable,
    variable = gsub("\\_.*", "", lab),
    feature = gsub("^.*?\\_", "", lab)
  ) %>% 
  ggplot(., aes(x = variable, y = value, group = cluster, color = as.factor(cluster))) +
  geom_point() +
  geom_line() +
  coord_flip() +
  labs(
    x = "Features",
    y = "Standardized Mean",
    color = "Cluster"
  ) +
  facet_wrap(~ feature) +
  theme_Publication()
```

In a final step we visualize the raw time series for each variable and add the average cluster development to see the features in action.

```{r time-series by variable and cluster}
data.frame(ID = as.numeric(names(kmeansOut$cluster)), cluster = kmeansOut$cluster) %>%
  merge(., raw_data) %>%
  select(
    ID,
    TIDnum,
    cluster,
    any_of(varNamS123)
  ) %>%
  reshape2::melt(id.vars = c("cluster", "ID","TIDnum")) %>% 
  ggplot(., aes(x=TIDnum, y=value, group=cluster, color=as.factor(cluster))) +
  geom_line(aes(x=TIDnum, y=value, group=ID, color=as.factor(cluster)), alpha=0.05) +
  stat_summary(fun=mean, geom="line") +
  facet_wrap(~ variable, scales = "free_y") +
  labs(
    x = "Time ID",
    y = "Response (Mean Cluster Response Bold)",
    color = "Cluster"
  ) +
  theme_Publication()
```

#### Distribution-based

For the distribution-based clustering we use the commonly used Gaussian mixture modeling (GMM) procedure. This procedure assumes  independent Gaussian distributions for each each cluster. These distributions are substantially better at identifying clusters if the data comes from multivariate Gaussian distributions (i.e., ellipses in a two dimensional space). 

We again begin by determining the optimal number of clusters. For GMMs this is done using common information criteria, such as the BIC.

<div class="alert alert-warning alert-dismissible fade in" role="alert">
  <a class="close" data-dismiss="alert" aria-label="close">&times;</a>
  <i class="fas fa-exclamation-triangle"></i> <b>Note: </b><br/>
  This is where we run into trouble. These plots are relatively uncommon and according to the internet this is most likely a problem of too many (non-distinguishing or highly correlated) features. A good paper on this is <a href="https://proceedings.neurips.cc/paper/2002/file/e58aea67b01fa747687f038dfde066f6-Paper.pdf">Law et al. (2002)</a>.
</div>

```{r gaussian mixture model}
library(mclust)

scaled_data <- featFullImp$featuresImpZMat

# scaled_data <- featS1Imp$featuresImpZMat  %>% #select(starts_with(varNamS123Int))
#   select(-any_of(names(which(colSums(is.na(.)) > 0))))


BIC <- mclustBIC(scaled_data)
plot(BIC)
summary(BIC)
mod1 <- Mclust(scaled_data)
summary(mod1, parameters = FALSE)


# multivariate <- densityMclust(scaled_data, plot=FALSE)
# summary(multivariate)            
# plot(multivariate, what = "BIC")
  
# merge(featOutZRowNam, as.data.frame(mod1$classification), by="row.names") %>% View


#table(class, mod1$classification)

#mod1dr <- MclustDR(mod1)
#summary(mod1dr, parameters = FALSE)
#plot(mod1dr, what = "classification")
```

<div class="alert alert-warning alert-dismissible fade in" role="alert">
  <a class="close" data-dismiss="alert" aria-label="close">&times;</a>
  <i class="fas fa-exclamation-triangle"></i> <b>Note: </b><br/>
  The feature selection issue is further highlighted when we do the GMMs by feature. Where we find a reasonable number of clusters for each feature. 
</div>

```{r MClust feature specific cluster numbers}
nMClust <- list()
for(f in unique(gsub("^.*?\\_", "", names(featFullImp$featuresImpZMat)))){
  scaled_data <- featFullImp$featuresImpZMat %>% select(ends_with(f))
  mod1 <- Mclust(scaled_data)
  #summary(mod1, parameters = FALSE)
  nMClust[[f]] <- mod1$G
}

nMClust %>%
  unlist %>%
  data.frame(feature=names(.), G=., row.names=NULL) %>%
  kbl(.,
      escape = FALSE,
      booktabs = T,
      align = c("l", "c"),
      digits=2,
      col.names = c(
        "Feature",
        "Number of Clusters"
      ),
      caption = "Number of MClust Clusters if separated by Feature") %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```


#### Density-based

For the density-based cluster analysis we focus on the *DBSCAN* algorithm ('Density-Based Spatial Clustering of Applications with Noise') as well as its extension the *OPTICS* procedure ('Ordering Points To Identify the Clustering Structure'). This procedure is more bottom up than the previous two approaches. Instead of providing the number of clusters, a cluster is specified if a minimum number of other points (the MinPts parameter) are within a given radius (the epsilon parameter). Together these two parameters determine a density at which a group of data points are considered a cluster. While this description is grossly oversimplified, it highlights one of the advantages of the procedures, namely that dense regions can expand to be any shape in the d-dimensional space.

<div class="alert alert-warning alert-dismissible fade in" role="alert">
  <a class="close" data-dismiss="alert" aria-label="close">&times;</a>
  <i class="fas fa-exclamation-triangle"></i> <b>Note: </b><br/>
  Same feature selection issue as above. 
</div>

```{r dbscan}
library(dbscan)
library(fpc)

#scaled_data <- featFullImp$featuresImpZMat %>% select(starts_with(varNamS123Int))
scaled_data <- featFullImp$featuresImpZMat  # %>% select(starts_with(varNamS123NoInt))#%>%
  #select(-any_of(names(which(colSums(is.na(scaled_data)) > 0))))
  
#is.na(scaled_data) %>% table

# to plot the eps values
eps_plot = kNNdistplot(scaled_data, k=3)
# to draw an optimum line
eps_plot %>% abline(h = 15, lty = 2)


potentialMinPts <- round(SciViews::ln(nrow(scaled_data)))
eps <- 15
minPts <- 2

res.fpc <- fpc::dbscan(scaled_data, eps=eps, MinPts=minPts, scale=FALSE, showplot=0)
res.db <- dbscan::dbscan(scaled_data, eps=eps, minPts=minPts)

resEqual.check <- all(res.fpc$cluster == res.db$cluster)
nClustDBSCAN <- length(unique(res.fpc$cluster))-1

fviz_cluster(res.fpc, scaled_data, geom = "point", ggtheme = theme_Publication())
#hullplot(scaled_data, res.fpc)

```


<div class="alert alert-warning alert-dismissible fade in" role="alert">
  <a class="close" data-dismiss="alert" aria-label="close">&times;</a>
  <i class="fas fa-exclamation-triangle"></i> <b>Note: </b><br/>
  And again the number of clusters is reasonable if we consider the features separately: 
</div>

```{r dbscan feature specific cluister numbers}
minPts <- 3
xi <- 0.04 #0.05
nDbscan <- list()
for(f in unique(gsub("^.*?\\_", "", names(featFullImp$featuresImpZMat)))){
  scaled_data <- featFullImp$featuresImpZMat %>% select(ends_with(f))
  #res.fpc <- dbscan::dbscan(scaled_data, eps=eps_cl, MinPts=minPts)
  
  res.optics <- dbscan::optics(scaled_data, minPts = 3)
  
  #plot(res.optics, col = "grey")
  #dend <- as.dendrogram(res.optics)
  #plot(dend, ylab = "Reachability dist.", leaflab = "none")
  #extractDBSCAN(res.optics, eps_cl = 19.12)
  
  res <- extractXi(res.optics, xi = xi)
  res$clusters_xi
  #plot(res)
  #hullplot(scaled_data, res)
  
  #summary(res.optics, parameters = FALSE)
  nDbscan[[f]] <- nrow(res$clusters_xi)
}

nDbscan %>%
  unlist %>%
  data.frame(feature=names(.), G=., row.names=NULL) %>%
  kbl(.,
      escape = FALSE,
      booktabs = T,
      align = c("l", "c"),
      digits=2,
      col.names = c(
        "Feature",
        "Number of Clusters"
      ),
      caption = "Number of OPTICS Clusters if separated by Feature") %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```


### Follow-up: By Response Type

_**currently removed for faster render until feature selection issue is resolved**_


### Follow-up: By Study

_**currently removed for faster render until feature selection issue is resolved**_


# **Software Information**

The full session information with all relevant system information and all loaded and installed packages is available in the collapsible section below.

<details>

<summary>

System Info

</summary>

\renewcommand{\arraystretch}{0.8}

<!-- decrease line spacing for the table -->

```{r Reproducibility-sessionInfo-R-environment, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width='100%', results='asis'}
df_session_platform <- devtools::session_info()$platform %>%
  unlist(.) %>%
  as.data.frame(.) %>%
  rownames_to_column(.)

colnames(df_session_platform) <- c("Setting", "Value")

kbl(
  df_session_platform,
  booktabs = T,
  align = "l",
  caption = "R environment session info for reproducibility of results" # complete caption for main document
) %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

\renewcommand{\arraystretch}{1}

<!-- reset row height/line spacing -->

</details>

<br>

<details>

<summary>

Package Info

</summary>

\renewcommand{\arraystretch}{0.6}

<!-- decrease line spacing for the table -->

```{r Reproducibility-sessionInfo-R-packages, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", out.width='100%', results='asis'}
df_session_packages <- devtools::session_info()$packages %>%
  as.data.frame(.) %>%
  filter(attached == TRUE) %>%
  dplyr::select(loadedversion, date, source) %>%
  rownames_to_column()

colnames(df_session_packages) <- c("Package", "Loaded version", "Date", "Source")

kbl(
  df_session_packages,
  booktabs = T,
  align = "l",
  caption = "Package info for reproducibility of results" # complete caption for main document
) %>%
  kable_classic(
    full_width = F,
    lightable_options = "hover",
    html_font = "Cambria"
  )
```

\renewcommand{\arraystretch}{1}

<!-- reset row height/line spacing -->

</details>

<br>

<details>

<summary>

Full Session Info (including loaded but unattached packages --- for troubleshooting only)

</summary>

`r pander(sessionInfo(), compact = FALSE)`

</details>

</br>

------------------------------------------------------------------------

</br>

# **References**

::: {.tocify-extend-page data-unique="tocify-extend-page" style="height: 0;"}
:::
