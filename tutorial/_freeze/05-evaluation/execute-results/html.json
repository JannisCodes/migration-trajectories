{
  "hash": "5a9d0113e5e19cc4c0dd49624b046666",
  "result": {
    "markdown": "---\ntitle: \"5. Cluster Evaluation\"\nbibliography: [packages.bib, ../referencesZotero.bib]\ncsl: ../apa.csl\n---\n\n\n\n\nNow that the participants have been assigned to their respective clusters based on the similarity of their time series features, the final evaluation step includes two main elements, (1) evaluating the performance of the clustering analyses to choose an optimal solution and (2) interpreting the extracted clusters conceptually. \n\n## Performance\n\nIn the performance evaluation step, we are mainly concerned with finding the optimal clustering solution. In the previous step, we extracted 9 possible solutions that might all be good at separating the participants. We thus have to evaluate and compare the performance of each solution. In practical terms, performance evaluation often means assessing the accuracy, stability, and separation or purity of the clustering [@keogh2003]. In our own illustration example, we used the `cluster.stats()` function from the `fpc` package, which calculates a wide variety of internal cluster validity statistics for each of the extracted clustering solutions. \n\n\n::: {#tbl-cluster-performance .cell tbl-cap='Cluster Performance for k = 2â€“10'}\n\n```{.r .numberLines .lineAnchors .cell-code}\nkmeans_performance <- list()\nfor (i in 2:10) {\n  kmeans_performance[[i - 1]] <-\n    fpc::cluster.stats(\n      d = dist(pca_scores),\n      clustering = kmeans_results[[i - 1]]$cluster,\n      G2 = TRUE,\n      G3 = TRUE,\n      aggregateonly = TRUE\n    ) %>% unlist\n}\ndf_kmeans_performance <-\n  data.frame(Reduce(rbind, kmeans_performance))\n\nkmeans_metrics_names <- c(\n  \"n\" = \"number of cases\", \n  \"cluster.number\" = \"number of points\", \n  \"min.cluster.size\" = \"size of smallest cluster\", \n  \"noisen\" = \"number of noise points\", # Not relevant for kmeans \n  \"average.between\" = \"average distance between clusters\", \n  \"average.within\" = \"average distance within clusters\", # (reweighted so that every observation, rather than every distance, has the same weight)\n  \"max.diameter\" = \"maximum cluster diameter\",\n  \"min.separation\" = \"minimum cluster separation\", \n  \"ave.within.cluster.ss\" = \"within clusters sum of squares\", # generalisation\n  \"avg.silwidth\" = \"average silhouette width\", \n  \"g2\" = \"Goodman Kruskal's Gamma coefficient\", # See Milligan and Cooper (1985), Gordon (1999, p. 62). \n  \"g3\" = \"G3 coefficient\", # See Gordon (1999, p. 62)\n  \"pearsongamma\" = \"Normalized gamma\", # correlation between distances and a 0-1-vector where 0 means same cluster, 1 means different clusters. see Halkidi et al. (2001)\n  \"dunn\" = \"Dunn index\", # minimum separation / maximum diameter, Dunn index, see Halkidi et al. (2002).\n  \"dunn2\" = \"Dunn index 2\", # minimum average dissimilarity between two cluster / maximum average within cluster dissimilarity, another version of the family of Dunn indexes\n  \"entropy\" = \"entropy of distribution of cluster memberships\", # see Meila(2007)\n  \"wb.ratio\" = \"average within / average between\", \n  \"ch\" = \"Calinski and Harabasz index\", # (Calinski and Harabasz 1974, optimal in Milligan and Cooper 1985; generalised for dissimilarites in Hennig and Liao 2013)\n  \"widestgap\" = \"widest within-cluster gap\", \n  \"sindex\" = \"separation index\"\n)\n\ndf_kmeans_performance %>%\n  kbl(.,\n      escape = FALSE,\n      booktabs = TRUE,\n      align = \"c\",\n      digits = 2,\n      row.names = FALSE,\n      col.names = kmeans_metrics_names) %>%\n  kable_classic(\n    full_width = F,\n    lightable_options = \"hover\",\n    html_font = \"Cambria\"\n  ) %>%\n  scroll_box(width = \"100%\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 5px; overflow-x: scroll; width:100%; \"><table class=\" lightable-classic lightable-hover\" style=\"font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> number of cases </th>\n   <th style=\"text-align:center;\"> number of points </th>\n   <th style=\"text-align:center;\"> size of smallest cluster </th>\n   <th style=\"text-align:center;\"> number of noise points </th>\n   <th style=\"text-align:center;\"> average distance between clusters </th>\n   <th style=\"text-align:center;\"> average distance within clusters </th>\n   <th style=\"text-align:center;\"> maximum cluster diameter </th>\n   <th style=\"text-align:center;\"> minimum cluster separation </th>\n   <th style=\"text-align:center;\"> within clusters sum of squares </th>\n   <th style=\"text-align:center;\"> average silhouette width </th>\n   <th style=\"text-align:center;\"> Goodman Kruskal's Gamma coefficient </th>\n   <th style=\"text-align:center;\"> G3 coefficient </th>\n   <th style=\"text-align:center;\"> Normalized gamma </th>\n   <th style=\"text-align:center;\"> Dunn index </th>\n   <th style=\"text-align:center;\"> Dunn index 2 </th>\n   <th style=\"text-align:center;\"> entropy of distribution of cluster memberships </th>\n   <th style=\"text-align:center;\"> average within / average between </th>\n   <th style=\"text-align:center;\"> Calinski and Harabasz index </th>\n   <th style=\"text-align:center;\"> widest within-cluster gap </th>\n   <th style=\"text-align:center;\"> separation index </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 156 </td>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 76 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 10.98 </td>\n   <td style=\"text-align:center;\"> 9.98 </td>\n   <td style=\"text-align:center;\"> 19.77 </td>\n   <td style=\"text-align:center;\"> 4.70 </td>\n   <td style=\"text-align:center;\"> 51.80 </td>\n   <td style=\"text-align:center;\"> 0.09 </td>\n   <td style=\"text-align:center;\"> 0.26 </td>\n   <td style=\"text-align:center;\"> 0.36 </td>\n   <td style=\"text-align:center;\"> 0.21 </td>\n   <td style=\"text-align:center;\"> 0.24 </td>\n   <td style=\"text-align:center;\"> 1.10 </td>\n   <td style=\"text-align:center;\"> 0.69 </td>\n   <td style=\"text-align:center;\"> 0.91 </td>\n   <td style=\"text-align:center;\"> 16.38 </td>\n   <td style=\"text-align:center;\"> 12.48 </td>\n   <td style=\"text-align:center;\"> 5.54 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 156 </td>\n   <td style=\"text-align:center;\"> 3 </td>\n   <td style=\"text-align:center;\"> 41 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 10.91 </td>\n   <td style=\"text-align:center;\"> 9.70 </td>\n   <td style=\"text-align:center;\"> 18.93 </td>\n   <td style=\"text-align:center;\"> 4.89 </td>\n   <td style=\"text-align:center;\"> 48.47 </td>\n   <td style=\"text-align:center;\"> 0.08 </td>\n   <td style=\"text-align:center;\"> 0.32 </td>\n   <td style=\"text-align:center;\"> 0.31 </td>\n   <td style=\"text-align:center;\"> 0.26 </td>\n   <td style=\"text-align:center;\"> 0.26 </td>\n   <td style=\"text-align:center;\"> 1.04 </td>\n   <td style=\"text-align:center;\"> 1.08 </td>\n   <td style=\"text-align:center;\"> 0.89 </td>\n   <td style=\"text-align:center;\"> 13.96 </td>\n   <td style=\"text-align:center;\"> 12.48 </td>\n   <td style=\"text-align:center;\"> 5.40 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 156 </td>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 21 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 10.92 </td>\n   <td style=\"text-align:center;\"> 9.54 </td>\n   <td style=\"text-align:center;\"> 18.54 </td>\n   <td style=\"text-align:center;\"> 5.15 </td>\n   <td style=\"text-align:center;\"> 46.58 </td>\n   <td style=\"text-align:center;\"> 0.08 </td>\n   <td style=\"text-align:center;\"> 0.40 </td>\n   <td style=\"text-align:center;\"> 0.26 </td>\n   <td style=\"text-align:center;\"> 0.30 </td>\n   <td style=\"text-align:center;\"> 0.28 </td>\n   <td style=\"text-align:center;\"> 0.93 </td>\n   <td style=\"text-align:center;\"> 1.32 </td>\n   <td style=\"text-align:center;\"> 0.87 </td>\n   <td style=\"text-align:center;\"> 11.68 </td>\n   <td style=\"text-align:center;\"> 12.48 </td>\n   <td style=\"text-align:center;\"> 5.51 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 156 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 10.95 </td>\n   <td style=\"text-align:center;\"> 9.44 </td>\n   <td style=\"text-align:center;\"> 18.00 </td>\n   <td style=\"text-align:center;\"> 4.89 </td>\n   <td style=\"text-align:center;\"> 45.05 </td>\n   <td style=\"text-align:center;\"> 0.08 </td>\n   <td style=\"text-align:center;\"> 0.45 </td>\n   <td style=\"text-align:center;\"> 0.24 </td>\n   <td style=\"text-align:center;\"> 0.33 </td>\n   <td style=\"text-align:center;\"> 0.27 </td>\n   <td style=\"text-align:center;\"> 0.80 </td>\n   <td style=\"text-align:center;\"> 1.41 </td>\n   <td style=\"text-align:center;\"> 0.86 </td>\n   <td style=\"text-align:center;\"> 10.28 </td>\n   <td style=\"text-align:center;\"> 13.34 </td>\n   <td style=\"text-align:center;\"> 5.39 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 156 </td>\n   <td style=\"text-align:center;\"> 6 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 10.80 </td>\n   <td style=\"text-align:center;\"> 9.32 </td>\n   <td style=\"text-align:center;\"> 18.00 </td>\n   <td style=\"text-align:center;\"> 4.69 </td>\n   <td style=\"text-align:center;\"> 43.67 </td>\n   <td style=\"text-align:center;\"> 0.06 </td>\n   <td style=\"text-align:center;\"> 0.44 </td>\n   <td style=\"text-align:center;\"> 0.24 </td>\n   <td style=\"text-align:center;\"> 0.28 </td>\n   <td style=\"text-align:center;\"> 0.26 </td>\n   <td style=\"text-align:center;\"> 0.77 </td>\n   <td style=\"text-align:center;\"> 1.68 </td>\n   <td style=\"text-align:center;\"> 0.86 </td>\n   <td style=\"text-align:center;\"> 9.37 </td>\n   <td style=\"text-align:center;\"> 13.34 </td>\n   <td style=\"text-align:center;\"> 5.26 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 156 </td>\n   <td style=\"text-align:center;\"> 7 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 10.77 </td>\n   <td style=\"text-align:center;\"> 9.23 </td>\n   <td style=\"text-align:center;\"> 18.00 </td>\n   <td style=\"text-align:center;\"> 5.03 </td>\n   <td style=\"text-align:center;\"> 42.48 </td>\n   <td style=\"text-align:center;\"> 0.06 </td>\n   <td style=\"text-align:center;\"> 0.45 </td>\n   <td style=\"text-align:center;\"> 0.23 </td>\n   <td style=\"text-align:center;\"> 0.28 </td>\n   <td style=\"text-align:center;\"> 0.28 </td>\n   <td style=\"text-align:center;\"> 0.74 </td>\n   <td style=\"text-align:center;\"> 1.84 </td>\n   <td style=\"text-align:center;\"> 0.86 </td>\n   <td style=\"text-align:center;\"> 8.67 </td>\n   <td style=\"text-align:center;\"> 13.34 </td>\n   <td style=\"text-align:center;\"> 5.31 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 156 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 10.74 </td>\n   <td style=\"text-align:center;\"> 9.12 </td>\n   <td style=\"text-align:center;\"> 18.00 </td>\n   <td style=\"text-align:center;\"> 4.69 </td>\n   <td style=\"text-align:center;\"> 41.31 </td>\n   <td style=\"text-align:center;\"> 0.05 </td>\n   <td style=\"text-align:center;\"> 0.48 </td>\n   <td style=\"text-align:center;\"> 0.22 </td>\n   <td style=\"text-align:center;\"> 0.27 </td>\n   <td style=\"text-align:center;\"> 0.26 </td>\n   <td style=\"text-align:center;\"> 0.73 </td>\n   <td style=\"text-align:center;\"> 1.99 </td>\n   <td style=\"text-align:center;\"> 0.85 </td>\n   <td style=\"text-align:center;\"> 8.19 </td>\n   <td style=\"text-align:center;\"> 13.34 </td>\n   <td style=\"text-align:center;\"> 5.15 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 156 </td>\n   <td style=\"text-align:center;\"> 9 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 10.71 </td>\n   <td style=\"text-align:center;\"> 9.04 </td>\n   <td style=\"text-align:center;\"> 18.00 </td>\n   <td style=\"text-align:center;\"> 4.69 </td>\n   <td style=\"text-align:center;\"> 40.31 </td>\n   <td style=\"text-align:center;\"> 0.05 </td>\n   <td style=\"text-align:center;\"> 0.49 </td>\n   <td style=\"text-align:center;\"> 0.22 </td>\n   <td style=\"text-align:center;\"> 0.26 </td>\n   <td style=\"text-align:center;\"> 0.26 </td>\n   <td style=\"text-align:center;\"> 0.74 </td>\n   <td style=\"text-align:center;\"> 2.12 </td>\n   <td style=\"text-align:center;\"> 0.84 </td>\n   <td style=\"text-align:center;\"> 7.75 </td>\n   <td style=\"text-align:center;\"> 13.34 </td>\n   <td style=\"text-align:center;\"> 5.09 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 156 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 10.74 </td>\n   <td style=\"text-align:center;\"> 9.04 </td>\n   <td style=\"text-align:center;\"> 18.00 </td>\n   <td style=\"text-align:center;\"> 4.69 </td>\n   <td style=\"text-align:center;\"> 39.35 </td>\n   <td style=\"text-align:center;\"> 0.05 </td>\n   <td style=\"text-align:center;\"> 0.51 </td>\n   <td style=\"text-align:center;\"> 0.21 </td>\n   <td style=\"text-align:center;\"> 0.29 </td>\n   <td style=\"text-align:center;\"> 0.26 </td>\n   <td style=\"text-align:center;\"> 0.63 </td>\n   <td style=\"text-align:center;\"> 2.07 </td>\n   <td style=\"text-align:center;\"> 0.84 </td>\n   <td style=\"text-align:center;\"> 7.40 </td>\n   <td style=\"text-align:center;\"> 13.92 </td>\n   <td style=\"text-align:center;\"> 5.09 </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\nWith real-world data no single evaluation measure is likely perfect, and different measures may produce different results depending on the characteristics of the data and the research question being addressed [@kittler1998]. We found that across most indices, the analysis with $k=2$ clusters performed the best (see [@tbl-cluster-performance]). \n\n### Visual Illustration\n\nTwo commonly reported performance measures showed this visually as well. The first statistic is the total within-cluster sum of square $WCSS$. While the within-cluster variation will naturally decrease with (more) smaller clusters, we observed that the decrease in $WCSS$ was largest until $k=2$ after which the decrease was much smaller. This method is also sometimes referred to as the 'elbow method' [@syakur2018].\n\n\n::: {.cell}\n\n```{.r .numberLines .lineAnchors .cell-code}\nfviz_nbclust(pca_scores, kmeans, method = \"wss\") +\n  geom_vline(xintercept = 2, linetype = 2)+\n  labs(subtitle = \"Performance Metric: Elbow method\") +\n  theme_Publication()\n```\n\n::: {.cell-output-display}\n![](05-evaluation_files/figure-html/Plot Elbow method Performance-1.png){width=672}\n:::\n:::\n\n\nA second, commonly used measure is the average silhouette score. This statistic measures the degree to which each time feature data point is similar to other points within the same cluster, compared to points in other clusters [@rousseeuw1987]. In our case, the $k=2$ solution maximized the silhouette coefficient ($s_2=$ 0.09).\n\n\n::: {.cell}\n\n```{.r .numberLines .lineAnchors .cell-code}\nfviz_nbclust(pca_scores, kmeans, method = \"silhouette\")+\n  labs(subtitle = \"Performance Metric: Silhouette method\") +\n  theme_Publication()\n```\n\n::: {.cell-output-display}\n![](05-evaluation_files/figure-html/Plot Silhouette method Performance-1.png){width=672}\n:::\n:::\n\n\n### Best-performing Solution\n\nWe thus, save the $k=2$ solution as the final model `kmeans_final`.\n\n\n::: {.cell}\n\n```{.r .numberLines .lineAnchors .cell-code}\nset.seed(12345)\nkmeans_final <- kmeans(pca_scores, centers = 2, nstart = 100)\nkmeans_final$cluster\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  20  21 \n  2   2   2   1   2   2   2   2   2   1   2   2   2   2   2   2   1   1   1   2 \n 22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41 \n  1   2   2   1   2   1   1   2   2   1   1   1   1   1   2   2   1   1   1   2 \n 42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61 \n  1   1   1   2   2   1   2   2   2   1   2   1   1   1   2   2   1   2   2   2 \n 62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81 \n  2   2   1   1   2   2   1   2   1   1   1   1   2   1   2   1   2   2   2   1 \n 82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 \n  1   2   1   1   1   1   1   2   1   2   2   1   1   2   2   1   1   2   1   2 \n102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 \n  1   1   1   1   2   1   2   1   2   2   1   2   2   1   1   2   2   2   2   2 \n122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 \n  2   1   2   1   1   1   1   1   1   1   2   2   2   2   1   2   2   1   2   2 \n142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 \n  1   1   1   2   1   2   2   1   2   1   2   1   2   1   1   2 \n```\n:::\n\n```{.r .numberLines .lineAnchors .cell-code}\nsaveRDS(kmeans_final, file = \"data/kmeans_final.Rds\")\n```\n:::\n\n\nWe can then use the cluster assignments to visualize the two clusters. This is commonly done using a projection method where the group separation is visualized on a two-dimensional pane. This is not ideal (because the clustering takes a multi-dimensional space into account), but can give a first indivation of the cluster separation.\n\n\n::: {.cell}\n\n```{.r .numberLines .lineAnchors .cell-code}\n# label: visualize the cluster assignment\n\nfviz_cluster(kmeans_final, geom = \"point\", data = z_data, ggtheme = theme_Publication())\n```\n\n::: {.cell-output-display}\n![](05-evaluation_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Interpretation\n\nThe interpretation of feature-based time series clustering in psychology involves understanding the meaning and implications of the obtained clusters. In order to make sense of the clustering results, we here focus on three general aspects of the results [@kaufman1990]. (1) Assessing differences between the clusters in the original time series features, (2) comparing the clusters based on prototype developments, (3) comparing the clusters based on between-person differences that were not included in the initial clustering.\n\n### TS Feature Comparison\n\nWe first inspect the clusters based on the average values of meaningful features. To do so, we merge the cluster assignments from the `kmeans_final` object back to the original set of standardized time series features `z_data` (same as `scaled_data` earlier). We then are able to group the features by the clusters and calculate average feature values for each cluster. We also pivot the dataframe into long format for easier visualizations, separate the variable and the feature identifiers, and add more descriptive labels for the visualizations.\n\n::: scrolling\n\n::: {.cell}\n\n```{.r .numberLines .lineAnchors .cell-code}\n# calculate number of removed datapoints\nmissingness_ooc_id <- full_join(\n  featData %>%\n    mutate(uid = paste(study, PID, sep = \"_\")) %>%\n    group_by(uid, ID) %>%\n    summarise(n_feature = n()) %>%\n    ungroup(),\n  dt_raw %>%\n    mutate(uid = paste(study, PID, sep = \"_\")) %>%\n    group_by(uid) %>%\n    summarise(n_raw = n()) %>%\n    ungroup(),\n  by = \"uid\"\n) %>%\n  mutate(\n    n_rm = n_raw - n_feature\n  ) %>%\n  filter(\n    !is.na(ID)\n  ) %>%\n  mutate(nrm_z = scale(n_rm)) %>%\n  select(ID, nrm_z)\ndiscrimination_ooc_id <- featData %>% \n  select(ID, EvDayDiscr.post) %>% \n  distinct(ID, EvDayDiscr.post, .keep_all = TRUE) %>%\n  mutate(EvDayDiscr.post_z = scale(EvDayDiscr.post, scale = TRUE)) %>%\n  replace_na(list(EvDayDiscr.post_z = NA)) %>%\n  select(\n    ID,\n    EvDayDiscr.post_z\n  )\n\nfeatCluster <- data.frame(ID = as.numeric(names(kmeans_final$cluster)), \n                            cluster = kmeans_final$cluster) %>%\n  left_join(z_data, by = \"ID\") %>%\n  left_join(discrimination_ooc_id, by = \"ID\") %>% \n  left_join(missingness_ooc_id, by = \"ID\") %>%\n  select(-ID) %>%\n  group_by(cluster) %>%\n  summarise(across(everything(), list(mean = mean), na.rm=TRUE)) %>%\n  ungroup %>% \n  select(-starts_with(\"ID\")) %>%\n  pivot_longer(\n    cols = -c(cluster),\n    names_to = c(\"variable\", \".value\"),\n    names_pattern = \"(.*)_(.*)\"\n  ) %>% \n  mutate(\n    nam = variable,\n    variable = gsub(\"\\\\_.*\", \"\", nam),\n    feature = gsub(\"^.*?\\\\_\", \"\", nam)\n  ) %>% \n  mutate(feature = ifelse(feature == \"z\", \"mean\", feature)) %>%\n  left_join(var_meta %>% select(name, label) %>% rbind(c(\"nrm\", \"OOC: Measurements removed\")), by = c(\"variable\" = \"name\")) %>%\n  select(\n    cluster,\n    nam,\n    variable,\n    label,\n    feature,\n    everything()\n  ) %>% \n  mutate(feature = gsub(\"^n$\", \"n (within ooc)\", feature)) %>% \n  mutate(feature = gsub(\"^mean$\", \"between ooc (mean)\", feature))\n```\n:::\n\n:::\n\nWe can then plot the average features and for each variable and flipping the axes allows us to assess the same data once with a focus on the variable (see [@fig-clusterFeatVarComb] A) and once with a focus on the features (see [@fig-clusterFeatVarComb] B).\n\n\n::: {.cell}\n\n```{.r .numberLines .lineAnchors .cell-code}\n# clusterVariableGrid\nclusterVariableGrid <- featCluster %>% \n  filter(variable != \"EvDayDiscr.post\",\n         variable != \"nrm\",\n         feature != \"n (within ooc)\") %>%\n  ggplot(., aes(x = feature, y = mean, group = cluster, color = as.factor(cluster))) +\n  geom_point() +\n  geom_line() +\n  scale_colour_manual(values = c(\"#384B6B\", \"#E2892B\")) + \n  #geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width=.1, position=position_dodge(0.1), size=0.5, alpha=0.4) +\n  coord_flip() +\n  labs(\n    x = \"Features\",\n    y = \"Standardized Mean\",\n    color = \"Cluster\"\n  ) +\n  facet_wrap(~ label) +\n  theme_Publication()\n\n# clusterFeatureGrid\nclusterFeatureGrid <- featCluster %>% \n  ggplot(., aes(x = reorder(label, as.numeric(!label %in% c(\"OOC: Discrimination\", \"OOC: Measurements removed\"))), y = mean, group = cluster, color = as.factor(cluster))) +\n  geom_point() +\n  geom_line() +\n  #geom_ribbon(aes(ymin = mean-se, ymax = mean+se), alpha = 0.2) +\n  #geom_errorbar(aes(ymin = mean-se, ymax = mean+se), width=.1, position=position_dodge(0.1), size=0.5, alpha=0.4) +\n  scale_colour_manual(values = c(\"#384B6B\", \"#E2892B\")) + \n  coord_flip() +\n  labs(\n    x = \"Variable\",\n    y = \"Standardized Mean\",\n    color = \"Cluster\"\n  ) +\n  facet_wrap(~ factor(feature, levels = unique(featCluster$feature)), ncol=4) +\n  theme_Publication()\n\nclusterFeatVarComb <- ggpubr::ggarrange(\n  clusterVariableGrid + theme(axis.title.x = element_blank()),\n  clusterFeatureGrid,\n  ncol = 1,\n  labels = c(\"(A) Variable Focus\", \"(B) Feature Focus\"),\n  common.legend = TRUE,\n  legend = \"bottom\",\n  align = \"v\"\n) \nggsave(\"../Figures/clusterFeatVarComb_tutorial.pdf\", clusterFeatVarComb, width = 12, height = 12)\nclusterFeatVarComb\n```\n\n::: {.cell-output-display}\n![Cluster comparison by feature and variable. Note that n and discrimination are out of cluster variables.](05-evaluation_files/figure-html/fig-clusterFeatVarComb-1.png){#fig-clusterFeatVarComb width=1152}\n:::\n:::\n\n\nWe discuss the individual differences and their theoretical relevance as part of the main manuscript. However, in terms of methodological insight, we find variables and features that distinguish the clusters better than others and a combination of variables and features lets us explore meaningful group differences in more detail. In our case, we see that the central tendency, variability, and linear trend are best at distinguishing a group with mainly positive experiences (cluster 1) from a group with a more negative experience (cluster 2). We also see that our clusters line up with the past literature on the importance of focusing on simpler and more meaningful statistics [@bringmann2018c;@eronen2021a].\n\n### Average Time Series\n\nIn the second step, we look at the prototypical trajectories of the clusters. For k-means clustering it is often recommended to use the average over time of the responses within the cluster [@niennattrakul2007].  We visualize the cluster-specific time point averages (see [@fig-cluster-timeseries] A) as well as an average spline per cluster (see [@fig-cluster-timeseries] A).\n\n\n::: {.cell}\n\n```{.r .numberLines .lineAnchors .cell-code}\nrawCluster <- data.frame(ID = as.numeric(names(kmeans_final$cluster)), \n                            cluster = kmeans_final$cluster) %>%\n  left_join(raw_data, by = \"ID\") %>%\n  select(\n    ID,\n    TIDnum,\n    cluster,\n    everything()\n  ) %>%\n  reshape2::melt(id.vars = c(\"cluster\", \"ID\", \"TIDnum\")) %>% \n  filter(\n    !variable %in% c(\"PID\", \"study\", \"date\", \"week\", \"TID\", \"EvDayDiscr.post\"),\n    !is.na(value)\n  ) %>%\n  left_join(var_meta %>% select(name, label), by = c(\"variable\" = \"name\")) %>%\n  mutate(value = as.numeric(value))\n\nclusterTS <- rawCluster %>% \n  ggplot(., aes(x=TIDnum, y=as.numeric(value), group=cluster, color=as.factor(cluster))) +\n  geom_line(aes(x=TIDnum, y=as.numeric(value), group=ID, color=as.factor(cluster)), alpha=0.05) +\n  stat_summary(fun=mean, geom=\"line\") +\n  scale_colour_manual(values = c(\"#384B6B\", \"#E2892B\")) + \n  facet_wrap(. ~ label) +\n  labs(\n    x = \"Time ID\",\n    y = \"Response (Mean Cluster Response Bold)\",\n    color = \"Cluster\"\n  ) +\n  theme_Publication() \n\nclusterTSTrend <- rawCluster %>% \n  ggplot(., aes(x=TIDnum, y=value, group=cluster, color=as.factor(cluster))) +\n  geom_line(stat=\"smooth\", method = \"gam\", aes(group=ID, color=as.factor(cluster)), se = FALSE, alpha=0.1) +\n  geom_smooth(aes(group=cluster), method = \"gam\", se = FALSE) +\n  scale_colour_manual(values = c(\"#384B6B\", \"#E2892B\")) + \n  facet_wrap(~ label) +\n  scale_y_continuous(limits = c(0, 100)) +\n  labs(\n    x = \"Time ID\",\n    y = \"Response (Mean Cluster Response Bold)\",\n    color = \"Cluster\"\n  ) +\n  theme_Publication()\n\nggpubr::ggarrange(\n  clusterTS + theme(axis.title.x = element_blank()),\n  clusterTSTrend,\n  ncol = 1,\n  labels = c(\"(A) Timepoint Means\", \"(B) Spline Trends\"),\n  common.legend = TRUE,\n  legend = \"bottom\"\n)\n```\n\n::: {.cell-output-display}\n![Average cluster trajectories (using LM and GAM)](05-evaluation_files/figure-html/fig-cluster-timeseries-1.png){#fig-cluster-timeseries width=1152}\n:::\n:::\n\n\nWhile we discuss the interpretations in more detail in the manuscript, the average cluster trajectories offer clearer insights into the mean differences, the variances, as well as the trends over time.\n\n### (Out-of-)Cluster Comparison\n\nFinally, we can also assess the clusters across other individual difference variables [@monden2022]. This out-of-feature comparison allows us to check for data artifacts, as well as check whether the developmental clusters are associated with important social markers and individual differences. \n\nTo illustrate artifact checks, we added the number of ESM measurements into the comparison and find that the participants in the deterioration cluster (cluster 2) on average completed slightly more ESM surveys in general and reported on more intergroup interactions in particular (see $n$ in [@fig-clusterFeatVarComb] B). This difference in the reported number of interactions might indicate either a clustering artifact or a meaningful difference. The higher average number of interactions in cluster 2 could, for example, indicate a clustering artifact if variances are substantially larger due to the larger samples (e.g., restriction of range in the smaller sample; @kogan2006). In our case, this seems less likely because one out of four variables did not differ in terms of the MAD (i.e., our selected measurement of the time series variance; see plot selection below). \n\n::: scrolling\n\n::: {.cell}\n\n```{.r .numberLines .lineAnchors .cell-code}\n# ESM features\nraw_feat_clust <-\n  data.frame(ID = as.numeric(names(kmeans_final$cluster)),\n             cluster = kmeans_final$cluster) %>%\n  left_join(z_data, by = \"ID\")\n\nconcepts_and_feature <- str_match(colnames(raw_feat_clust), \"(.*)_(.*)\")\nconcepts <- as.character(na.omit(unique(concepts_and_feature[, 2])))\nts_features <- as.character(na.omit(unique(concepts_and_feature[, 3])))\n\nresults <- run_all_tests(\n  df = raw_feat_clust,\n  cluster_var = \"cluster\",\n  ts_features = ts_features,\n  concepts = concepts\n)\n\nresults_df <- convert_test_to_dataframe(\n  results = results,\n  ts_features = ts_features, \n  concept = concepts\n)\n\n\n# validation features\nmissingness_ooc <- full_join(\n  featData %>%\n    mutate(uid = paste(study, PID, sep = \"_\")) %>%\n    group_by(uid, ID) %>%\n    summarise(n_feature = n()) %>%\n    ungroup(),\n  dt_raw %>%\n    mutate(uid = paste(study, PID, sep = \"_\")) %>%\n    group_by(uid) %>%\n    summarise(n_raw = n()) %>%\n    ungroup(),\n  by = \"uid\"\n) %>%\n  mutate(\n    n_rm = n_raw - n_feature\n  ) %>%\n  filter(\n    !is.na(ID)\n  ) %>%\n  select(ID, n_rm) %>%\n  mutate(n_rm_z = scale(n_rm)) %>%\n  left_join(data.frame(ID = as.numeric(names(kmeans_final$cluster)),\n             cluster = kmeans_final$cluster), by = \"ID\")\nt_missingness <- t.test(\n  missingness_ooc$n_rm_z[missingness_ooc$cluster == 1],\n  missingness_ooc$n_rm_z[missingness_ooc$cluster == 2],\n  na.action = na.omit\n)\n\ndiscrimination_ooc <-\n  featData %>% \n  select(ID, EvDayDiscr.post) %>% \n  distinct(ID, EvDayDiscr.post, .keep_all = TRUE) %>%\n  mutate(EvDayDiscr.post_z = scale(EvDayDiscr.post, scale = TRUE)) %>%\n  replace_na(list(EvDayDiscr.post_z = NA)) %>%\n  select(\n    ID,\n    EvDayDiscr.post_z\n  ) %>%\n  left_join(data.frame(ID = as.numeric(names(kmeans_final$cluster)),\n             cluster = kmeans_final$cluster), by = \"ID\")\n\nt_discrimination <- t.test(\n  discrimination_ooc$EvDayDiscr.post_z[discrimination_ooc$cluster == 1],\n  discrimination_ooc$EvDayDiscr.post_z[discrimination_ooc$cluster == 2],\n  na.action = na.omit\n)\n\nresults_df <- rbind(\n  results_df,\n  data.frame(\n    concept = c(\"EvDayDiscr.post\", \"missingness\"),\n    feature = c(\"mean\", \"mean\"),\n    difference = c(t_discrimination$estimate[1] - t_discrimination$estimate[2], t_missingness$estimate[1] - t_missingness$estimate[2]),\n    t_value = c(t_discrimination$statistic, t_missingness$statistic),\n    df = c(t_discrimination$parameter, t_missingness$parameter),\n    p_value = c(t_discrimination$p.value, t_missingness$p.value),\n    lwr = c(t_discrimination$conf.int[1], t_missingness$conf.int[1]),\n    upr = c(t_discrimination$conf.int[2], t_missingness$conf.int[2])\n  )\n) %>%\n  annotate_significance(., \n                        variableLab = var_meta %>% select(name, label), \n                        join_by = c('concept' = 'name'))\n\nresults_df[results_df$concept==\"missingness\",colnames(results_df)==\"label\"] <- \"Missingness (N removed)\"\n\nfeature_plot_list <- generate_feature_plots(results_df)\n```\n:::\n\n:::\n\n\n<div id=\"plot_ar01\" style=\"display: none;\">![](05-evaluation_files/figure-html/render hidden plots-1.png){width=672}</div><div id=\"plot_edf\" style=\"display: none;\">![](05-evaluation_files/figure-html/render hidden plots-2.png){width=672}</div><div id=\"plot_lin\" style=\"display: none;\">![](05-evaluation_files/figure-html/render hidden plots-3.png){width=672}</div><div id=\"plot_mac\" style=\"display: none;\">![](05-evaluation_files/figure-html/render hidden plots-4.png){width=672}</div><div id=\"plot_mad\" style=\"display: block;\">![](05-evaluation_files/figure-html/render hidden plots-5.png){width=672}</div><div id=\"plot_median\" style=\"display: none;\">![](05-evaluation_files/figure-html/render hidden plots-6.png){width=672}</div><div id=\"plot_n\" style=\"display: none;\">![](05-evaluation_files/figure-html/render hidden plots-7.png){width=672}</div><div id=\"plot_mean\" style=\"display: none;\">![](05-evaluation_files/figure-html/render hidden plots-8.png){width=672}</div>\n\n\n<select id=\"plot_selector\" onchange=\"showPlot()\">\n  <option value=\"plot_mad\" selected>MAD</option>\n  <option value=\"plot_median\">Median</option>\n  <option value=\"plot_n\">Measurements</option>\n  <option value=\"plot_lin\">linear slope</option>\n  <option value=\"plot_mac\">MAC</option>\n  <option value=\"plot_edf\">GAM edf</option>\n  <option value=\"plot_ar01\">AR lag1</option>\n  <option value=\"plot_mean\">Validation</option>\n</select>\n\n<script>\nfunction showPlot() {\n  var selectedPlot = document.getElementById(\"plot_selector\").value;\n  var plots = document.querySelectorAll('div[id^=\"plot_\"]');\n  \n  plots.forEach(function(plot) {\n    if (plot.id === selectedPlot) {\n      plot.style.display = 'block';\n    } else {\n      plot.style.display = 'none';\n    }\n  });\n}\n\n// Call showPlot on page load to display the default plot\ndocument.addEventListener('DOMContentLoaded', showPlot);\n</script>\n\nAt the same time, however, the difference in the number of experienced interactions might also indicate a meaningful difference, where the deteriorating cluster (cluster 2) on average reported more outgroup interactions (<i>difference</i> = 1.03, <i>t</i>(150.83) = 7.50, <i>p</i> < .001, <i>95%CI</i> [0.76, 1.30]), but these interactions were less voluntary (<i>difference</i> = -1.04, <i>t</i>(108.89) = -7.71, <i>p</i> < .001, <i>95%CI</i> [-1.31, -0.77]), less meaningful (<i>difference</i> = -1.00, <i>t</i>(136.40) = -7.16, <i>p</i> < .001, <i>95%CI</i> [-1.28, -0.73]), and less positive (<i>difference</i> = -1.38, <i>t</i>(152.31) = -11.94, <i>p</i> < .001, <i>95%CI</i> [-1.61, -1.15]). \n\nTo underscore the value of examining out-of-feature individual variations, we also contrasted the two samples based on the participants' self-reported experiences of discrimination in the Netherlands, captured during the post-measurement phase. Upon analyzing the group differences, it was evident that those in the deteriorating cluster (cluster 2) indicated notably elevated instances of daily discrimination (<i>difference</i> = -0.40, <i>t</i>(151.71) = -2.56, <i>p</i> = 0.011, <i>95%CI</i> [-0.71, -0.09]; also see [@fig-clusterFeatVarComb] B). In short, both intensive longitudinal measurement (e.g., the sum of specific ESM measurements) and cross-sectional variables (e.g., general discrimination differences) that were not included in the original clustering step can be leveraged to delve deeper into understanding cluster disparities.\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "05-evaluation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}