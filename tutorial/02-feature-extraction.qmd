---
title: "2. Feature Extraction"
bibliography: [packages.bib, ../referencesZotero.bib]
csl: ../apa.csl
---


```{r}
#| label: setup
#| include: FALSE

lib <- c(
  "rmarkdown",
  "knitr",
  "tidyverse",
  "kableExtra",
  "tsFeatureExtracR"
)
invisible(lapply(lib, library, character.only = TRUE))
rm(lib)

knitr::opts_chunk$set(
  class.source = 'numberLines lineAnchors'
)

# import sourced data for faster renders
source("scripts/load_data.R")
```

Armed with a relevant selection of key variables, the main aim of the feature extraction is to describe the most important and meaningful aspects of a time series. In its most general approach, feature extraction can include any numeric summary of the time series [@maharaj2019]. Given this flexibility, a staggering variety of time series features have been proposed across different disciplines [@wang2006;@fulcher2013;@adya2001].

## Feature Choice

With such a large number of options, choosing meaningful and relevant time series features is not an easy feat. However, not all time series features might be relevant to psychological time series or any particular research question. For example, a psychologist interested in well-being might not necessarily be interested in the exact time point after which 50% of the summed well-being values lie (i.e., relative mass quantile index) or how much different sine wave patterns within the well-being data correlate with one another (i.e., cross power spectral density). Instead, we advocate that we look at time series features that have a strong backing within the experience sampling (i.e., ESM) literature and offer meaningful interpretability.

## Extraction Extraction

For our illustration, we rely on the work of researchers in the ESM literature, where different research teams have proposed a number meaningful measures to capture time series features that are relevant to psychological time series [@kuppens2017;@krone2018;@dejonckheere2019;@wendt2020]. The main article discusses the individual time series features, their conceptual interpretation, as well as the technical calculation in more detail. Here we focus on the code to extract the features for the included variables. 

::: {.grid}

::: {.g-col-4}
[![](images/logo_tsfeatureextractr.png)](https://github.com/JannisCodes/tsFeatureExtracR)
[Download the R package here](https://tsfeatureclustr.com/assets/tsFeatureExtracR_0.1.0.tar.gz)
:::

::: {.g-col-8}
The selection of time series features should always be driven by the research questions and field-specific conventions, but for our illustration we chose six time series features that relate to common psychological research questions and recent works within the field: (1) central tendency, (2) variability, (3) instability, (4) self-similarity, (5) linear trend, and (6) nonlinearity. We provide the `featureExtractor()` function to extract a number of measurements for each time series aspect. The function is also available as part of the R package that bundles a number of functions we used for the illustration. 
:::

:::

The time series feature operationalizations we chose for our illustration here are marked in bold.

1. central tendency
    + mean
    + **median**
2. variation around the central tendency (i.e., variability)
    + SD
    + **MAD**
3. (in)stability
    + root mean square of successive differences (rmssd)
    + **mean absolute change (mac)**
4. self-similarity
    + **lag-1 autocorrelation**
    + periodicity (lag-2 & lag-14 autocorrelation)
5. linear trend
    + **OLS regression parameter**
6. non-linear trend
    + **GAM edf**

_Note:_ Beyond the time series features described in this illustration, the [`tsfresh` python package](https://tsfresh.readthedocs.io/en/latest/) allows users to extract almost 800 time series features [@christ2018]. The package is well maintained and offers clear documentation of how each time series feature is calculated.

We, thus, use enter our cleaned variables into the `featureExtractor()` function, where we additionally specify the participant id `pid` and the numeric time id `tid` as well as a vector with the column names of the variables for which we want to extract the features `items`.

```{r}
#| label: run feature extraction
#| eval: false

# devtools::install_github("JannisCodes/tsFeatureExtracR")
library(tsFeatureExtracR)

# just in case check that only variables that have any data (i.e., not all NA)
featData <- dtAll %>%
  arrange(ID, TIDnum) %>%
  select_if(~ sum(!is.na(.)) > 1) %>% 
  as.data.frame

# Don't re-run unless you have a few hours to spare!
featFull <-
  featureExtractor(
    data = featData,
    pid = "ID",
    tid = "TIDnum",
    items = var_meta$name[var_meta$cluster == 1]
  )

# remove internal test account we missed earlier
featFull$features <- featFull$features %>%
    filter(
      ID != 19
    )

```

## Feature Missingness

After the feature extraction, again check how much missingess the data now entails. This check is important for two reasons. Firstly, the feature extraction should largely reduce the context-specific missingness as part of the summarization. However, secondly, even small numbers of missingness might not be accepted by some feature reduction or feature clustering algorithms. We this assess the missingess and impute the missing features based on the remaining data (provided that the missingness is small).

### Check Missingness

To assess the missingness, we enter the extracted features into our `feature_missing()` function, which assess global and feature=specific missingess (including a graphical representation of frequency of different amounts of missingess across the extracted features). 

```{r}
#| label: feature missinginess

feature_selection <- c(
  "ar01",
  "edf",
  "lin",
  "mac",
  "mad",
  "median"
)

perc_miss_features <-
  feature_missing(featFull$features %>% 
                    select(ends_with(feature_selection)), 
                  title = "Feature-wise Missingness")

perc_miss_features$plt_miss_per_feature
```

For the time series features across the three studies, we find that about `r format(round(perc_miss_features$miss_overall, 2), nsmall=2)`% of the extracted features are missing across the `r nrow(featFull$features)` participants and `r ncol(featFull$features)` features. This might, for example, happen if participants do not have two subsequent measurements with outgroup interactions, so that an autocorrelation with lag-1 cannot be calculated for the contact-specific variables. The small number of missing values indicates that the feature-based approach indeed largely avoids the structural missingness issue. 

### Impute Missing Features

For all missing features, we impute a single predictive mean matching imputation using a custom wrapper (i.e., the `featureImputer()` function) of the `MICE` package [@buuren2011]. Note again that with this procedure we only need to impute an extremely small number of missing values as most feature calculations can use the available data instead.

```{r}
#| label: feature imputer
#| eval: false

featFullImp <- featureImputer(featFull)
```

With the extracted and imputed features we will next have to check whether the feature set is still too large for a clustering analysis.


## References

::: {#refs}
:::
